{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ANN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahmoudFarouq/faceVerification/blob/master/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Z-5T0lvsS2o9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage import color\n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout\n",
        "from keras import regularizers\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "35AcxGqCTfNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "404c35f3-87af-467f-a1d4-45d3693e0530"
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/MahmoudFarouq/faceVerification/raw/master/dataObject.pkl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-12-26 14:57:00--  https://github.com/MahmoudFarouq/faceVerification/raw/master/dataObject.pkl\n",
            "Resolving github.com (github.com)... 192.30.253.112, 192.30.253.113\n",
            "Connecting to github.com (github.com)|192.30.253.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/MahmoudFarouq/faceVerification/master/dataObject.pkl [following]\n",
            "--2018-12-26 14:57:00--  https://raw.githubusercontent.com/MahmoudFarouq/faceVerification/master/dataObject.pkl\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27648166 (26M) [application/octet-stream]\n",
            "Saving to: ‘dataObject.pkl’\n",
            "\n",
            "dataObject.pkl      100%[===================>]  26.37M   111MB/s    in 0.2s    \n",
            "\n",
            "2018-12-26 14:57:03 (111 MB/s) - ‘dataObject.pkl’ saved [27648166/27648166]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xtALZhcAToKY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"dataObject.pkl\", 'rb') as dataFile:\n",
        "  X = pickle.load(dataFile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mFnDfjWjTu_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ab41c87-2a1a-4ae9-9529-3cb9bf324f4c"
      },
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 48, 48, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "aHanehPfUPJi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = [\n",
        "    np.full((250, 1), 0), \n",
        "    np.full((250, 1), 1), \n",
        "    np.full((250, 1), 2), \n",
        "    np.full((250, 1), 3)\n",
        "]\n",
        "y = np.vstack(y)\n",
        "\n",
        "# onehot encoding\n",
        "y_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y = y_encoder.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1Zop48vwVsBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c952f62-0f40-4f64-dd15-b68990c14208"
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1000, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "f5oeL3IQhVh8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "b204f13f-dc35-4e93-f78c-77bfd8d0239b"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow( X[750].reshape(48, 48, 3) )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8922f15d68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFLCAYAAABft66eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f89257e4278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "jFacrW8BnYh8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = color.rgb2gray( X.reshape(-1, 48, 48, 3) )\n",
        "X = X.reshape(1000, -1)\n",
        "X /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1Cy7OusnscZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GbN1raeMoRUR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n = X_train[0].shape[0] # num of features -> 48 * 48 * 3 in this case\n",
        "drop = 0.2\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(1000, activation=\"relu\", input_shape=(n,)))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(800, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(600, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(500, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(400, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(400, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(300, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(300, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(200, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(150, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(150, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(100, activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(50 , activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(20 , activation=\"relu\"))\n",
        "model.add(Dropout(drop))\n",
        "model.add(Dense(4  , activation=\"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5m51anwdoZ-a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = Adam(lr=0.0001)\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cInlEZ_1odOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34785
        },
        "outputId": "a7ef5daf-2605-455e-af92-215d66b12525"
      },
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train, \n",
        "    y_train,\n",
        "    validation_data=(X_test,y_test),\n",
        "    validation_split=0.8,\n",
        "    epochs=1000,\n",
        "    shuffle=True,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/1000\n",
            "800/800 [==============================] - 4s 5ms/step - loss: 1.3863 - acc: 0.2412 - val_loss: 1.3864 - val_acc: 0.2450\n",
            "Epoch 2/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 1.3863 - acc: 0.2412 - val_loss: 1.3864 - val_acc: 0.2450\n",
            "Epoch 3/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.3863 - acc: 0.2313 - val_loss: 1.3865 - val_acc: 0.2450\n",
            "Epoch 4/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 1.3862 - acc: 0.2475 - val_loss: 1.3866 - val_acc: 0.2450\n",
            "Epoch 5/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.3862 - acc: 0.2550 - val_loss: 1.3866 - val_acc: 0.2450\n",
            "Epoch 6/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 1.3863 - acc: 0.2500 - val_loss: 1.3866 - val_acc: 0.2450\n",
            "Epoch 7/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 1.3860 - acc: 0.2538 - val_loss: 1.3866 - val_acc: 0.2450\n",
            "Epoch 8/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.3862 - acc: 0.2687 - val_loss: 1.3866 - val_acc: 0.2450\n",
            "Epoch 9/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.3863 - acc: 0.2412 - val_loss: 1.3866 - val_acc: 0.1850\n",
            "Epoch 10/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.3864 - acc: 0.2250 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 11/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 1.3864 - acc: 0.2462 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 12/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 1.3861 - acc: 0.2687 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 13/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.3862 - acc: 0.2438 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 14/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 1.3864 - acc: 0.2387 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 15/1000\n",
            "800/800 [==============================] - 0s 281us/step - loss: 1.3863 - acc: 0.2512 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 16/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 1.3862 - acc: 0.2525 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 17/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 1.3862 - acc: 0.2525 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 18/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 1.3861 - acc: 0.2613 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 19/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.3862 - acc: 0.2512 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 20/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.3864 - acc: 0.2587 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 21/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.3864 - acc: 0.2525 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 22/1000\n",
            "800/800 [==============================] - 0s 278us/step - loss: 1.3862 - acc: 0.2625 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 23/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.3862 - acc: 0.2575 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 24/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.3861 - acc: 0.2562 - val_loss: 1.3867 - val_acc: 0.2200\n",
            "Epoch 25/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.3860 - acc: 0.2650 - val_loss: 1.3868 - val_acc: 0.2200\n",
            "Epoch 26/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.3856 - acc: 0.2650 - val_loss: 1.3870 - val_acc: 0.2200\n",
            "Epoch 27/1000\n",
            "800/800 [==============================] - 0s 281us/step - loss: 1.3859 - acc: 0.2512 - val_loss: 1.3871 - val_acc: 0.2200\n",
            "Epoch 28/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.3850 - acc: 0.2625 - val_loss: 1.3880 - val_acc: 0.2200\n",
            "Epoch 29/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 1.3822 - acc: 0.2550 - val_loss: 1.3924 - val_acc: 0.2200\n",
            "Epoch 30/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.3860 - acc: 0.2600 - val_loss: 1.3882 - val_acc: 0.2200\n",
            "Epoch 31/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.3832 - acc: 0.2450 - val_loss: 1.3865 - val_acc: 0.2200\n",
            "Epoch 32/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.3836 - acc: 0.2562 - val_loss: 1.3858 - val_acc: 0.2200\n",
            "Epoch 33/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 1.3785 - acc: 0.2637 - val_loss: 1.3887 - val_acc: 0.2200\n",
            "Epoch 34/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 1.3805 - acc: 0.2500 - val_loss: 1.3866 - val_acc: 0.2200\n",
            "Epoch 35/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.3716 - acc: 0.2662 - val_loss: 1.3851 - val_acc: 0.2400\n",
            "Epoch 36/1000\n",
            "800/800 [==============================] - 0s 278us/step - loss: 1.3746 - acc: 0.2900 - val_loss: 1.3850 - val_acc: 0.2850\n",
            "Epoch 37/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.3718 - acc: 0.3187 - val_loss: 1.3874 - val_acc: 0.2500\n",
            "Epoch 38/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 1.3703 - acc: 0.3375 - val_loss: 1.3858 - val_acc: 0.2850\n",
            "Epoch 39/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.3667 - acc: 0.3112 - val_loss: 1.3857 - val_acc: 0.2900\n",
            "Epoch 40/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.3592 - acc: 0.3325 - val_loss: 1.3865 - val_acc: 0.2900\n",
            "Epoch 41/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 1.3610 - acc: 0.3550 - val_loss: 1.3937 - val_acc: 0.2800\n",
            "Epoch 42/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.3685 - acc: 0.3425 - val_loss: 1.3850 - val_acc: 0.3200\n",
            "Epoch 43/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.3590 - acc: 0.3387 - val_loss: 1.3935 - val_acc: 0.2850\n",
            "Epoch 44/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 1.3714 - acc: 0.3362 - val_loss: 1.3833 - val_acc: 0.3100\n",
            "Epoch 45/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 1.3569 - acc: 0.3550 - val_loss: 1.3875 - val_acc: 0.2950\n",
            "Epoch 46/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 1.3487 - acc: 0.3562 - val_loss: 1.3836 - val_acc: 0.2950\n",
            "Epoch 47/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 1.3557 - acc: 0.3488 - val_loss: 1.3887 - val_acc: 0.3000\n",
            "Epoch 48/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.3574 - acc: 0.3450 - val_loss: 1.3835 - val_acc: 0.3200\n",
            "Epoch 49/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.3654 - acc: 0.3450 - val_loss: 1.3815 - val_acc: 0.3150\n",
            "Epoch 50/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.3606 - acc: 0.3400 - val_loss: 1.3894 - val_acc: 0.2850\n",
            "Epoch 51/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.3606 - acc: 0.3362 - val_loss: 1.3788 - val_acc: 0.3150\n",
            "Epoch 52/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 1.3440 - acc: 0.3588 - val_loss: 1.3947 - val_acc: 0.2700\n",
            "Epoch 53/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.3607 - acc: 0.3438 - val_loss: 1.3803 - val_acc: 0.3050\n",
            "Epoch 54/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.3518 - acc: 0.3463 - val_loss: 1.3825 - val_acc: 0.2950\n",
            "Epoch 55/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 1.3462 - acc: 0.3700 - val_loss: 1.3847 - val_acc: 0.2950\n",
            "Epoch 56/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 1.3471 - acc: 0.3625 - val_loss: 1.3853 - val_acc: 0.3000\n",
            "Epoch 57/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 1.3566 - acc: 0.3700 - val_loss: 1.3834 - val_acc: 0.3100\n",
            "Epoch 58/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.3495 - acc: 0.3737 - val_loss: 1.3895 - val_acc: 0.2950\n",
            "Epoch 59/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 1.3490 - acc: 0.3550 - val_loss: 1.3797 - val_acc: 0.3200\n",
            "Epoch 60/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 1.3495 - acc: 0.3863 - val_loss: 1.3859 - val_acc: 0.2900\n",
            "Epoch 61/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.3583 - acc: 0.3488 - val_loss: 1.3773 - val_acc: 0.3050\n",
            "Epoch 62/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.3407 - acc: 0.3763 - val_loss: 1.3793 - val_acc: 0.2950\n",
            "Epoch 63/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 1.3464 - acc: 0.3663 - val_loss: 1.3778 - val_acc: 0.3200\n",
            "Epoch 64/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.3354 - acc: 0.3788 - val_loss: 1.3784 - val_acc: 0.3200\n",
            "Epoch 65/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.3370 - acc: 0.3763 - val_loss: 1.3844 - val_acc: 0.3100\n",
            "Epoch 66/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.3473 - acc: 0.3562 - val_loss: 1.3790 - val_acc: 0.3200\n",
            "Epoch 67/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.3393 - acc: 0.3613 - val_loss: 1.3813 - val_acc: 0.3100\n",
            "Epoch 68/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 1.3401 - acc: 0.3775 - val_loss: 1.3788 - val_acc: 0.3100\n",
            "Epoch 69/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.3382 - acc: 0.3650 - val_loss: 1.3814 - val_acc: 0.3100\n",
            "Epoch 70/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.3115 - acc: 0.3887 - val_loss: 1.3825 - val_acc: 0.3150\n",
            "Epoch 71/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.3344 - acc: 0.3800 - val_loss: 1.3903 - val_acc: 0.3150\n",
            "Epoch 72/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.3385 - acc: 0.3712 - val_loss: 1.3741 - val_acc: 0.3100\n",
            "Epoch 73/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.3297 - acc: 0.3775 - val_loss: 1.3747 - val_acc: 0.3200\n",
            "Epoch 74/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.3204 - acc: 0.3788 - val_loss: 1.3754 - val_acc: 0.3150\n",
            "Epoch 75/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.3253 - acc: 0.3863 - val_loss: 1.3752 - val_acc: 0.3150\n",
            "Epoch 76/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.3171 - acc: 0.3800 - val_loss: 1.3852 - val_acc: 0.3050\n",
            "Epoch 77/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 1.3251 - acc: 0.3812 - val_loss: 1.3881 - val_acc: 0.3100\n",
            "Epoch 78/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 1.3140 - acc: 0.3775 - val_loss: 1.3800 - val_acc: 0.3100\n",
            "Epoch 79/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.3107 - acc: 0.3750 - val_loss: 1.3778 - val_acc: 0.3000\n",
            "Epoch 80/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 1.3120 - acc: 0.3900 - val_loss: 1.3801 - val_acc: 0.3150\n",
            "Epoch 81/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.3119 - acc: 0.3675 - val_loss: 1.3710 - val_acc: 0.3000\n",
            "Epoch 82/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 1.3071 - acc: 0.3825 - val_loss: 1.3716 - val_acc: 0.3050\n",
            "Epoch 83/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 1.2963 - acc: 0.3763 - val_loss: 1.3812 - val_acc: 0.2900\n",
            "Epoch 84/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 1.3028 - acc: 0.3725 - val_loss: 1.3933 - val_acc: 0.3050\n",
            "Epoch 85/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.2920 - acc: 0.3887 - val_loss: 1.3678 - val_acc: 0.3100\n",
            "Epoch 86/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 1.2823 - acc: 0.3800 - val_loss: 1.3813 - val_acc: 0.2950\n",
            "Epoch 87/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 1.2781 - acc: 0.3987 - val_loss: 1.3602 - val_acc: 0.3250\n",
            "Epoch 88/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.3157 - acc: 0.3675 - val_loss: 1.3967 - val_acc: 0.2850\n",
            "Epoch 89/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 1.2771 - acc: 0.3887 - val_loss: 1.3895 - val_acc: 0.2900\n",
            "Epoch 90/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 1.2836 - acc: 0.3925 - val_loss: 1.3937 - val_acc: 0.2900\n",
            "Epoch 91/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.2770 - acc: 0.3875 - val_loss: 1.3890 - val_acc: 0.2900\n",
            "Epoch 92/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 1.2610 - acc: 0.3837 - val_loss: 1.3564 - val_acc: 0.2950\n",
            "Epoch 93/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.2642 - acc: 0.3837 - val_loss: 1.3596 - val_acc: 0.3200\n",
            "Epoch 94/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.2412 - acc: 0.4163 - val_loss: 1.3650 - val_acc: 0.3050\n",
            "Epoch 95/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 1.2507 - acc: 0.4100 - val_loss: 1.3614 - val_acc: 0.3250\n",
            "Epoch 96/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 1.2314 - acc: 0.4088 - val_loss: 1.4089 - val_acc: 0.2850\n",
            "Epoch 97/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 1.2492 - acc: 0.4175 - val_loss: 1.3747 - val_acc: 0.3100\n",
            "Epoch 98/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.2586 - acc: 0.3825 - val_loss: 1.3580 - val_acc: 0.3200\n",
            "Epoch 99/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 1.2451 - acc: 0.4175 - val_loss: 1.4212 - val_acc: 0.2900\n",
            "Epoch 100/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.2603 - acc: 0.3725 - val_loss: 1.3585 - val_acc: 0.3000\n",
            "Epoch 101/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.2336 - acc: 0.4262 - val_loss: 1.3677 - val_acc: 0.3050\n",
            "Epoch 102/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.2394 - acc: 0.4075 - val_loss: 1.4050 - val_acc: 0.2850\n",
            "Epoch 103/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.2317 - acc: 0.3975 - val_loss: 1.4107 - val_acc: 0.2850\n",
            "Epoch 104/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 1.2140 - acc: 0.4300 - val_loss: 1.3782 - val_acc: 0.3200\n",
            "Epoch 105/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.2074 - acc: 0.4275 - val_loss: 1.4202 - val_acc: 0.3250\n",
            "Epoch 106/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 1.2387 - acc: 0.4250 - val_loss: 1.3903 - val_acc: 0.3350\n",
            "Epoch 107/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 1.1774 - acc: 0.4450 - val_loss: 1.3999 - val_acc: 0.3200\n",
            "Epoch 108/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.1959 - acc: 0.4225 - val_loss: 1.3828 - val_acc: 0.3250\n",
            "Epoch 109/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 1.1715 - acc: 0.4462 - val_loss: 1.3954 - val_acc: 0.3500\n",
            "Epoch 110/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.1825 - acc: 0.4150 - val_loss: 1.4051 - val_acc: 0.3150\n",
            "Epoch 111/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.1734 - acc: 0.4375 - val_loss: 1.4183 - val_acc: 0.2950\n",
            "Epoch 112/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 1.1783 - acc: 0.4313 - val_loss: 1.4002 - val_acc: 0.3450\n",
            "Epoch 113/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.1353 - acc: 0.4437 - val_loss: 1.5347 - val_acc: 0.2650\n",
            "Epoch 114/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 1.2271 - acc: 0.3962 - val_loss: 1.4750 - val_acc: 0.3200\n",
            "Epoch 115/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.1926 - acc: 0.4187 - val_loss: 1.3925 - val_acc: 0.3650\n",
            "Epoch 116/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 1.1485 - acc: 0.4537 - val_loss: 1.3738 - val_acc: 0.3700\n",
            "Epoch 117/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 1.1404 - acc: 0.4650 - val_loss: 1.4306 - val_acc: 0.3400\n",
            "Epoch 118/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.1257 - acc: 0.4675 - val_loss: 1.4237 - val_acc: 0.3700\n",
            "Epoch 119/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 1.1352 - acc: 0.4700 - val_loss: 1.4534 - val_acc: 0.3600\n",
            "Epoch 120/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 1.1267 - acc: 0.4313 - val_loss: 1.4304 - val_acc: 0.3600\n",
            "Epoch 121/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.1050 - acc: 0.4400 - val_loss: 1.4602 - val_acc: 0.3550\n",
            "Epoch 122/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 1.1345 - acc: 0.4213 - val_loss: 1.4459 - val_acc: 0.3700\n",
            "Epoch 123/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 1.1354 - acc: 0.4450 - val_loss: 1.3970 - val_acc: 0.3700\n",
            "Epoch 124/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 1.0887 - acc: 0.4475 - val_loss: 1.3989 - val_acc: 0.3600\n",
            "Epoch 125/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.1237 - acc: 0.4388 - val_loss: 1.4262 - val_acc: 0.3750\n",
            "Epoch 126/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 1.1024 - acc: 0.4362 - val_loss: 1.4211 - val_acc: 0.3700\n",
            "Epoch 127/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 1.0878 - acc: 0.4637 - val_loss: 1.4538 - val_acc: 0.3250\n",
            "Epoch 128/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.0667 - acc: 0.4512 - val_loss: 1.5202 - val_acc: 0.2750\n",
            "Epoch 129/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.0843 - acc: 0.4662 - val_loss: 1.5196 - val_acc: 0.3050\n",
            "Epoch 130/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 1.1149 - acc: 0.4225 - val_loss: 1.4797 - val_acc: 0.3400\n",
            "Epoch 131/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 1.1226 - acc: 0.4675 - val_loss: 1.4218 - val_acc: 0.3650\n",
            "Epoch 132/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.1228 - acc: 0.4163 - val_loss: 1.3911 - val_acc: 0.3800\n",
            "Epoch 133/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.0706 - acc: 0.4775 - val_loss: 1.4165 - val_acc: 0.3400\n",
            "Epoch 134/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 1.0828 - acc: 0.4200 - val_loss: 1.4536 - val_acc: 0.3700\n",
            "Epoch 135/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 1.0677 - acc: 0.4675 - val_loss: 1.4718 - val_acc: 0.3500\n",
            "Epoch 136/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.0263 - acc: 0.4962 - val_loss: 1.5097 - val_acc: 0.3400\n",
            "Epoch 137/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.0359 - acc: 0.4750 - val_loss: 1.5373 - val_acc: 0.3350\n",
            "Epoch 138/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 1.0615 - acc: 0.4575 - val_loss: 1.4754 - val_acc: 0.3750\n",
            "Epoch 139/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 1.0283 - acc: 0.4600 - val_loss: 1.4824 - val_acc: 0.3600\n",
            "Epoch 140/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 1.0775 - acc: 0.4475 - val_loss: 1.5260 - val_acc: 0.3400\n",
            "Epoch 141/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 1.0892 - acc: 0.4713 - val_loss: 1.4074 - val_acc: 0.3800\n",
            "Epoch 142/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 1.1029 - acc: 0.4500 - val_loss: 1.5747 - val_acc: 0.3500\n",
            "Epoch 143/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.1137 - acc: 0.4113 - val_loss: 1.4688 - val_acc: 0.3400\n",
            "Epoch 144/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 1.1258 - acc: 0.4113 - val_loss: 1.4218 - val_acc: 0.3300\n",
            "Epoch 145/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 1.1071 - acc: 0.4388 - val_loss: 1.4021 - val_acc: 0.3900\n",
            "Epoch 146/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.0701 - acc: 0.4875 - val_loss: 1.4437 - val_acc: 0.3500\n",
            "Epoch 147/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.0557 - acc: 0.4475 - val_loss: 1.4505 - val_acc: 0.3600\n",
            "Epoch 148/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 1.0564 - acc: 0.4512 - val_loss: 1.4375 - val_acc: 0.3750\n",
            "Epoch 149/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 1.0113 - acc: 0.4812 - val_loss: 1.5100 - val_acc: 0.3350\n",
            "Epoch 150/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 1.0231 - acc: 0.4863 - val_loss: 1.4933 - val_acc: 0.3650\n",
            "Epoch 151/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 1.0230 - acc: 0.4675 - val_loss: 1.4735 - val_acc: 0.3850\n",
            "Epoch 152/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.9718 - acc: 0.5138 - val_loss: 1.5043 - val_acc: 0.3850\n",
            "Epoch 153/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.9805 - acc: 0.5038 - val_loss: 1.5139 - val_acc: 0.4000\n",
            "Epoch 154/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.9865 - acc: 0.4488 - val_loss: 1.5508 - val_acc: 0.3850\n",
            "Epoch 155/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.9998 - acc: 0.4887 - val_loss: 1.5439 - val_acc: 0.3700\n",
            "Epoch 156/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.0459 - acc: 0.4537 - val_loss: 1.4770 - val_acc: 0.3750\n",
            "Epoch 157/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 1.0358 - acc: 0.4500 - val_loss: 1.4262 - val_acc: 0.3850\n",
            "Epoch 158/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.9971 - acc: 0.4900 - val_loss: 1.4777 - val_acc: 0.3950\n",
            "Epoch 159/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 1.0029 - acc: 0.4563 - val_loss: 1.5094 - val_acc: 0.3950\n",
            "Epoch 160/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 1.0125 - acc: 0.4863 - val_loss: 1.5361 - val_acc: 0.3400\n",
            "Epoch 161/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 1.0144 - acc: 0.4475 - val_loss: 1.6142 - val_acc: 0.3350\n",
            "Epoch 162/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.9882 - acc: 0.4875 - val_loss: 1.5287 - val_acc: 0.3400\n",
            "Epoch 163/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.9675 - acc: 0.4863 - val_loss: 1.5266 - val_acc: 0.3700\n",
            "Epoch 164/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.9541 - acc: 0.4925 - val_loss: 1.5740 - val_acc: 0.3600\n",
            "Epoch 165/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.9274 - acc: 0.4863 - val_loss: 1.6222 - val_acc: 0.3500\n",
            "Epoch 166/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.9475 - acc: 0.5038 - val_loss: 1.6446 - val_acc: 0.3500\n",
            "Epoch 167/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 1.0109 - acc: 0.4688 - val_loss: 1.6146 - val_acc: 0.3450\n",
            "Epoch 168/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 1.0308 - acc: 0.4500 - val_loss: 1.5271 - val_acc: 0.3700\n",
            "Epoch 169/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.9896 - acc: 0.4975 - val_loss: 1.4579 - val_acc: 0.3650\n",
            "Epoch 170/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.9801 - acc: 0.4775 - val_loss: 1.5517 - val_acc: 0.3250\n",
            "Epoch 171/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 1.0257 - acc: 0.4637 - val_loss: 1.4852 - val_acc: 0.3750\n",
            "Epoch 172/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 1.0094 - acc: 0.4675 - val_loss: 1.4911 - val_acc: 0.3700\n",
            "Epoch 173/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 1.0078 - acc: 0.4587 - val_loss: 1.4883 - val_acc: 0.3550\n",
            "Epoch 174/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.9574 - acc: 0.4850 - val_loss: 1.4593 - val_acc: 0.4150\n",
            "Epoch 175/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.9462 - acc: 0.4863 - val_loss: 1.5218 - val_acc: 0.3750\n",
            "Epoch 176/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.9596 - acc: 0.4812 - val_loss: 1.6306 - val_acc: 0.3450\n",
            "Epoch 177/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.9439 - acc: 0.4950 - val_loss: 1.6050 - val_acc: 0.3450\n",
            "Epoch 178/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.9558 - acc: 0.4838 - val_loss: 1.5289 - val_acc: 0.3450\n",
            "Epoch 179/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.9368 - acc: 0.4775 - val_loss: 1.5416 - val_acc: 0.3700\n",
            "Epoch 180/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.9378 - acc: 0.4875 - val_loss: 1.6321 - val_acc: 0.3500\n",
            "Epoch 181/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.9558 - acc: 0.5062 - val_loss: 1.5982 - val_acc: 0.3450\n",
            "Epoch 182/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.9385 - acc: 0.4863 - val_loss: 1.5894 - val_acc: 0.3600\n",
            "Epoch 183/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.9251 - acc: 0.4962 - val_loss: 1.5919 - val_acc: 0.3500\n",
            "Epoch 184/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.9329 - acc: 0.4612 - val_loss: 1.6045 - val_acc: 0.3400\n",
            "Epoch 185/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.9299 - acc: 0.4775 - val_loss: 1.5502 - val_acc: 0.3350\n",
            "Epoch 186/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.9670 - acc: 0.4675 - val_loss: 1.5366 - val_acc: 0.3650\n",
            "Epoch 187/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.9529 - acc: 0.4925 - val_loss: 1.5510 - val_acc: 0.3650\n",
            "Epoch 188/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.9513 - acc: 0.4688 - val_loss: 1.5065 - val_acc: 0.3450\n",
            "Epoch 189/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.9557 - acc: 0.4838 - val_loss: 1.5250 - val_acc: 0.3700\n",
            "Epoch 190/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.9439 - acc: 0.4763 - val_loss: 1.5381 - val_acc: 0.3600\n",
            "Epoch 191/1000\n",
            "800/800 [==============================] - 0s 278us/step - loss: 0.9000 - acc: 0.4887 - val_loss: 1.5790 - val_acc: 0.3650\n",
            "Epoch 192/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.9017 - acc: 0.4913 - val_loss: 1.5757 - val_acc: 0.4250\n",
            "Epoch 193/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.8874 - acc: 0.5125 - val_loss: 1.6858 - val_acc: 0.3500\n",
            "Epoch 194/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.9237 - acc: 0.4788 - val_loss: 1.6133 - val_acc: 0.3850\n",
            "Epoch 195/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.9197 - acc: 0.4863 - val_loss: 1.5640 - val_acc: 0.4250\n",
            "Epoch 196/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.9160 - acc: 0.4838 - val_loss: 1.5250 - val_acc: 0.3900\n",
            "Epoch 197/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.9064 - acc: 0.4975 - val_loss: 1.5692 - val_acc: 0.3700\n",
            "Epoch 198/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.9340 - acc: 0.4625 - val_loss: 1.5919 - val_acc: 0.3650\n",
            "Epoch 199/1000\n",
            "800/800 [==============================] - 0s 247us/step - loss: 0.8874 - acc: 0.5162 - val_loss: 1.6174 - val_acc: 0.3550\n",
            "Epoch 200/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.8749 - acc: 0.4938 - val_loss: 1.6326 - val_acc: 0.3650\n",
            "Epoch 201/1000\n",
            "800/800 [==============================] - 0s 287us/step - loss: 0.9014 - acc: 0.4763 - val_loss: 1.6400 - val_acc: 0.3800\n",
            "Epoch 202/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.8659 - acc: 0.5337 - val_loss: 1.6708 - val_acc: 0.3850\n",
            "Epoch 203/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8943 - acc: 0.4788 - val_loss: 1.6841 - val_acc: 0.3600\n",
            "Epoch 204/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.8848 - acc: 0.5062 - val_loss: 1.7105 - val_acc: 0.3900\n",
            "Epoch 205/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.9048 - acc: 0.4838 - val_loss: 1.6221 - val_acc: 0.3750\n",
            "Epoch 206/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8827 - acc: 0.4900 - val_loss: 1.6186 - val_acc: 0.3950\n",
            "Epoch 207/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.8793 - acc: 0.4763 - val_loss: 1.7181 - val_acc: 0.3700\n",
            "Epoch 208/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.9064 - acc: 0.4863 - val_loss: 1.7454 - val_acc: 0.3850\n",
            "Epoch 209/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.8819 - acc: 0.4938 - val_loss: 1.7332 - val_acc: 0.3950\n",
            "Epoch 210/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.8870 - acc: 0.4987 - val_loss: 1.5896 - val_acc: 0.3900\n",
            "Epoch 211/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.8875 - acc: 0.4875 - val_loss: 1.6201 - val_acc: 0.3700\n",
            "Epoch 212/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.8705 - acc: 0.5150 - val_loss: 1.6445 - val_acc: 0.3750\n",
            "Epoch 213/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.8453 - acc: 0.5112 - val_loss: 1.6207 - val_acc: 0.3650\n",
            "Epoch 214/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.8738 - acc: 0.5075 - val_loss: 1.6331 - val_acc: 0.3900\n",
            "Epoch 215/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.8362 - acc: 0.5375 - val_loss: 1.6600 - val_acc: 0.3800\n",
            "Epoch 216/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.8243 - acc: 0.5125 - val_loss: 1.7241 - val_acc: 0.3800\n",
            "Epoch 217/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.8537 - acc: 0.4725 - val_loss: 1.8002 - val_acc: 0.3650\n",
            "Epoch 218/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.8562 - acc: 0.4800 - val_loss: 1.7676 - val_acc: 0.3950\n",
            "Epoch 219/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.8515 - acc: 0.4900 - val_loss: 1.6582 - val_acc: 0.3950\n",
            "Epoch 220/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.8369 - acc: 0.5138 - val_loss: 1.6855 - val_acc: 0.4000\n",
            "Epoch 221/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.8606 - acc: 0.4825 - val_loss: 1.6326 - val_acc: 0.4050\n",
            "Epoch 222/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.8838 - acc: 0.4800 - val_loss: 1.6562 - val_acc: 0.4150\n",
            "Epoch 223/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.9093 - acc: 0.4950 - val_loss: 1.6430 - val_acc: 0.3850\n",
            "Epoch 224/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.9144 - acc: 0.4612 - val_loss: 1.5992 - val_acc: 0.4000\n",
            "Epoch 225/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.8569 - acc: 0.5225 - val_loss: 1.6822 - val_acc: 0.4050\n",
            "Epoch 226/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.8456 - acc: 0.5188 - val_loss: 1.7584 - val_acc: 0.3750\n",
            "Epoch 227/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.8394 - acc: 0.5138 - val_loss: 1.7460 - val_acc: 0.3750\n",
            "Epoch 228/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 0.8423 - acc: 0.5200 - val_loss: 1.7054 - val_acc: 0.3900\n",
            "Epoch 229/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.8405 - acc: 0.4913 - val_loss: 1.7444 - val_acc: 0.4050\n",
            "Epoch 230/1000\n",
            "800/800 [==============================] - 0s 287us/step - loss: 0.8638 - acc: 0.4800 - val_loss: 1.8705 - val_acc: 0.3800\n",
            "Epoch 231/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.8670 - acc: 0.5025 - val_loss: 1.8080 - val_acc: 0.3700\n",
            "Epoch 232/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.8560 - acc: 0.4950 - val_loss: 1.6412 - val_acc: 0.3850\n",
            "Epoch 233/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.8637 - acc: 0.4650 - val_loss: 1.6189 - val_acc: 0.3900\n",
            "Epoch 234/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.8689 - acc: 0.5087 - val_loss: 1.6539 - val_acc: 0.3950\n",
            "Epoch 235/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.8767 - acc: 0.4738 - val_loss: 1.6389 - val_acc: 0.3800\n",
            "Epoch 236/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.8399 - acc: 0.4913 - val_loss: 1.7407 - val_acc: 0.3800\n",
            "Epoch 237/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8281 - acc: 0.5162 - val_loss: 1.7494 - val_acc: 0.3700\n",
            "Epoch 238/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8522 - acc: 0.4850 - val_loss: 1.6804 - val_acc: 0.3800\n",
            "Epoch 239/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.8123 - acc: 0.4987 - val_loss: 1.7851 - val_acc: 0.3550\n",
            "Epoch 240/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.8278 - acc: 0.4975 - val_loss: 1.9155 - val_acc: 0.3800\n",
            "Epoch 241/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.8624 - acc: 0.4900 - val_loss: 1.8739 - val_acc: 0.3850\n",
            "Epoch 242/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.8349 - acc: 0.5138 - val_loss: 1.7789 - val_acc: 0.4150\n",
            "Epoch 243/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.8429 - acc: 0.5025 - val_loss: 1.7332 - val_acc: 0.4050\n",
            "Epoch 244/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.8261 - acc: 0.5212 - val_loss: 1.7060 - val_acc: 0.4100\n",
            "Epoch 245/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.8286 - acc: 0.5075 - val_loss: 1.7118 - val_acc: 0.3650\n",
            "Epoch 246/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.8248 - acc: 0.4825 - val_loss: 1.7680 - val_acc: 0.3800\n",
            "Epoch 247/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.8106 - acc: 0.5125 - val_loss: 1.7913 - val_acc: 0.3750\n",
            "Epoch 248/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.8056 - acc: 0.5200 - val_loss: 1.8307 - val_acc: 0.3750\n",
            "Epoch 249/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.8216 - acc: 0.5300 - val_loss: 1.8362 - val_acc: 0.3750\n",
            "Epoch 250/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8233 - acc: 0.5100 - val_loss: 1.8763 - val_acc: 0.3950\n",
            "Epoch 251/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.8301 - acc: 0.4738 - val_loss: 1.8347 - val_acc: 0.3550\n",
            "Epoch 252/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.8266 - acc: 0.4825 - val_loss: 1.7685 - val_acc: 0.3750\n",
            "Epoch 253/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.8041 - acc: 0.5325 - val_loss: 1.8537 - val_acc: 0.3850\n",
            "Epoch 254/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.8377 - acc: 0.5000 - val_loss: 1.8588 - val_acc: 0.4000\n",
            "Epoch 255/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.8203 - acc: 0.4875 - val_loss: 1.9267 - val_acc: 0.3750\n",
            "Epoch 256/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8303 - acc: 0.4863 - val_loss: 1.7683 - val_acc: 0.4050\n",
            "Epoch 257/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.8155 - acc: 0.5162 - val_loss: 1.7480 - val_acc: 0.4000\n",
            "Epoch 258/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.8164 - acc: 0.4900 - val_loss: 1.8018 - val_acc: 0.3900\n",
            "Epoch 259/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.8312 - acc: 0.5350 - val_loss: 1.7813 - val_acc: 0.4150\n",
            "Epoch 260/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 0.8112 - acc: 0.5188 - val_loss: 2.0078 - val_acc: 0.4100\n",
            "Epoch 261/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.8450 - acc: 0.5062 - val_loss: 2.0209 - val_acc: 0.3600\n",
            "Epoch 262/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.8103 - acc: 0.5262 - val_loss: 1.6974 - val_acc: 0.4400\n",
            "Epoch 263/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8080 - acc: 0.5087 - val_loss: 1.7372 - val_acc: 0.3850\n",
            "Epoch 264/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8041 - acc: 0.5325 - val_loss: 1.7374 - val_acc: 0.4300\n",
            "Epoch 265/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.7954 - acc: 0.5388 - val_loss: 1.9627 - val_acc: 0.3950\n",
            "Epoch 266/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.7912 - acc: 0.5200 - val_loss: 1.8553 - val_acc: 0.3950\n",
            "Epoch 267/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.7868 - acc: 0.5300 - val_loss: 1.8349 - val_acc: 0.3900\n",
            "Epoch 268/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.7857 - acc: 0.5312 - val_loss: 1.8867 - val_acc: 0.3650\n",
            "Epoch 269/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.7981 - acc: 0.5025 - val_loss: 2.0295 - val_acc: 0.3800\n",
            "Epoch 270/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.7846 - acc: 0.5175 - val_loss: 1.8432 - val_acc: 0.3850\n",
            "Epoch 271/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.8182 - acc: 0.4825 - val_loss: 1.9368 - val_acc: 0.3900\n",
            "Epoch 272/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.8087 - acc: 0.4987 - val_loss: 1.9239 - val_acc: 0.4100\n",
            "Epoch 273/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.7977 - acc: 0.5175 - val_loss: 1.8381 - val_acc: 0.3750\n",
            "Epoch 274/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.7836 - acc: 0.5388 - val_loss: 1.8807 - val_acc: 0.4200\n",
            "Epoch 275/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.8243 - acc: 0.5100 - val_loss: 2.0123 - val_acc: 0.4100\n",
            "Epoch 276/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.8623 - acc: 0.5150 - val_loss: 1.9688 - val_acc: 0.3850\n",
            "Epoch 277/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.8690 - acc: 0.5150 - val_loss: 1.8734 - val_acc: 0.4100\n",
            "Epoch 278/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.8338 - acc: 0.4900 - val_loss: 1.7140 - val_acc: 0.4000\n",
            "Epoch 279/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.8317 - acc: 0.4962 - val_loss: 1.7752 - val_acc: 0.3900\n",
            "Epoch 280/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.8103 - acc: 0.5288 - val_loss: 1.7623 - val_acc: 0.3900\n",
            "Epoch 281/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.7885 - acc: 0.5463 - val_loss: 1.9127 - val_acc: 0.4100\n",
            "Epoch 282/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.7996 - acc: 0.5012 - val_loss: 1.8670 - val_acc: 0.3700\n",
            "Epoch 283/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.7914 - acc: 0.4900 - val_loss: 1.8374 - val_acc: 0.3650\n",
            "Epoch 284/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.7883 - acc: 0.5012 - val_loss: 1.8408 - val_acc: 0.4050\n",
            "Epoch 285/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.7894 - acc: 0.5300 - val_loss: 1.9364 - val_acc: 0.3950\n",
            "Epoch 286/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.8083 - acc: 0.4863 - val_loss: 1.8178 - val_acc: 0.3700\n",
            "Epoch 287/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.8097 - acc: 0.5587 - val_loss: 1.8179 - val_acc: 0.3650\n",
            "Epoch 288/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.7988 - acc: 0.5250 - val_loss: 1.8113 - val_acc: 0.4000\n",
            "Epoch 289/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.8083 - acc: 0.5300 - val_loss: 2.0694 - val_acc: 0.3800\n",
            "Epoch 290/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.7725 - acc: 0.5563 - val_loss: 2.0059 - val_acc: 0.4200\n",
            "Epoch 291/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.8055 - acc: 0.5075 - val_loss: 2.0656 - val_acc: 0.4000\n",
            "Epoch 292/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.7991 - acc: 0.5138 - val_loss: 1.8899 - val_acc: 0.3750\n",
            "Epoch 293/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.7785 - acc: 0.5375 - val_loss: 1.8650 - val_acc: 0.4050\n",
            "Epoch 294/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.7966 - acc: 0.5262 - val_loss: 1.8081 - val_acc: 0.4150\n",
            "Epoch 295/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.7803 - acc: 0.5388 - val_loss: 1.7825 - val_acc: 0.4000\n",
            "Epoch 296/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.7873 - acc: 0.5200 - val_loss: 1.7839 - val_acc: 0.4150\n",
            "Epoch 297/1000\n",
            "800/800 [==============================] - 0s 284us/step - loss: 0.7713 - acc: 0.5587 - val_loss: 1.7969 - val_acc: 0.3700\n",
            "Epoch 298/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.7949 - acc: 0.5325 - val_loss: 1.8549 - val_acc: 0.3900\n",
            "Epoch 299/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.7988 - acc: 0.5012 - val_loss: 1.9278 - val_acc: 0.4200\n",
            "Epoch 300/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.7688 - acc: 0.5125 - val_loss: 1.8410 - val_acc: 0.3750\n",
            "Epoch 301/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.7957 - acc: 0.5288 - val_loss: 1.8787 - val_acc: 0.4000\n",
            "Epoch 302/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.7872 - acc: 0.5175 - val_loss: 2.0013 - val_acc: 0.4150\n",
            "Epoch 303/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.7693 - acc: 0.5450 - val_loss: 1.9142 - val_acc: 0.4050\n",
            "Epoch 304/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.7391 - acc: 0.5538 - val_loss: 1.9126 - val_acc: 0.3750\n",
            "Epoch 305/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 0.7475 - acc: 0.5200 - val_loss: 2.0621 - val_acc: 0.4200\n",
            "Epoch 306/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.7642 - acc: 0.5225 - val_loss: 1.9715 - val_acc: 0.3750\n",
            "Epoch 307/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.7565 - acc: 0.5325 - val_loss: 2.0506 - val_acc: 0.3800\n",
            "Epoch 308/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.7591 - acc: 0.5237 - val_loss: 1.9696 - val_acc: 0.3850\n",
            "Epoch 309/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.7470 - acc: 0.5563 - val_loss: 1.9365 - val_acc: 0.3900\n",
            "Epoch 310/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.7866 - acc: 0.5250 - val_loss: 1.8945 - val_acc: 0.4150\n",
            "Epoch 311/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.7605 - acc: 0.5450 - val_loss: 2.0946 - val_acc: 0.3850\n",
            "Epoch 312/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.8048 - acc: 0.5288 - val_loss: 1.7898 - val_acc: 0.3850\n",
            "Epoch 313/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.7768 - acc: 0.5413 - val_loss: 2.0417 - val_acc: 0.3850\n",
            "Epoch 314/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.8072 - acc: 0.5237 - val_loss: 2.0675 - val_acc: 0.3750\n",
            "Epoch 315/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.8003 - acc: 0.5312 - val_loss: 2.1840 - val_acc: 0.3600\n",
            "Epoch 316/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.8215 - acc: 0.5262 - val_loss: 1.8884 - val_acc: 0.3650\n",
            "Epoch 317/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.8356 - acc: 0.5237 - val_loss: 1.8330 - val_acc: 0.4050\n",
            "Epoch 318/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.8003 - acc: 0.5538 - val_loss: 1.7985 - val_acc: 0.3600\n",
            "Epoch 319/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.7614 - acc: 0.5575 - val_loss: 1.8810 - val_acc: 0.4250\n",
            "Epoch 320/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.7586 - acc: 0.5575 - val_loss: 1.9735 - val_acc: 0.4100\n",
            "Epoch 321/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.7669 - acc: 0.5463 - val_loss: 1.9555 - val_acc: 0.3900\n",
            "Epoch 322/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.8020 - acc: 0.5538 - val_loss: 1.8844 - val_acc: 0.4050\n",
            "Epoch 323/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.7740 - acc: 0.5112 - val_loss: 1.9129 - val_acc: 0.4000\n",
            "Epoch 324/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.7591 - acc: 0.5237 - val_loss: 1.8949 - val_acc: 0.4100\n",
            "Epoch 325/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.7475 - acc: 0.5637 - val_loss: 1.9025 - val_acc: 0.3900\n",
            "Epoch 326/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.7621 - acc: 0.5337 - val_loss: 1.8998 - val_acc: 0.3950\n",
            "Epoch 327/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.7556 - acc: 0.5487 - val_loss: 2.0124 - val_acc: 0.4150\n",
            "Epoch 328/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.7469 - acc: 0.5388 - val_loss: 1.8890 - val_acc: 0.4150\n",
            "Epoch 329/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.7512 - acc: 0.5463 - val_loss: 1.8926 - val_acc: 0.4200\n",
            "Epoch 330/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.7493 - acc: 0.5675 - val_loss: 2.0922 - val_acc: 0.4350\n",
            "Epoch 331/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 0.7593 - acc: 0.5312 - val_loss: 2.0217 - val_acc: 0.4250\n",
            "Epoch 332/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.7547 - acc: 0.5475 - val_loss: 1.9522 - val_acc: 0.4050\n",
            "Epoch 333/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.7477 - acc: 0.5388 - val_loss: 1.9399 - val_acc: 0.4150\n",
            "Epoch 334/1000\n",
            "800/800 [==============================] - 0s 281us/step - loss: 0.7638 - acc: 0.5150 - val_loss: 1.9496 - val_acc: 0.4150\n",
            "Epoch 335/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.7529 - acc: 0.5538 - val_loss: 1.9271 - val_acc: 0.3950\n",
            "Epoch 336/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.7275 - acc: 0.5737 - val_loss: 1.8980 - val_acc: 0.4000\n",
            "Epoch 337/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.7272 - acc: 0.5538 - val_loss: 2.0036 - val_acc: 0.3950\n",
            "Epoch 338/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.7394 - acc: 0.5450 - val_loss: 2.1162 - val_acc: 0.3950\n",
            "Epoch 339/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.7426 - acc: 0.5550 - val_loss: 2.3325 - val_acc: 0.3800\n",
            "Epoch 340/1000\n",
            "800/800 [==============================] - 0s 280us/step - loss: 0.7867 - acc: 0.5613 - val_loss: 2.2676 - val_acc: 0.4050\n",
            "Epoch 341/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.7363 - acc: 0.5837 - val_loss: 1.9315 - val_acc: 0.4000\n",
            "Epoch 342/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.7598 - acc: 0.5475 - val_loss: 1.9521 - val_acc: 0.3850\n",
            "Epoch 343/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.7324 - acc: 0.5962 - val_loss: 2.0217 - val_acc: 0.4000\n",
            "Epoch 344/1000\n",
            "800/800 [==============================] - 0s 284us/step - loss: 0.7417 - acc: 0.5775 - val_loss: 2.2700 - val_acc: 0.3900\n",
            "Epoch 345/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.7482 - acc: 0.5650 - val_loss: 2.0885 - val_acc: 0.4350\n",
            "Epoch 346/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.7259 - acc: 0.5675 - val_loss: 2.0019 - val_acc: 0.3900\n",
            "Epoch 347/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.7163 - acc: 0.6000 - val_loss: 2.0031 - val_acc: 0.4000\n",
            "Epoch 348/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.7306 - acc: 0.5800 - val_loss: 2.1995 - val_acc: 0.4100\n",
            "Epoch 349/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.7295 - acc: 0.5925 - val_loss: 2.0507 - val_acc: 0.4300\n",
            "Epoch 350/1000\n",
            "800/800 [==============================] - 0s 282us/step - loss: 0.7179 - acc: 0.6050 - val_loss: 2.0274 - val_acc: 0.4250\n",
            "Epoch 351/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.7187 - acc: 0.5837 - val_loss: 2.1647 - val_acc: 0.4300\n",
            "Epoch 352/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.7304 - acc: 0.5725 - val_loss: 2.2843 - val_acc: 0.4650\n",
            "Epoch 353/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 0.7323 - acc: 0.5887 - val_loss: 1.9799 - val_acc: 0.4100\n",
            "Epoch 354/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.7216 - acc: 0.5938 - val_loss: 1.9507 - val_acc: 0.3950\n",
            "Epoch 355/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.7131 - acc: 0.6025 - val_loss: 2.1801 - val_acc: 0.3800\n",
            "Epoch 356/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.7201 - acc: 0.6038 - val_loss: 1.9841 - val_acc: 0.4150\n",
            "Epoch 357/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.7139 - acc: 0.5850 - val_loss: 2.1760 - val_acc: 0.4350\n",
            "Epoch 358/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.7152 - acc: 0.5975 - val_loss: 2.4430 - val_acc: 0.4450\n",
            "Epoch 359/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.7966 - acc: 0.5575 - val_loss: 2.3556 - val_acc: 0.3900\n",
            "Epoch 360/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.7788 - acc: 0.5525 - val_loss: 2.0818 - val_acc: 0.4300\n",
            "Epoch 361/1000\n",
            "800/800 [==============================] - 0s 245us/step - loss: 0.7487 - acc: 0.5900 - val_loss: 2.1220 - val_acc: 0.4250\n",
            "Epoch 362/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.7075 - acc: 0.6187 - val_loss: 2.5125 - val_acc: 0.4100\n",
            "Epoch 363/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.7519 - acc: 0.6013 - val_loss: 2.2638 - val_acc: 0.4350\n",
            "Epoch 364/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.7178 - acc: 0.6262 - val_loss: 1.9888 - val_acc: 0.4250\n",
            "Epoch 365/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.7288 - acc: 0.6100 - val_loss: 1.9726 - val_acc: 0.4150\n",
            "Epoch 366/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.6664 - acc: 0.6700 - val_loss: 2.0546 - val_acc: 0.4150\n",
            "Epoch 367/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.7187 - acc: 0.5962 - val_loss: 2.0902 - val_acc: 0.4450\n",
            "Epoch 368/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.7050 - acc: 0.6163 - val_loss: 2.0636 - val_acc: 0.3750\n",
            "Epoch 369/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.7037 - acc: 0.6275 - val_loss: 2.0896 - val_acc: 0.4200\n",
            "Epoch 370/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.6793 - acc: 0.6412 - val_loss: 2.0826 - val_acc: 0.4100\n",
            "Epoch 371/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 0.6678 - acc: 0.6450 - val_loss: 2.1002 - val_acc: 0.4350\n",
            "Epoch 372/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.7052 - acc: 0.6300 - val_loss: 1.9812 - val_acc: 0.3950\n",
            "Epoch 373/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.6691 - acc: 0.6588 - val_loss: 2.1887 - val_acc: 0.4300\n",
            "Epoch 374/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.6465 - acc: 0.6950 - val_loss: 2.3370 - val_acc: 0.4650\n",
            "Epoch 375/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.6558 - acc: 0.6713 - val_loss: 2.1832 - val_acc: 0.4450\n",
            "Epoch 376/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.6484 - acc: 0.6700 - val_loss: 2.3193 - val_acc: 0.4100\n",
            "Epoch 377/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.6921 - acc: 0.6575 - val_loss: 2.0443 - val_acc: 0.4300\n",
            "Epoch 378/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.6701 - acc: 0.6438 - val_loss: 2.1642 - val_acc: 0.4250\n",
            "Epoch 379/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.6782 - acc: 0.6525 - val_loss: 2.2267 - val_acc: 0.4400\n",
            "Epoch 380/1000\n",
            "800/800 [==============================] - 0s 247us/step - loss: 0.6429 - acc: 0.6887 - val_loss: 2.2237 - val_acc: 0.4400\n",
            "Epoch 381/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.6893 - acc: 0.6412 - val_loss: 2.1600 - val_acc: 0.4250\n",
            "Epoch 382/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.6556 - acc: 0.6875 - val_loss: 2.2002 - val_acc: 0.4000\n",
            "Epoch 383/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.6642 - acc: 0.6613 - val_loss: 2.2426 - val_acc: 0.4050\n",
            "Epoch 384/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.6710 - acc: 0.6625 - val_loss: 2.2310 - val_acc: 0.4150\n",
            "Epoch 385/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.7232 - acc: 0.6487 - val_loss: 2.2609 - val_acc: 0.4400\n",
            "Epoch 386/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.7441 - acc: 0.6238 - val_loss: 2.2108 - val_acc: 0.4600\n",
            "Epoch 387/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.6871 - acc: 0.6588 - val_loss: 2.2067 - val_acc: 0.4100\n",
            "Epoch 388/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.6522 - acc: 0.6913 - val_loss: 2.2167 - val_acc: 0.4100\n",
            "Epoch 389/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.6194 - acc: 0.7213 - val_loss: 2.2420 - val_acc: 0.4300\n",
            "Epoch 390/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.6199 - acc: 0.7250 - val_loss: 2.1920 - val_acc: 0.4000\n",
            "Epoch 391/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.6678 - acc: 0.6900 - val_loss: 2.3249 - val_acc: 0.4350\n",
            "Epoch 392/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.6132 - acc: 0.7175 - val_loss: 2.4340 - val_acc: 0.4500\n",
            "Epoch 393/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.6171 - acc: 0.7275 - val_loss: 2.3305 - val_acc: 0.4200\n",
            "Epoch 394/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.6114 - acc: 0.7200 - val_loss: 2.3776 - val_acc: 0.4100\n",
            "Epoch 395/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.6398 - acc: 0.7125 - val_loss: 2.2950 - val_acc: 0.4000\n",
            "Epoch 396/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.6139 - acc: 0.7137 - val_loss: 2.3631 - val_acc: 0.4200\n",
            "Epoch 397/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.6090 - acc: 0.7375 - val_loss: 2.4069 - val_acc: 0.4300\n",
            "Epoch 398/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.5752 - acc: 0.7450 - val_loss: 2.5278 - val_acc: 0.4650\n",
            "Epoch 399/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.5519 - acc: 0.7450 - val_loss: 2.8281 - val_acc: 0.4400\n",
            "Epoch 400/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 0.6342 - acc: 0.7388 - val_loss: 2.6364 - val_acc: 0.4350\n",
            "Epoch 401/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.5986 - acc: 0.7413 - val_loss: 2.4636 - val_acc: 0.4250\n",
            "Epoch 402/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.5968 - acc: 0.7350 - val_loss: 2.3285 - val_acc: 0.4000\n",
            "Epoch 403/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.6254 - acc: 0.6975 - val_loss: 2.5231 - val_acc: 0.4600\n",
            "Epoch 404/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.5850 - acc: 0.7400 - val_loss: 2.2990 - val_acc: 0.4200\n",
            "Epoch 405/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.5777 - acc: 0.7587 - val_loss: 2.4439 - val_acc: 0.4200\n",
            "Epoch 406/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.6051 - acc: 0.7263 - val_loss: 2.3728 - val_acc: 0.4050\n",
            "Epoch 407/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.5912 - acc: 0.7538 - val_loss: 2.7689 - val_acc: 0.4250\n",
            "Epoch 408/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.6059 - acc: 0.7425 - val_loss: 2.2469 - val_acc: 0.4000\n",
            "Epoch 409/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.6669 - acc: 0.7037 - val_loss: 2.3581 - val_acc: 0.4250\n",
            "Epoch 410/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.7065 - acc: 0.6975 - val_loss: 2.2762 - val_acc: 0.4600\n",
            "Epoch 411/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.6209 - acc: 0.7450 - val_loss: 2.3348 - val_acc: 0.4500\n",
            "Epoch 412/1000\n",
            "800/800 [==============================] - 0s 282us/step - loss: 0.5244 - acc: 0.7712 - val_loss: 2.1160 - val_acc: 0.4700\n",
            "Epoch 413/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.5226 - acc: 0.7963 - val_loss: 2.4917 - val_acc: 0.4850\n",
            "Epoch 414/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 0.5420 - acc: 0.7638 - val_loss: 2.9252 - val_acc: 0.4400\n",
            "Epoch 415/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.5894 - acc: 0.7437 - val_loss: 2.2261 - val_acc: 0.4000\n",
            "Epoch 416/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.6195 - acc: 0.7312 - val_loss: 2.4039 - val_acc: 0.4150\n",
            "Epoch 417/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.6638 - acc: 0.7200 - val_loss: 2.2574 - val_acc: 0.3850\n",
            "Epoch 418/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.6400 - acc: 0.7413 - val_loss: 2.2561 - val_acc: 0.4000\n",
            "Epoch 419/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.6438 - acc: 0.7362 - val_loss: 2.1519 - val_acc: 0.4350\n",
            "Epoch 420/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 0.5215 - acc: 0.7888 - val_loss: 2.2498 - val_acc: 0.4550\n",
            "Epoch 421/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.5423 - acc: 0.7812 - val_loss: 2.2471 - val_acc: 0.4300\n",
            "Epoch 422/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 0.5157 - acc: 0.8013 - val_loss: 2.2649 - val_acc: 0.4300\n",
            "Epoch 423/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.5436 - acc: 0.7850 - val_loss: 2.2687 - val_acc: 0.4750\n",
            "Epoch 424/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.5410 - acc: 0.7975 - val_loss: 2.3458 - val_acc: 0.4600\n",
            "Epoch 425/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.5009 - acc: 0.8025 - val_loss: 2.5411 - val_acc: 0.4700\n",
            "Epoch 426/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.4416 - acc: 0.8438 - val_loss: 2.7312 - val_acc: 0.4700\n",
            "Epoch 427/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.4809 - acc: 0.8100 - val_loss: 2.8423 - val_acc: 0.4900\n",
            "Epoch 428/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.4473 - acc: 0.8263 - val_loss: 2.7292 - val_acc: 0.4150\n",
            "Epoch 429/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.4862 - acc: 0.8225 - val_loss: 2.8602 - val_acc: 0.4550\n",
            "Epoch 430/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.5153 - acc: 0.8025 - val_loss: 2.5018 - val_acc: 0.4600\n",
            "Epoch 431/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.5907 - acc: 0.7625 - val_loss: 2.6132 - val_acc: 0.3850\n",
            "Epoch 432/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.5208 - acc: 0.7888 - val_loss: 2.4579 - val_acc: 0.4650\n",
            "Epoch 433/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.5091 - acc: 0.7913 - val_loss: 2.4046 - val_acc: 0.4400\n",
            "Epoch 434/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.5131 - acc: 0.8100 - val_loss: 2.6393 - val_acc: 0.4600\n",
            "Epoch 435/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.5054 - acc: 0.7937 - val_loss: 2.5501 - val_acc: 0.4350\n",
            "Epoch 436/1000\n",
            "800/800 [==============================] - 0s 281us/step - loss: 0.5036 - acc: 0.8113 - val_loss: 2.5891 - val_acc: 0.4700\n",
            "Epoch 437/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.4702 - acc: 0.8300 - val_loss: 2.4563 - val_acc: 0.4300\n",
            "Epoch 438/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.4925 - acc: 0.8038 - val_loss: 2.5975 - val_acc: 0.4000\n",
            "Epoch 439/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.5554 - acc: 0.8000 - val_loss: 2.4874 - val_acc: 0.4550\n",
            "Epoch 440/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.4697 - acc: 0.8225 - val_loss: 2.5161 - val_acc: 0.4450\n",
            "Epoch 441/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.4575 - acc: 0.8362 - val_loss: 2.6142 - val_acc: 0.4450\n",
            "Epoch 442/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.4390 - acc: 0.8412 - val_loss: 2.8456 - val_acc: 0.4450\n",
            "Epoch 443/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.3810 - acc: 0.8725 - val_loss: 2.9133 - val_acc: 0.4750\n",
            "Epoch 444/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.4635 - acc: 0.8300 - val_loss: 2.7326 - val_acc: 0.4350\n",
            "Epoch 445/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.4510 - acc: 0.8362 - val_loss: 2.7005 - val_acc: 0.4750\n",
            "Epoch 446/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.4329 - acc: 0.8237 - val_loss: 2.5555 - val_acc: 0.4300\n",
            "Epoch 447/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.4738 - acc: 0.8313 - val_loss: 3.0223 - val_acc: 0.4350\n",
            "Epoch 448/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.5578 - acc: 0.8050 - val_loss: 2.6897 - val_acc: 0.4500\n",
            "Epoch 449/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.5531 - acc: 0.7913 - val_loss: 2.4812 - val_acc: 0.4750\n",
            "Epoch 450/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.5079 - acc: 0.8100 - val_loss: 2.7073 - val_acc: 0.4550\n",
            "Epoch 451/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.4287 - acc: 0.8412 - val_loss: 2.5941 - val_acc: 0.4700\n",
            "Epoch 452/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.4532 - acc: 0.8488 - val_loss: 2.3995 - val_acc: 0.4500\n",
            "Epoch 453/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.4569 - acc: 0.8387 - val_loss: 2.4150 - val_acc: 0.4750\n",
            "Epoch 454/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.3963 - acc: 0.8638 - val_loss: 2.4600 - val_acc: 0.4650\n",
            "Epoch 455/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.4185 - acc: 0.8588 - val_loss: 2.9781 - val_acc: 0.4450\n",
            "Epoch 456/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.4266 - acc: 0.8525 - val_loss: 2.6281 - val_acc: 0.4850\n",
            "Epoch 457/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.4094 - acc: 0.8600 - val_loss: 2.9108 - val_acc: 0.4500\n",
            "Epoch 458/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.4024 - acc: 0.8712 - val_loss: 2.8709 - val_acc: 0.4650\n",
            "Epoch 459/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.4109 - acc: 0.8663 - val_loss: 2.9803 - val_acc: 0.4500\n",
            "Epoch 460/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.4602 - acc: 0.8337 - val_loss: 2.9541 - val_acc: 0.4700\n",
            "Epoch 461/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.4554 - acc: 0.8475 - val_loss: 2.6738 - val_acc: 0.4200\n",
            "Epoch 462/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.5137 - acc: 0.8200 - val_loss: 2.7752 - val_acc: 0.4300\n",
            "Epoch 463/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.5314 - acc: 0.8013 - val_loss: 2.4637 - val_acc: 0.4550\n",
            "Epoch 464/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.3614 - acc: 0.8725 - val_loss: 2.7429 - val_acc: 0.4750\n",
            "Epoch 465/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.3381 - acc: 0.8888 - val_loss: 2.4699 - val_acc: 0.4450\n",
            "Epoch 466/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.4431 - acc: 0.8438 - val_loss: 2.5378 - val_acc: 0.4800\n",
            "Epoch 467/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.4407 - acc: 0.8513 - val_loss: 2.8315 - val_acc: 0.4600\n",
            "Epoch 468/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.4200 - acc: 0.8562 - val_loss: 3.2373 - val_acc: 0.4350\n",
            "Epoch 469/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.4604 - acc: 0.8237 - val_loss: 3.3899 - val_acc: 0.3850\n",
            "Epoch 470/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.4731 - acc: 0.8113 - val_loss: 2.6649 - val_acc: 0.4500\n",
            "Epoch 471/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.4297 - acc: 0.8562 - val_loss: 2.7365 - val_acc: 0.4650\n",
            "Epoch 472/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.4239 - acc: 0.8750 - val_loss: 2.4237 - val_acc: 0.4550\n",
            "Epoch 473/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.3806 - acc: 0.8738 - val_loss: 2.4720 - val_acc: 0.4550\n",
            "Epoch 474/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.3415 - acc: 0.8837 - val_loss: 2.5990 - val_acc: 0.4750\n",
            "Epoch 475/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.4171 - acc: 0.8638 - val_loss: 2.5992 - val_acc: 0.4550\n",
            "Epoch 476/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.3314 - acc: 0.8850 - val_loss: 2.8754 - val_acc: 0.4850\n",
            "Epoch 477/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2792 - acc: 0.9200 - val_loss: 2.7036 - val_acc: 0.4750\n",
            "Epoch 478/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.3277 - acc: 0.9000 - val_loss: 2.7611 - val_acc: 0.4650\n",
            "Epoch 479/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.3279 - acc: 0.9000 - val_loss: 3.0555 - val_acc: 0.4600\n",
            "Epoch 480/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.3735 - acc: 0.8862 - val_loss: 3.3498 - val_acc: 0.4350\n",
            "Epoch 481/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.4432 - acc: 0.8525 - val_loss: 2.9525 - val_acc: 0.4400\n",
            "Epoch 482/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.3062 - acc: 0.9025 - val_loss: 2.9990 - val_acc: 0.4800\n",
            "Epoch 483/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.3983 - acc: 0.8650 - val_loss: 2.6256 - val_acc: 0.4450\n",
            "Epoch 484/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.4346 - acc: 0.8575 - val_loss: 3.1507 - val_acc: 0.4750\n",
            "Epoch 485/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.4312 - acc: 0.8538 - val_loss: 2.8681 - val_acc: 0.4500\n",
            "Epoch 486/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.4988 - acc: 0.8175 - val_loss: 2.9918 - val_acc: 0.4500\n",
            "Epoch 487/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.3860 - acc: 0.8562 - val_loss: 3.0684 - val_acc: 0.4550\n",
            "Epoch 488/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.3802 - acc: 0.8612 - val_loss: 3.1275 - val_acc: 0.4050\n",
            "Epoch 489/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.4425 - acc: 0.8562 - val_loss: 2.9173 - val_acc: 0.4350\n",
            "Epoch 490/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.3878 - acc: 0.8687 - val_loss: 2.8338 - val_acc: 0.4800\n",
            "Epoch 491/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.3264 - acc: 0.8912 - val_loss: 2.7740 - val_acc: 0.4300\n",
            "Epoch 492/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 0.3143 - acc: 0.8938 - val_loss: 2.8384 - val_acc: 0.4500\n",
            "Epoch 493/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.2957 - acc: 0.9138 - val_loss: 2.7293 - val_acc: 0.4750\n",
            "Epoch 494/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.2862 - acc: 0.9037 - val_loss: 2.8741 - val_acc: 0.4600\n",
            "Epoch 495/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.3738 - acc: 0.8850 - val_loss: 3.0786 - val_acc: 0.4650\n",
            "Epoch 496/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.3312 - acc: 0.8975 - val_loss: 3.0417 - val_acc: 0.4550\n",
            "Epoch 497/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.2825 - acc: 0.9113 - val_loss: 3.1303 - val_acc: 0.4800\n",
            "Epoch 498/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.3134 - acc: 0.8975 - val_loss: 2.8477 - val_acc: 0.4100\n",
            "Epoch 499/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.2875 - acc: 0.8950 - val_loss: 3.0771 - val_acc: 0.4650\n",
            "Epoch 500/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.2821 - acc: 0.9062 - val_loss: 3.0877 - val_acc: 0.4550\n",
            "Epoch 501/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.2953 - acc: 0.8912 - val_loss: 3.3145 - val_acc: 0.4600\n",
            "Epoch 502/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.3083 - acc: 0.8925 - val_loss: 3.0170 - val_acc: 0.4300\n",
            "Epoch 503/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.3315 - acc: 0.9088 - val_loss: 3.3763 - val_acc: 0.4300\n",
            "Epoch 504/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.4937 - acc: 0.8313 - val_loss: 2.8281 - val_acc: 0.4150\n",
            "Epoch 505/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.4064 - acc: 0.8600 - val_loss: 2.8906 - val_acc: 0.4900\n",
            "Epoch 506/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.3055 - acc: 0.8900 - val_loss: 2.8248 - val_acc: 0.4800\n",
            "Epoch 507/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.2896 - acc: 0.9113 - val_loss: 2.8365 - val_acc: 0.4800\n",
            "Epoch 508/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.3186 - acc: 0.9025 - val_loss: 2.9290 - val_acc: 0.4800\n",
            "Epoch 509/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.3076 - acc: 0.9050 - val_loss: 3.1101 - val_acc: 0.4900\n",
            "Epoch 510/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.2389 - acc: 0.9237 - val_loss: 3.1593 - val_acc: 0.4600\n",
            "Epoch 511/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.2762 - acc: 0.9225 - val_loss: 3.1493 - val_acc: 0.4700\n",
            "Epoch 512/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.2666 - acc: 0.9163 - val_loss: 3.0111 - val_acc: 0.4600\n",
            "Epoch 513/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.2828 - acc: 0.9163 - val_loss: 2.9813 - val_acc: 0.4400\n",
            "Epoch 514/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.2791 - acc: 0.9175 - val_loss: 2.9087 - val_acc: 0.4550\n",
            "Epoch 515/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2491 - acc: 0.9250 - val_loss: 3.1625 - val_acc: 0.4800\n",
            "Epoch 516/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.2796 - acc: 0.9025 - val_loss: 2.9534 - val_acc: 0.4500\n",
            "Epoch 517/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.2918 - acc: 0.9050 - val_loss: 2.9891 - val_acc: 0.4650\n",
            "Epoch 518/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.2298 - acc: 0.9250 - val_loss: 3.0741 - val_acc: 0.4500\n",
            "Epoch 519/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.3483 - acc: 0.8813 - val_loss: 3.1315 - val_acc: 0.4350\n",
            "Epoch 520/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.3543 - acc: 0.8825 - val_loss: 3.0614 - val_acc: 0.4850\n",
            "Epoch 521/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.2958 - acc: 0.9050 - val_loss: 2.8977 - val_acc: 0.4900\n",
            "Epoch 522/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.2895 - acc: 0.9100 - val_loss: 3.0314 - val_acc: 0.4700\n",
            "Epoch 523/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.3087 - acc: 0.9025 - val_loss: 3.2758 - val_acc: 0.4700\n",
            "Epoch 524/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.2614 - acc: 0.9088 - val_loss: 2.9541 - val_acc: 0.4500\n",
            "Epoch 525/1000\n",
            "800/800 [==============================] - 0s 246us/step - loss: 0.2445 - acc: 0.9275 - val_loss: 3.0475 - val_acc: 0.4850\n",
            "Epoch 526/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.2505 - acc: 0.9213 - val_loss: 2.9516 - val_acc: 0.4800\n",
            "Epoch 527/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.3114 - acc: 0.8975 - val_loss: 2.7947 - val_acc: 0.4800\n",
            "Epoch 528/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.2909 - acc: 0.8987 - val_loss: 2.9502 - val_acc: 0.4700\n",
            "Epoch 529/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.2582 - acc: 0.9200 - val_loss: 3.0142 - val_acc: 0.4750\n",
            "Epoch 530/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.2414 - acc: 0.9225 - val_loss: 2.9273 - val_acc: 0.4650\n",
            "Epoch 531/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.2289 - acc: 0.9363 - val_loss: 3.0220 - val_acc: 0.4650\n",
            "Epoch 532/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.2809 - acc: 0.9100 - val_loss: 3.3277 - val_acc: 0.4450\n",
            "Epoch 533/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.3471 - acc: 0.8888 - val_loss: 3.0510 - val_acc: 0.4750\n",
            "Epoch 534/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.2653 - acc: 0.9275 - val_loss: 3.2079 - val_acc: 0.4700\n",
            "Epoch 535/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2668 - acc: 0.9237 - val_loss: 3.0292 - val_acc: 0.4700\n",
            "Epoch 536/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.2154 - acc: 0.9400 - val_loss: 3.1271 - val_acc: 0.4450\n",
            "Epoch 537/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.2863 - acc: 0.9163 - val_loss: 2.9875 - val_acc: 0.4850\n",
            "Epoch 538/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.2112 - acc: 0.9363 - val_loss: 3.2474 - val_acc: 0.4850\n",
            "Epoch 539/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.2433 - acc: 0.9263 - val_loss: 3.2456 - val_acc: 0.4600\n",
            "Epoch 540/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.2593 - acc: 0.9138 - val_loss: 3.2364 - val_acc: 0.4550\n",
            "Epoch 541/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2202 - acc: 0.9312 - val_loss: 3.1718 - val_acc: 0.4700\n",
            "Epoch 542/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.1773 - acc: 0.9462 - val_loss: 3.4985 - val_acc: 0.4650\n",
            "Epoch 543/1000\n",
            "800/800 [==============================] - 0s 285us/step - loss: 0.1816 - acc: 0.9462 - val_loss: 3.5262 - val_acc: 0.4650\n",
            "Epoch 544/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.2614 - acc: 0.9213 - val_loss: 3.9480 - val_acc: 0.4500\n",
            "Epoch 545/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.3229 - acc: 0.9138 - val_loss: 3.8638 - val_acc: 0.4350\n",
            "Epoch 546/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.3571 - acc: 0.8762 - val_loss: 3.8806 - val_acc: 0.4350\n",
            "Epoch 547/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.4020 - acc: 0.8712 - val_loss: 3.4798 - val_acc: 0.4550\n",
            "Epoch 548/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.2651 - acc: 0.9237 - val_loss: 3.0457 - val_acc: 0.4600\n",
            "Epoch 549/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2122 - acc: 0.9337 - val_loss: 2.8754 - val_acc: 0.4350\n",
            "Epoch 550/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.2179 - acc: 0.9275 - val_loss: 2.9164 - val_acc: 0.4550\n",
            "Epoch 551/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.2213 - acc: 0.9350 - val_loss: 3.0633 - val_acc: 0.4700\n",
            "Epoch 552/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2294 - acc: 0.9387 - val_loss: 3.3369 - val_acc: 0.4750\n",
            "Epoch 553/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.1940 - acc: 0.9438 - val_loss: 3.5864 - val_acc: 0.4950\n",
            "Epoch 554/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.2645 - acc: 0.9187 - val_loss: 3.1875 - val_acc: 0.4450\n",
            "Epoch 555/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.2547 - acc: 0.9300 - val_loss: 3.2781 - val_acc: 0.4800\n",
            "Epoch 556/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.1932 - acc: 0.9325 - val_loss: 3.2871 - val_acc: 0.4550\n",
            "Epoch 557/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 0.1901 - acc: 0.9475 - val_loss: 3.2959 - val_acc: 0.4600\n",
            "Epoch 558/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.1548 - acc: 0.9612 - val_loss: 3.5722 - val_acc: 0.4600\n",
            "Epoch 559/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1560 - acc: 0.9575 - val_loss: 3.6953 - val_acc: 0.4350\n",
            "Epoch 560/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2565 - acc: 0.9288 - val_loss: 3.7732 - val_acc: 0.4550\n",
            "Epoch 561/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1918 - acc: 0.9487 - val_loss: 3.5736 - val_acc: 0.4750\n",
            "Epoch 562/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1755 - acc: 0.9500 - val_loss: 3.4209 - val_acc: 0.4700\n",
            "Epoch 563/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.3204 - acc: 0.8875 - val_loss: 3.2774 - val_acc: 0.4400\n",
            "Epoch 564/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.3984 - acc: 0.8700 - val_loss: 3.4241 - val_acc: 0.4650\n",
            "Epoch 565/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.3251 - acc: 0.9113 - val_loss: 3.0669 - val_acc: 0.4300\n",
            "Epoch 566/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.2515 - acc: 0.9237 - val_loss: 3.1634 - val_acc: 0.4750\n",
            "Epoch 567/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.2103 - acc: 0.9312 - val_loss: 2.8227 - val_acc: 0.4600\n",
            "Epoch 568/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.2242 - acc: 0.9325 - val_loss: 3.1165 - val_acc: 0.4850\n",
            "Epoch 569/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2042 - acc: 0.9400 - val_loss: 3.0281 - val_acc: 0.4450\n",
            "Epoch 570/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1897 - acc: 0.9475 - val_loss: 3.1799 - val_acc: 0.5050\n",
            "Epoch 571/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1564 - acc: 0.9550 - val_loss: 3.1502 - val_acc: 0.4850\n",
            "Epoch 572/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1726 - acc: 0.9525 - val_loss: 3.3988 - val_acc: 0.4950\n",
            "Epoch 573/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1702 - acc: 0.9500 - val_loss: 3.3866 - val_acc: 0.4550\n",
            "Epoch 574/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.2347 - acc: 0.9275 - val_loss: 3.4069 - val_acc: 0.4750\n",
            "Epoch 575/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2296 - acc: 0.9337 - val_loss: 3.3761 - val_acc: 0.4650\n",
            "Epoch 576/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1924 - acc: 0.9413 - val_loss: 3.4136 - val_acc: 0.4800\n",
            "Epoch 577/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.2142 - acc: 0.9263 - val_loss: 3.4125 - val_acc: 0.4850\n",
            "Epoch 578/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2047 - acc: 0.9312 - val_loss: 3.4533 - val_acc: 0.4700\n",
            "Epoch 579/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.2729 - acc: 0.9263 - val_loss: 3.5225 - val_acc: 0.4850\n",
            "Epoch 580/1000\n",
            "800/800 [==============================] - 0s 242us/step - loss: 0.2479 - acc: 0.9175 - val_loss: 3.3329 - val_acc: 0.4650\n",
            "Epoch 581/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.1454 - acc: 0.9612 - val_loss: 3.4174 - val_acc: 0.4800\n",
            "Epoch 582/1000\n",
            "800/800 [==============================] - 0s 280us/step - loss: 0.1645 - acc: 0.9525 - val_loss: 3.4559 - val_acc: 0.4650\n",
            "Epoch 583/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.1913 - acc: 0.9438 - val_loss: 3.4114 - val_acc: 0.4700\n",
            "Epoch 584/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1544 - acc: 0.9650 - val_loss: 3.5279 - val_acc: 0.4650\n",
            "Epoch 585/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.1671 - acc: 0.9438 - val_loss: 3.7588 - val_acc: 0.4600\n",
            "Epoch 586/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.5430 - acc: 0.8538 - val_loss: 3.2574 - val_acc: 0.4500\n",
            "Epoch 587/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.4149 - acc: 0.8813 - val_loss: 3.3495 - val_acc: 0.4550\n",
            "Epoch 588/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.3470 - acc: 0.8762 - val_loss: 2.8237 - val_acc: 0.4250\n",
            "Epoch 589/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.2050 - acc: 0.9275 - val_loss: 2.8586 - val_acc: 0.4600\n",
            "Epoch 590/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.2042 - acc: 0.9387 - val_loss: 2.9518 - val_acc: 0.4700\n",
            "Epoch 591/1000\n",
            "800/800 [==============================] - 0s 283us/step - loss: 0.1889 - acc: 0.9450 - val_loss: 3.0386 - val_acc: 0.4900\n",
            "Epoch 592/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.2307 - acc: 0.9325 - val_loss: 3.0329 - val_acc: 0.4400\n",
            "Epoch 593/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2310 - acc: 0.9337 - val_loss: 3.1983 - val_acc: 0.4450\n",
            "Epoch 594/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.3028 - acc: 0.9088 - val_loss: 3.1318 - val_acc: 0.4550\n",
            "Epoch 595/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1862 - acc: 0.9413 - val_loss: 3.1146 - val_acc: 0.4200\n",
            "Epoch 596/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.2591 - acc: 0.9250 - val_loss: 3.0411 - val_acc: 0.4650\n",
            "Epoch 597/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.2242 - acc: 0.9288 - val_loss: 3.2382 - val_acc: 0.4900\n",
            "Epoch 598/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.2244 - acc: 0.9237 - val_loss: 3.2228 - val_acc: 0.4700\n",
            "Epoch 599/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.2173 - acc: 0.9413 - val_loss: 3.2490 - val_acc: 0.4600\n",
            "Epoch 600/1000\n",
            "800/800 [==============================] - 0s 240us/step - loss: 0.1848 - acc: 0.9500 - val_loss: 3.2393 - val_acc: 0.4650\n",
            "Epoch 601/1000\n",
            "800/800 [==============================] - 0s 287us/step - loss: 0.1610 - acc: 0.9475 - val_loss: 3.4384 - val_acc: 0.4550\n",
            "Epoch 602/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1585 - acc: 0.9575 - val_loss: 3.5464 - val_acc: 0.5050\n",
            "Epoch 603/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.1454 - acc: 0.9563 - val_loss: 3.3634 - val_acc: 0.4450\n",
            "Epoch 604/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.1435 - acc: 0.9563 - val_loss: 3.5446 - val_acc: 0.4550\n",
            "Epoch 605/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2321 - acc: 0.9187 - val_loss: 3.3447 - val_acc: 0.4800\n",
            "Epoch 606/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.1849 - acc: 0.9500 - val_loss: 3.5340 - val_acc: 0.4800\n",
            "Epoch 607/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 0.1593 - acc: 0.9550 - val_loss: 3.5591 - val_acc: 0.4700\n",
            "Epoch 608/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.1468 - acc: 0.9550 - val_loss: 3.5758 - val_acc: 0.4750\n",
            "Epoch 609/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.1737 - acc: 0.9487 - val_loss: 3.7644 - val_acc: 0.4850\n",
            "Epoch 610/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.1861 - acc: 0.9425 - val_loss: 3.6065 - val_acc: 0.4350\n",
            "Epoch 611/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.1668 - acc: 0.9537 - val_loss: 3.8222 - val_acc: 0.4800\n",
            "Epoch 612/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.2365 - acc: 0.9138 - val_loss: 3.5789 - val_acc: 0.4150\n",
            "Epoch 613/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.2654 - acc: 0.9225 - val_loss: 3.6301 - val_acc: 0.4650\n",
            "Epoch 614/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2713 - acc: 0.9100 - val_loss: 3.1682 - val_acc: 0.4450\n",
            "Epoch 615/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.2184 - acc: 0.9300 - val_loss: 3.4016 - val_acc: 0.4900\n",
            "Epoch 616/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.2219 - acc: 0.9263 - val_loss: 3.2725 - val_acc: 0.4700\n",
            "Epoch 617/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.2015 - acc: 0.9375 - val_loss: 3.3478 - val_acc: 0.4850\n",
            "Epoch 618/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.2266 - acc: 0.9400 - val_loss: 3.2103 - val_acc: 0.5100\n",
            "Epoch 619/1000\n",
            "800/800 [==============================] - 0s 243us/step - loss: 0.1582 - acc: 0.9550 - val_loss: 3.3138 - val_acc: 0.5150\n",
            "Epoch 620/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1631 - acc: 0.9525 - val_loss: 3.3198 - val_acc: 0.4850\n",
            "Epoch 621/1000\n",
            "800/800 [==============================] - 0s 280us/step - loss: 0.2030 - acc: 0.9350 - val_loss: 3.6725 - val_acc: 0.4600\n",
            "Epoch 622/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.1827 - acc: 0.9438 - val_loss: 3.8768 - val_acc: 0.4800\n",
            "Epoch 623/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1625 - acc: 0.9625 - val_loss: 3.5840 - val_acc: 0.4500\n",
            "Epoch 624/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1366 - acc: 0.9500 - val_loss: 3.4537 - val_acc: 0.4950\n",
            "Epoch 625/1000\n",
            "800/800 [==============================] - 0s 281us/step - loss: 0.1561 - acc: 0.9612 - val_loss: 3.5053 - val_acc: 0.4400\n",
            "Epoch 626/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.1177 - acc: 0.9713 - val_loss: 3.5204 - val_acc: 0.4400\n",
            "Epoch 627/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.1056 - acc: 0.9812 - val_loss: 3.4958 - val_acc: 0.4850\n",
            "Epoch 628/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.1103 - acc: 0.9662 - val_loss: 3.6154 - val_acc: 0.4800\n",
            "Epoch 629/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1267 - acc: 0.9650 - val_loss: 3.6233 - val_acc: 0.5100\n",
            "Epoch 630/1000\n",
            "800/800 [==============================] - 0s 287us/step - loss: 0.1195 - acc: 0.9637 - val_loss: 3.7258 - val_acc: 0.4550\n",
            "Epoch 631/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1451 - acc: 0.9575 - val_loss: 3.9436 - val_acc: 0.5050\n",
            "Epoch 632/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.1962 - acc: 0.9500 - val_loss: 3.8496 - val_acc: 0.4700\n",
            "Epoch 633/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.2199 - acc: 0.9350 - val_loss: 4.2002 - val_acc: 0.4450\n",
            "Epoch 634/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.2046 - acc: 0.9438 - val_loss: 3.7679 - val_acc: 0.4900\n",
            "Epoch 635/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.2625 - acc: 0.9200 - val_loss: 3.6498 - val_acc: 0.4400\n",
            "Epoch 636/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.2991 - acc: 0.9050 - val_loss: 3.2842 - val_acc: 0.4800\n",
            "Epoch 637/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.2331 - acc: 0.9275 - val_loss: 3.3695 - val_acc: 0.4300\n",
            "Epoch 638/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.2594 - acc: 0.9075 - val_loss: 3.2330 - val_acc: 0.4550\n",
            "Epoch 639/1000\n",
            "800/800 [==============================] - 0s 244us/step - loss: 0.1931 - acc: 0.9425 - val_loss: 3.2569 - val_acc: 0.4700\n",
            "Epoch 640/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 0.1715 - acc: 0.9450 - val_loss: 3.0744 - val_acc: 0.4600\n",
            "Epoch 641/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1667 - acc: 0.9550 - val_loss: 3.2439 - val_acc: 0.4600\n",
            "Epoch 642/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1481 - acc: 0.9525 - val_loss: 3.3781 - val_acc: 0.5000\n",
            "Epoch 643/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 0.1512 - acc: 0.9525 - val_loss: 3.3876 - val_acc: 0.4550\n",
            "Epoch 644/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1076 - acc: 0.9800 - val_loss: 3.4720 - val_acc: 0.4900\n",
            "Epoch 645/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 0.1111 - acc: 0.9700 - val_loss: 3.5469 - val_acc: 0.4700\n",
            "Epoch 646/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1191 - acc: 0.9700 - val_loss: 3.7659 - val_acc: 0.4500\n",
            "Epoch 647/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1347 - acc: 0.9637 - val_loss: 3.6097 - val_acc: 0.4450\n",
            "Epoch 648/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1122 - acc: 0.9713 - val_loss: 3.7102 - val_acc: 0.4900\n",
            "Epoch 649/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.1837 - acc: 0.9475 - val_loss: 3.7792 - val_acc: 0.4850\n",
            "Epoch 650/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.2179 - acc: 0.9400 - val_loss: 3.7215 - val_acc: 0.4800\n",
            "Epoch 651/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.2883 - acc: 0.9237 - val_loss: 3.6597 - val_acc: 0.4700\n",
            "Epoch 652/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1932 - acc: 0.9387 - val_loss: 3.5942 - val_acc: 0.4450\n",
            "Epoch 653/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1829 - acc: 0.9438 - val_loss: 3.3586 - val_acc: 0.4900\n",
            "Epoch 654/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1527 - acc: 0.9612 - val_loss: 3.4725 - val_acc: 0.4650\n",
            "Epoch 655/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1343 - acc: 0.9650 - val_loss: 3.6745 - val_acc: 0.4650\n",
            "Epoch 656/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1863 - acc: 0.9487 - val_loss: 3.6054 - val_acc: 0.4750\n",
            "Epoch 657/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1511 - acc: 0.9612 - val_loss: 3.5980 - val_acc: 0.4450\n",
            "Epoch 658/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1622 - acc: 0.9462 - val_loss: 3.6277 - val_acc: 0.4750\n",
            "Epoch 659/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.0941 - acc: 0.9750 - val_loss: 3.6678 - val_acc: 0.4700\n",
            "Epoch 660/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.1097 - acc: 0.9713 - val_loss: 3.7712 - val_acc: 0.4600\n",
            "Epoch 661/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1567 - acc: 0.9637 - val_loss: 3.6891 - val_acc: 0.4900\n",
            "Epoch 662/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.0939 - acc: 0.9700 - val_loss: 3.9240 - val_acc: 0.4950\n",
            "Epoch 663/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.2075 - acc: 0.9425 - val_loss: 3.8857 - val_acc: 0.4750\n",
            "Epoch 664/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1782 - acc: 0.9537 - val_loss: 4.0555 - val_acc: 0.4700\n",
            "Epoch 665/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.1797 - acc: 0.9487 - val_loss: 4.0923 - val_acc: 0.4600\n",
            "Epoch 666/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.2136 - acc: 0.9363 - val_loss: 3.8602 - val_acc: 0.4650\n",
            "Epoch 667/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.2280 - acc: 0.9288 - val_loss: 3.5952 - val_acc: 0.4950\n",
            "Epoch 668/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 0.1581 - acc: 0.9537 - val_loss: 3.4515 - val_acc: 0.4600\n",
            "Epoch 669/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.1782 - acc: 0.9425 - val_loss: 3.7695 - val_acc: 0.4800\n",
            "Epoch 670/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1580 - acc: 0.9513 - val_loss: 3.5418 - val_acc: 0.4750\n",
            "Epoch 671/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1336 - acc: 0.9650 - val_loss: 3.6960 - val_acc: 0.4750\n",
            "Epoch 672/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1558 - acc: 0.9537 - val_loss: 3.8101 - val_acc: 0.4950\n",
            "Epoch 673/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.1462 - acc: 0.9662 - val_loss: 3.7679 - val_acc: 0.4850\n",
            "Epoch 674/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.1830 - acc: 0.9575 - val_loss: 3.8269 - val_acc: 0.4600\n",
            "Epoch 675/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1752 - acc: 0.9475 - val_loss: 3.6380 - val_acc: 0.4650\n",
            "Epoch 676/1000\n",
            "800/800 [==============================] - 0s 244us/step - loss: 0.1623 - acc: 0.9637 - val_loss: 3.7473 - val_acc: 0.4800\n",
            "Epoch 677/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.1418 - acc: 0.9550 - val_loss: 3.5297 - val_acc: 0.4700\n",
            "Epoch 678/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1809 - acc: 0.9525 - val_loss: 3.5251 - val_acc: 0.4900\n",
            "Epoch 679/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1212 - acc: 0.9713 - val_loss: 3.4973 - val_acc: 0.4850\n",
            "Epoch 680/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.1406 - acc: 0.9625 - val_loss: 3.5048 - val_acc: 0.4850\n",
            "Epoch 681/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.0954 - acc: 0.9725 - val_loss: 3.6973 - val_acc: 0.5200\n",
            "Epoch 682/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1072 - acc: 0.9700 - val_loss: 3.8043 - val_acc: 0.5050\n",
            "Epoch 683/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.1239 - acc: 0.9662 - val_loss: 3.6882 - val_acc: 0.4850\n",
            "Epoch 684/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1173 - acc: 0.9587 - val_loss: 3.8060 - val_acc: 0.4550\n",
            "Epoch 685/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1425 - acc: 0.9625 - val_loss: 3.7726 - val_acc: 0.4750\n",
            "Epoch 686/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1364 - acc: 0.9675 - val_loss: 3.7642 - val_acc: 0.4850\n",
            "Epoch 687/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1917 - acc: 0.9450 - val_loss: 4.1513 - val_acc: 0.4800\n",
            "Epoch 688/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1826 - acc: 0.9537 - val_loss: 3.6096 - val_acc: 0.4800\n",
            "Epoch 689/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1590 - acc: 0.9537 - val_loss: 3.8196 - val_acc: 0.4500\n",
            "Epoch 690/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1396 - acc: 0.9563 - val_loss: 3.7367 - val_acc: 0.4450\n",
            "Epoch 691/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1542 - acc: 0.9575 - val_loss: 3.6247 - val_acc: 0.4600\n",
            "Epoch 692/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.1376 - acc: 0.9513 - val_loss: 3.7979 - val_acc: 0.4650\n",
            "Epoch 693/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.1408 - acc: 0.9563 - val_loss: 3.7882 - val_acc: 0.4650\n",
            "Epoch 694/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1191 - acc: 0.9675 - val_loss: 3.7590 - val_acc: 0.4850\n",
            "Epoch 695/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1133 - acc: 0.9600 - val_loss: 3.7173 - val_acc: 0.4900\n",
            "Epoch 696/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1357 - acc: 0.9587 - val_loss: 3.8897 - val_acc: 0.4350\n",
            "Epoch 697/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.0932 - acc: 0.9725 - val_loss: 3.8804 - val_acc: 0.4750\n",
            "Epoch 698/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.0965 - acc: 0.9763 - val_loss: 3.8506 - val_acc: 0.4650\n",
            "Epoch 699/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.0819 - acc: 0.9812 - val_loss: 3.9544 - val_acc: 0.5050\n",
            "Epoch 700/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1177 - acc: 0.9675 - val_loss: 4.0773 - val_acc: 0.4650\n",
            "Epoch 701/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1243 - acc: 0.9650 - val_loss: 4.1247 - val_acc: 0.4950\n",
            "Epoch 702/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1512 - acc: 0.9563 - val_loss: 3.9797 - val_acc: 0.4900\n",
            "Epoch 703/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.0981 - acc: 0.9675 - val_loss: 4.0754 - val_acc: 0.4700\n",
            "Epoch 704/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1172 - acc: 0.9725 - val_loss: 4.0488 - val_acc: 0.4550\n",
            "Epoch 705/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1449 - acc: 0.9612 - val_loss: 4.4365 - val_acc: 0.4550\n",
            "Epoch 706/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 0.2347 - acc: 0.9450 - val_loss: 4.3999 - val_acc: 0.4650\n",
            "Epoch 707/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.4639 - acc: 0.8612 - val_loss: 3.9241 - val_acc: 0.4950\n",
            "Epoch 708/1000\n",
            "800/800 [==============================] - 0s 281us/step - loss: 0.4040 - acc: 0.8750 - val_loss: 3.3055 - val_acc: 0.4850\n",
            "Epoch 709/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.3513 - acc: 0.8775 - val_loss: 3.1468 - val_acc: 0.4800\n",
            "Epoch 710/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2341 - acc: 0.9125 - val_loss: 3.1317 - val_acc: 0.4550\n",
            "Epoch 711/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 0.2142 - acc: 0.9438 - val_loss: 3.1143 - val_acc: 0.4950\n",
            "Epoch 712/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1434 - acc: 0.9675 - val_loss: 3.2406 - val_acc: 0.4800\n",
            "Epoch 713/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1247 - acc: 0.9662 - val_loss: 3.1112 - val_acc: 0.4350\n",
            "Epoch 714/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.1399 - acc: 0.9575 - val_loss: 3.5060 - val_acc: 0.4850\n",
            "Epoch 715/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1204 - acc: 0.9612 - val_loss: 3.3064 - val_acc: 0.5000\n",
            "Epoch 716/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.0948 - acc: 0.9750 - val_loss: 3.3715 - val_acc: 0.5200\n",
            "Epoch 717/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.0814 - acc: 0.9812 - val_loss: 3.4824 - val_acc: 0.5150\n",
            "Epoch 718/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0716 - acc: 0.9800 - val_loss: 3.6599 - val_acc: 0.5050\n",
            "Epoch 719/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.0861 - acc: 0.9700 - val_loss: 3.7198 - val_acc: 0.4900\n",
            "Epoch 720/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.0609 - acc: 0.9800 - val_loss: 3.8506 - val_acc: 0.5050\n",
            "Epoch 721/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.0973 - acc: 0.9750 - val_loss: 4.0037 - val_acc: 0.4750\n",
            "Epoch 722/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.0994 - acc: 0.9738 - val_loss: 3.7781 - val_acc: 0.4950\n",
            "Epoch 723/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1724 - acc: 0.9575 - val_loss: 3.9733 - val_acc: 0.5100\n",
            "Epoch 724/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1862 - acc: 0.9450 - val_loss: 4.0459 - val_acc: 0.4650\n",
            "Epoch 725/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.2039 - acc: 0.9387 - val_loss: 3.9459 - val_acc: 0.4750\n",
            "Epoch 726/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1812 - acc: 0.9375 - val_loss: 3.7993 - val_acc: 0.4850\n",
            "Epoch 727/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.1552 - acc: 0.9550 - val_loss: 3.6506 - val_acc: 0.5000\n",
            "Epoch 728/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1924 - acc: 0.9450 - val_loss: 3.6066 - val_acc: 0.5000\n",
            "Epoch 729/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1917 - acc: 0.9375 - val_loss: 3.4876 - val_acc: 0.5100\n",
            "Epoch 730/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1721 - acc: 0.9537 - val_loss: 3.3394 - val_acc: 0.5100\n",
            "Epoch 731/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1186 - acc: 0.9637 - val_loss: 3.4207 - val_acc: 0.4650\n",
            "Epoch 732/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1003 - acc: 0.9725 - val_loss: 3.6434 - val_acc: 0.5050\n",
            "Epoch 733/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.1007 - acc: 0.9725 - val_loss: 3.6228 - val_acc: 0.4900\n",
            "Epoch 734/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.0934 - acc: 0.9725 - val_loss: 3.8622 - val_acc: 0.4600\n",
            "Epoch 735/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.2246 - acc: 0.9387 - val_loss: 4.0166 - val_acc: 0.5050\n",
            "Epoch 736/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.1610 - acc: 0.9537 - val_loss: 3.8270 - val_acc: 0.4800\n",
            "Epoch 737/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.2257 - acc: 0.9275 - val_loss: 3.7435 - val_acc: 0.5050\n",
            "Epoch 738/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1973 - acc: 0.9413 - val_loss: 3.6842 - val_acc: 0.4800\n",
            "Epoch 739/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.2159 - acc: 0.9325 - val_loss: 3.3581 - val_acc: 0.5000\n",
            "Epoch 740/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.1659 - acc: 0.9600 - val_loss: 3.5271 - val_acc: 0.4450\n",
            "Epoch 741/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1855 - acc: 0.9375 - val_loss: 3.2397 - val_acc: 0.4700\n",
            "Epoch 742/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1301 - acc: 0.9587 - val_loss: 3.4417 - val_acc: 0.4900\n",
            "Epoch 743/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1600 - acc: 0.9475 - val_loss: 3.6326 - val_acc: 0.4950\n",
            "Epoch 744/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1198 - acc: 0.9625 - val_loss: 3.9163 - val_acc: 0.4800\n",
            "Epoch 745/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1366 - acc: 0.9513 - val_loss: 3.5333 - val_acc: 0.5100\n",
            "Epoch 746/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.1238 - acc: 0.9637 - val_loss: 3.5170 - val_acc: 0.5000\n",
            "Epoch 747/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1968 - acc: 0.9537 - val_loss: 3.6299 - val_acc: 0.5100\n",
            "Epoch 748/1000\n",
            "800/800 [==============================] - 0s 249us/step - loss: 0.1338 - acc: 0.9563 - val_loss: 3.5467 - val_acc: 0.5100\n",
            "Epoch 749/1000\n",
            "800/800 [==============================] - 0s 279us/step - loss: 0.1360 - acc: 0.9587 - val_loss: 3.5387 - val_acc: 0.4750\n",
            "Epoch 750/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1069 - acc: 0.9713 - val_loss: 3.5030 - val_acc: 0.5250\n",
            "Epoch 751/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.0912 - acc: 0.9750 - val_loss: 3.5944 - val_acc: 0.4950\n",
            "Epoch 752/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0849 - acc: 0.9788 - val_loss: 3.7295 - val_acc: 0.4850\n",
            "Epoch 753/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1032 - acc: 0.9650 - val_loss: 3.7093 - val_acc: 0.4950\n",
            "Epoch 754/1000\n",
            "800/800 [==============================] - 0s 247us/step - loss: 0.0726 - acc: 0.9838 - val_loss: 3.8611 - val_acc: 0.4700\n",
            "Epoch 755/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.0977 - acc: 0.9650 - val_loss: 4.0544 - val_acc: 0.4800\n",
            "Epoch 756/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0996 - acc: 0.9700 - val_loss: 4.0627 - val_acc: 0.4350\n",
            "Epoch 757/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1309 - acc: 0.9675 - val_loss: 4.3081 - val_acc: 0.4900\n",
            "Epoch 758/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1317 - acc: 0.9563 - val_loss: 3.9609 - val_acc: 0.4850\n",
            "Epoch 759/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.1191 - acc: 0.9612 - val_loss: 3.8979 - val_acc: 0.5000\n",
            "Epoch 760/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1754 - acc: 0.9487 - val_loss: 3.8973 - val_acc: 0.4800\n",
            "Epoch 761/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1268 - acc: 0.9625 - val_loss: 3.6963 - val_acc: 0.5000\n",
            "Epoch 762/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1418 - acc: 0.9537 - val_loss: 3.7378 - val_acc: 0.5100\n",
            "Epoch 763/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.1189 - acc: 0.9637 - val_loss: 3.7220 - val_acc: 0.4700\n",
            "Epoch 764/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 0.1421 - acc: 0.9650 - val_loss: 3.9295 - val_acc: 0.4900\n",
            "Epoch 765/1000\n",
            "800/800 [==============================] - 0s 285us/step - loss: 0.1378 - acc: 0.9575 - val_loss: 3.8926 - val_acc: 0.4600\n",
            "Epoch 766/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.2647 - acc: 0.9200 - val_loss: 3.9709 - val_acc: 0.4500\n",
            "Epoch 767/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1900 - acc: 0.9325 - val_loss: 3.6960 - val_acc: 0.4650\n",
            "Epoch 768/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.1928 - acc: 0.9513 - val_loss: 3.6060 - val_acc: 0.4850\n",
            "Epoch 769/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1087 - acc: 0.9713 - val_loss: 3.7972 - val_acc: 0.5050\n",
            "Epoch 770/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1142 - acc: 0.9662 - val_loss: 3.7321 - val_acc: 0.4950\n",
            "Epoch 771/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0745 - acc: 0.9763 - val_loss: 3.7792 - val_acc: 0.4650\n",
            "Epoch 772/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.0981 - acc: 0.9675 - val_loss: 3.8806 - val_acc: 0.4650\n",
            "Epoch 773/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1114 - acc: 0.9675 - val_loss: 3.8594 - val_acc: 0.5000\n",
            "Epoch 774/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.1787 - acc: 0.9475 - val_loss: 3.7248 - val_acc: 0.4850\n",
            "Epoch 775/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1007 - acc: 0.9738 - val_loss: 3.6943 - val_acc: 0.5050\n",
            "Epoch 776/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1058 - acc: 0.9612 - val_loss: 3.8664 - val_acc: 0.5000\n",
            "Epoch 777/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1102 - acc: 0.9738 - val_loss: 3.8384 - val_acc: 0.4850\n",
            "Epoch 778/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1191 - acc: 0.9725 - val_loss: 4.1125 - val_acc: 0.4800\n",
            "Epoch 779/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1080 - acc: 0.9675 - val_loss: 3.8231 - val_acc: 0.4750\n",
            "Epoch 780/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.1180 - acc: 0.9600 - val_loss: 3.8300 - val_acc: 0.4800\n",
            "Epoch 781/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1441 - acc: 0.9612 - val_loss: 3.7155 - val_acc: 0.4600\n",
            "Epoch 782/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1989 - acc: 0.9413 - val_loss: 3.7620 - val_acc: 0.5050\n",
            "Epoch 783/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1797 - acc: 0.9462 - val_loss: 3.7257 - val_acc: 0.4950\n",
            "Epoch 784/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1761 - acc: 0.9513 - val_loss: 3.7895 - val_acc: 0.4800\n",
            "Epoch 785/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.1762 - acc: 0.9500 - val_loss: 3.9707 - val_acc: 0.4450\n",
            "Epoch 786/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2492 - acc: 0.9237 - val_loss: 3.6051 - val_acc: 0.4950\n",
            "Epoch 787/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.2073 - acc: 0.9363 - val_loss: 3.3460 - val_acc: 0.4600\n",
            "Epoch 788/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.2191 - acc: 0.9175 - val_loss: 3.3851 - val_acc: 0.4600\n",
            "Epoch 789/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.1403 - acc: 0.9563 - val_loss: 3.7843 - val_acc: 0.4900\n",
            "Epoch 790/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.1385 - acc: 0.9550 - val_loss: 3.3833 - val_acc: 0.4550\n",
            "Epoch 791/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0984 - acc: 0.9637 - val_loss: 3.4720 - val_acc: 0.4950\n",
            "Epoch 792/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0803 - acc: 0.9812 - val_loss: 3.5770 - val_acc: 0.5000\n",
            "Epoch 793/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.0791 - acc: 0.9812 - val_loss: 3.6940 - val_acc: 0.5000\n",
            "Epoch 794/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.0839 - acc: 0.9738 - val_loss: 3.7097 - val_acc: 0.5100\n",
            "Epoch 795/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1389 - acc: 0.9600 - val_loss: 3.9417 - val_acc: 0.4700\n",
            "Epoch 796/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1388 - acc: 0.9550 - val_loss: 3.7334 - val_acc: 0.4950\n",
            "Epoch 797/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 0.1080 - acc: 0.9725 - val_loss: 3.6601 - val_acc: 0.5050\n",
            "Epoch 798/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0750 - acc: 0.9788 - val_loss: 3.6733 - val_acc: 0.5100\n",
            "Epoch 799/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.0633 - acc: 0.9800 - val_loss: 3.7293 - val_acc: 0.5100\n",
            "Epoch 800/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.0867 - acc: 0.9788 - val_loss: 3.7880 - val_acc: 0.5150\n",
            "Epoch 801/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0808 - acc: 0.9775 - val_loss: 3.7636 - val_acc: 0.5050\n",
            "Epoch 802/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.0494 - acc: 0.9875 - val_loss: 3.9503 - val_acc: 0.5150\n",
            "Epoch 803/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.0742 - acc: 0.9713 - val_loss: 3.9811 - val_acc: 0.5350\n",
            "Epoch 804/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.0536 - acc: 0.9825 - val_loss: 4.0839 - val_acc: 0.5050\n",
            "Epoch 805/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.0980 - acc: 0.9775 - val_loss: 4.0866 - val_acc: 0.5000\n",
            "Epoch 806/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0712 - acc: 0.9812 - val_loss: 4.1897 - val_acc: 0.5300\n",
            "Epoch 807/1000\n",
            "800/800 [==============================] - 0s 243us/step - loss: 0.0972 - acc: 0.9763 - val_loss: 4.1731 - val_acc: 0.4650\n",
            "Epoch 808/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.2365 - acc: 0.9288 - val_loss: 4.1905 - val_acc: 0.4550\n",
            "Epoch 809/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1740 - acc: 0.9462 - val_loss: 4.1985 - val_acc: 0.4850\n",
            "Epoch 810/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1334 - acc: 0.9650 - val_loss: 3.9952 - val_acc: 0.4900\n",
            "Epoch 811/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1367 - acc: 0.9612 - val_loss: 3.5150 - val_acc: 0.5100\n",
            "Epoch 812/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1113 - acc: 0.9688 - val_loss: 3.6959 - val_acc: 0.5050\n",
            "Epoch 813/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.0996 - acc: 0.9688 - val_loss: 3.8291 - val_acc: 0.4900\n",
            "Epoch 814/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.1386 - acc: 0.9625 - val_loss: 3.9243 - val_acc: 0.4750\n",
            "Epoch 815/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.3235 - acc: 0.9138 - val_loss: 4.1287 - val_acc: 0.4750\n",
            "Epoch 816/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.3419 - acc: 0.9100 - val_loss: 3.5624 - val_acc: 0.4950\n",
            "Epoch 817/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.3454 - acc: 0.8987 - val_loss: 3.3116 - val_acc: 0.4650\n",
            "Epoch 818/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1558 - acc: 0.9513 - val_loss: 3.2901 - val_acc: 0.4400\n",
            "Epoch 819/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1729 - acc: 0.9513 - val_loss: 3.3870 - val_acc: 0.4850\n",
            "Epoch 820/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1441 - acc: 0.9500 - val_loss: 3.3069 - val_acc: 0.4950\n",
            "Epoch 821/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.1055 - acc: 0.9700 - val_loss: 3.2326 - val_acc: 0.4850\n",
            "Epoch 822/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1017 - acc: 0.9650 - val_loss: 3.3995 - val_acc: 0.4850\n",
            "Epoch 823/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.0909 - acc: 0.9800 - val_loss: 3.4751 - val_acc: 0.5100\n",
            "Epoch 824/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.0996 - acc: 0.9700 - val_loss: 3.5288 - val_acc: 0.4950\n",
            "Epoch 825/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.0727 - acc: 0.9875 - val_loss: 3.7765 - val_acc: 0.5250\n",
            "Epoch 826/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.0880 - acc: 0.9738 - val_loss: 3.8783 - val_acc: 0.4850\n",
            "Epoch 827/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.1011 - acc: 0.9713 - val_loss: 3.6320 - val_acc: 0.5000\n",
            "Epoch 828/1000\n",
            "800/800 [==============================] - 0s 278us/step - loss: 0.1031 - acc: 0.9688 - val_loss: 3.8651 - val_acc: 0.4750\n",
            "Epoch 829/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.0988 - acc: 0.9738 - val_loss: 4.0784 - val_acc: 0.4600\n",
            "Epoch 830/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.0695 - acc: 0.9812 - val_loss: 4.0478 - val_acc: 0.5050\n",
            "Epoch 831/1000\n",
            "800/800 [==============================] - 0s 278us/step - loss: 0.0881 - acc: 0.9775 - val_loss: 3.9640 - val_acc: 0.4650\n",
            "Epoch 832/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.1268 - acc: 0.9675 - val_loss: 4.3401 - val_acc: 0.4750\n",
            "Epoch 833/1000\n",
            "800/800 [==============================] - 0s 280us/step - loss: 0.1409 - acc: 0.9563 - val_loss: 3.9115 - val_acc: 0.5050\n",
            "Epoch 834/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1106 - acc: 0.9637 - val_loss: 3.9045 - val_acc: 0.4800\n",
            "Epoch 835/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.0959 - acc: 0.9738 - val_loss: 3.8764 - val_acc: 0.5150\n",
            "Epoch 836/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1386 - acc: 0.9625 - val_loss: 3.8261 - val_acc: 0.4950\n",
            "Epoch 837/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.0843 - acc: 0.9775 - val_loss: 3.8063 - val_acc: 0.4800\n",
            "Epoch 838/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1157 - acc: 0.9625 - val_loss: 3.7347 - val_acc: 0.5050\n",
            "Epoch 839/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.1024 - acc: 0.9725 - val_loss: 4.1476 - val_acc: 0.4800\n",
            "Epoch 840/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0985 - acc: 0.9662 - val_loss: 3.8851 - val_acc: 0.4800\n",
            "Epoch 841/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1220 - acc: 0.9637 - val_loss: 3.9310 - val_acc: 0.4750\n",
            "Epoch 842/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.0842 - acc: 0.9637 - val_loss: 3.8818 - val_acc: 0.5150\n",
            "Epoch 843/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.1625 - acc: 0.9513 - val_loss: 3.7924 - val_acc: 0.5100\n",
            "Epoch 844/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.0776 - acc: 0.9713 - val_loss: 3.7921 - val_acc: 0.5050\n",
            "Epoch 845/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.1094 - acc: 0.9700 - val_loss: 3.7210 - val_acc: 0.4950\n",
            "Epoch 846/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.1095 - acc: 0.9700 - val_loss: 3.9371 - val_acc: 0.5000\n",
            "Epoch 847/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0809 - acc: 0.9713 - val_loss: 3.7945 - val_acc: 0.4900\n",
            "Epoch 848/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.1087 - acc: 0.9775 - val_loss: 3.9442 - val_acc: 0.4900\n",
            "Epoch 849/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.0804 - acc: 0.9725 - val_loss: 4.1157 - val_acc: 0.4950\n",
            "Epoch 850/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0922 - acc: 0.9788 - val_loss: 4.1346 - val_acc: 0.4700\n",
            "Epoch 851/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.1068 - acc: 0.9750 - val_loss: 4.2143 - val_acc: 0.4950\n",
            "Epoch 852/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0910 - acc: 0.9812 - val_loss: 4.0027 - val_acc: 0.5150\n",
            "Epoch 853/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.1011 - acc: 0.9738 - val_loss: 4.0214 - val_acc: 0.4850\n",
            "Epoch 854/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1341 - acc: 0.9662 - val_loss: 4.0400 - val_acc: 0.4950\n",
            "Epoch 855/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0992 - acc: 0.9713 - val_loss: 4.0214 - val_acc: 0.4700\n",
            "Epoch 856/1000\n",
            "800/800 [==============================] - 0s 253us/step - loss: 0.1152 - acc: 0.9625 - val_loss: 4.0332 - val_acc: 0.4850\n",
            "Epoch 857/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.1005 - acc: 0.9713 - val_loss: 3.8374 - val_acc: 0.4800\n",
            "Epoch 858/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.0967 - acc: 0.9800 - val_loss: 3.8377 - val_acc: 0.5000\n",
            "Epoch 859/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.0724 - acc: 0.9788 - val_loss: 4.0762 - val_acc: 0.5100\n",
            "Epoch 860/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0709 - acc: 0.9838 - val_loss: 3.8890 - val_acc: 0.5000\n",
            "Epoch 861/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0724 - acc: 0.9838 - val_loss: 4.0463 - val_acc: 0.5300\n",
            "Epoch 862/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.0528 - acc: 0.9850 - val_loss: 4.1886 - val_acc: 0.5100\n",
            "Epoch 863/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0607 - acc: 0.9838 - val_loss: 4.1502 - val_acc: 0.5200\n",
            "Epoch 864/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.0454 - acc: 0.9937 - val_loss: 4.2133 - val_acc: 0.5150\n",
            "Epoch 865/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.0538 - acc: 0.9888 - val_loss: 4.2123 - val_acc: 0.5050\n",
            "Epoch 866/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0625 - acc: 0.9875 - val_loss: 4.2979 - val_acc: 0.5000\n",
            "Epoch 867/1000\n",
            "800/800 [==============================] - 0s 281us/step - loss: 0.0608 - acc: 0.9838 - val_loss: 4.4157 - val_acc: 0.5100\n",
            "Epoch 868/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0949 - acc: 0.9800 - val_loss: 4.5377 - val_acc: 0.4650\n",
            "Epoch 869/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0830 - acc: 0.9775 - val_loss: 4.3714 - val_acc: 0.5100\n",
            "Epoch 870/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.1387 - acc: 0.9575 - val_loss: 4.4185 - val_acc: 0.4800\n",
            "Epoch 871/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.1840 - acc: 0.9513 - val_loss: 4.1213 - val_acc: 0.5000\n",
            "Epoch 872/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.1294 - acc: 0.9612 - val_loss: 4.0267 - val_acc: 0.4900\n",
            "Epoch 873/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.0869 - acc: 0.9738 - val_loss: 3.8885 - val_acc: 0.5000\n",
            "Epoch 874/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.0996 - acc: 0.9775 - val_loss: 4.0211 - val_acc: 0.4850\n",
            "Epoch 875/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.0743 - acc: 0.9750 - val_loss: 3.9555 - val_acc: 0.4950\n",
            "Epoch 876/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.1001 - acc: 0.9675 - val_loss: 4.0630 - val_acc: 0.5000\n",
            "Epoch 877/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1047 - acc: 0.9688 - val_loss: 4.0505 - val_acc: 0.5000\n",
            "Epoch 878/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0560 - acc: 0.9862 - val_loss: 4.1102 - val_acc: 0.5050\n",
            "Epoch 879/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.0906 - acc: 0.9788 - val_loss: 4.3633 - val_acc: 0.4850\n",
            "Epoch 880/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0583 - acc: 0.9875 - val_loss: 4.1464 - val_acc: 0.5100\n",
            "Epoch 881/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.0429 - acc: 0.9888 - val_loss: 4.2272 - val_acc: 0.4950\n",
            "Epoch 882/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.0875 - acc: 0.9750 - val_loss: 4.3787 - val_acc: 0.4850\n",
            "Epoch 883/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.0867 - acc: 0.9675 - val_loss: 4.3079 - val_acc: 0.4950\n",
            "Epoch 884/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1067 - acc: 0.9675 - val_loss: 4.1172 - val_acc: 0.5200\n",
            "Epoch 885/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1079 - acc: 0.9725 - val_loss: 4.0762 - val_acc: 0.5100\n",
            "Epoch 886/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.0868 - acc: 0.9800 - val_loss: 4.1301 - val_acc: 0.5200\n",
            "Epoch 887/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1336 - acc: 0.9575 - val_loss: 4.1206 - val_acc: 0.4500\n",
            "Epoch 888/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.1825 - acc: 0.9375 - val_loss: 4.2564 - val_acc: 0.4950\n",
            "Epoch 889/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1412 - acc: 0.9637 - val_loss: 3.9399 - val_acc: 0.5000\n",
            "Epoch 890/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1634 - acc: 0.9537 - val_loss: 3.7920 - val_acc: 0.5000\n",
            "Epoch 891/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1439 - acc: 0.9513 - val_loss: 4.0307 - val_acc: 0.4850\n",
            "Epoch 892/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1139 - acc: 0.9612 - val_loss: 4.0385 - val_acc: 0.4450\n",
            "Epoch 893/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.1697 - acc: 0.9400 - val_loss: 3.8278 - val_acc: 0.4800\n",
            "Epoch 894/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0897 - acc: 0.9725 - val_loss: 3.7722 - val_acc: 0.5100\n",
            "Epoch 895/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.0855 - acc: 0.9750 - val_loss: 3.8445 - val_acc: 0.5100\n",
            "Epoch 896/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0621 - acc: 0.9838 - val_loss: 3.8439 - val_acc: 0.5200\n",
            "Epoch 897/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.1278 - acc: 0.9637 - val_loss: 3.9869 - val_acc: 0.4950\n",
            "Epoch 898/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1029 - acc: 0.9700 - val_loss: 3.7852 - val_acc: 0.4950\n",
            "Epoch 899/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1319 - acc: 0.9688 - val_loss: 4.4714 - val_acc: 0.4750\n",
            "Epoch 900/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.2571 - acc: 0.9163 - val_loss: 4.1055 - val_acc: 0.4650\n",
            "Epoch 901/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.3672 - acc: 0.9037 - val_loss: 3.8052 - val_acc: 0.4650\n",
            "Epoch 902/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.2452 - acc: 0.9187 - val_loss: 3.3469 - val_acc: 0.4750\n",
            "Epoch 903/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.2957 - acc: 0.9150 - val_loss: 3.3687 - val_acc: 0.4600\n",
            "Epoch 904/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.3050 - acc: 0.9050 - val_loss: 2.9224 - val_acc: 0.4700\n",
            "Epoch 905/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1973 - acc: 0.9387 - val_loss: 3.2194 - val_acc: 0.4800\n",
            "Epoch 906/1000\n",
            "800/800 [==============================] - 0s 255us/step - loss: 0.1850 - acc: 0.9375 - val_loss: 2.9438 - val_acc: 0.4700\n",
            "Epoch 907/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1564 - acc: 0.9537 - val_loss: 3.1935 - val_acc: 0.4700\n",
            "Epoch 908/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1506 - acc: 0.9475 - val_loss: 3.1473 - val_acc: 0.5150\n",
            "Epoch 909/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.1280 - acc: 0.9675 - val_loss: 3.1713 - val_acc: 0.5050\n",
            "Epoch 910/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1535 - acc: 0.9563 - val_loss: 3.1790 - val_acc: 0.4800\n",
            "Epoch 911/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1160 - acc: 0.9600 - val_loss: 3.7340 - val_acc: 0.5150\n",
            "Epoch 912/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1236 - acc: 0.9563 - val_loss: 3.3218 - val_acc: 0.5050\n",
            "Epoch 913/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1069 - acc: 0.9725 - val_loss: 3.5267 - val_acc: 0.5050\n",
            "Epoch 914/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.1437 - acc: 0.9625 - val_loss: 3.4227 - val_acc: 0.5350\n",
            "Epoch 915/1000\n",
            "800/800 [==============================] - 0s 286us/step - loss: 0.1309 - acc: 0.9575 - val_loss: 3.2010 - val_acc: 0.5200\n",
            "Epoch 916/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0797 - acc: 0.9750 - val_loss: 3.6170 - val_acc: 0.5250\n",
            "Epoch 917/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.0961 - acc: 0.9725 - val_loss: 3.5687 - val_acc: 0.5050\n",
            "Epoch 918/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0842 - acc: 0.9750 - val_loss: 3.7369 - val_acc: 0.5200\n",
            "Epoch 919/1000\n",
            "800/800 [==============================] - 0s 243us/step - loss: 0.0896 - acc: 0.9763 - val_loss: 3.6357 - val_acc: 0.5150\n",
            "Epoch 920/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.0728 - acc: 0.9812 - val_loss: 3.6882 - val_acc: 0.5050\n",
            "Epoch 921/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.0646 - acc: 0.9825 - val_loss: 3.5999 - val_acc: 0.5400\n",
            "Epoch 922/1000\n",
            "800/800 [==============================] - 0s 261us/step - loss: 0.0905 - acc: 0.9675 - val_loss: 3.9067 - val_acc: 0.4750\n",
            "Epoch 923/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.1736 - acc: 0.9587 - val_loss: 4.1352 - val_acc: 0.5000\n",
            "Epoch 924/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1744 - acc: 0.9462 - val_loss: 3.8664 - val_acc: 0.4900\n",
            "Epoch 925/1000\n",
            "800/800 [==============================] - 0s 280us/step - loss: 0.1361 - acc: 0.9550 - val_loss: 3.7373 - val_acc: 0.5000\n",
            "Epoch 926/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.1490 - acc: 0.9537 - val_loss: 3.5846 - val_acc: 0.4500\n",
            "Epoch 927/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0828 - acc: 0.9738 - val_loss: 3.5482 - val_acc: 0.5150\n",
            "Epoch 928/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.1208 - acc: 0.9637 - val_loss: 3.5237 - val_acc: 0.4600\n",
            "Epoch 929/1000\n",
            "800/800 [==============================] - 0s 277us/step - loss: 0.0840 - acc: 0.9788 - val_loss: 3.6842 - val_acc: 0.4850\n",
            "Epoch 930/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1338 - acc: 0.9575 - val_loss: 3.8799 - val_acc: 0.4750\n",
            "Epoch 931/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0766 - acc: 0.9800 - val_loss: 3.7841 - val_acc: 0.4750\n",
            "Epoch 932/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0922 - acc: 0.9713 - val_loss: 3.6307 - val_acc: 0.4600\n",
            "Epoch 933/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0984 - acc: 0.9763 - val_loss: 3.6017 - val_acc: 0.4900\n",
            "Epoch 934/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0993 - acc: 0.9713 - val_loss: 3.9546 - val_acc: 0.4950\n",
            "Epoch 935/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0705 - acc: 0.9812 - val_loss: 3.8125 - val_acc: 0.4900\n",
            "Epoch 936/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.0623 - acc: 0.9825 - val_loss: 3.7356 - val_acc: 0.5500\n",
            "Epoch 937/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0752 - acc: 0.9788 - val_loss: 3.7927 - val_acc: 0.4900\n",
            "Epoch 938/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.0644 - acc: 0.9788 - val_loss: 3.8571 - val_acc: 0.5050\n",
            "Epoch 939/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0719 - acc: 0.9875 - val_loss: 3.9411 - val_acc: 0.4900\n",
            "Epoch 940/1000\n",
            "800/800 [==============================] - 0s 269us/step - loss: 0.0768 - acc: 0.9788 - val_loss: 3.9575 - val_acc: 0.5050\n",
            "Epoch 941/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.0522 - acc: 0.9913 - val_loss: 4.0558 - val_acc: 0.5050\n",
            "Epoch 942/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.0554 - acc: 0.9800 - val_loss: 4.0675 - val_acc: 0.5100\n",
            "Epoch 943/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.0777 - acc: 0.9763 - val_loss: 4.1086 - val_acc: 0.5250\n",
            "Epoch 944/1000\n",
            "800/800 [==============================] - 0s 285us/step - loss: 0.0318 - acc: 0.9913 - val_loss: 4.0970 - val_acc: 0.5150\n",
            "Epoch 945/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0567 - acc: 0.9825 - val_loss: 4.1364 - val_acc: 0.5100\n",
            "Epoch 946/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0414 - acc: 0.9862 - val_loss: 4.3596 - val_acc: 0.4950\n",
            "Epoch 947/1000\n",
            "800/800 [==============================] - 0s 273us/step - loss: 0.0942 - acc: 0.9700 - val_loss: 4.3087 - val_acc: 0.5150\n",
            "Epoch 948/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.1222 - acc: 0.9688 - val_loss: 4.2408 - val_acc: 0.4800\n",
            "Epoch 949/1000\n",
            "800/800 [==============================] - 0s 276us/step - loss: 0.0973 - acc: 0.9700 - val_loss: 4.1485 - val_acc: 0.4850\n",
            "Epoch 950/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.0629 - acc: 0.9838 - val_loss: 4.2120 - val_acc: 0.5150\n",
            "Epoch 951/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 0.0708 - acc: 0.9750 - val_loss: 4.2433 - val_acc: 0.4950\n",
            "Epoch 952/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.0687 - acc: 0.9838 - val_loss: 4.4458 - val_acc: 0.4850\n",
            "Epoch 953/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1192 - acc: 0.9612 - val_loss: 4.4895 - val_acc: 0.5150\n",
            "Epoch 954/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.1100 - acc: 0.9750 - val_loss: 4.2193 - val_acc: 0.5100\n",
            "Epoch 955/1000\n",
            "800/800 [==============================] - 0s 246us/step - loss: 0.1392 - acc: 0.9637 - val_loss: 4.1353 - val_acc: 0.4750\n",
            "Epoch 956/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.1054 - acc: 0.9662 - val_loss: 4.0770 - val_acc: 0.5400\n",
            "Epoch 957/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1072 - acc: 0.9688 - val_loss: 4.3420 - val_acc: 0.4950\n",
            "Epoch 958/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1246 - acc: 0.9575 - val_loss: 4.1201 - val_acc: 0.5300\n",
            "Epoch 959/1000\n",
            "800/800 [==============================] - 0s 268us/step - loss: 0.1160 - acc: 0.9600 - val_loss: 3.9967 - val_acc: 0.4700\n",
            "Epoch 960/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0759 - acc: 0.9763 - val_loss: 4.2757 - val_acc: 0.5100\n",
            "Epoch 961/1000\n",
            "800/800 [==============================] - 0s 257us/step - loss: 0.1088 - acc: 0.9662 - val_loss: 4.0218 - val_acc: 0.5000\n",
            "Epoch 962/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.1680 - acc: 0.9537 - val_loss: 3.8251 - val_acc: 0.4600\n",
            "Epoch 963/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.1727 - acc: 0.9513 - val_loss: 3.8386 - val_acc: 0.4900\n",
            "Epoch 964/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0985 - acc: 0.9738 - val_loss: 3.8217 - val_acc: 0.5000\n",
            "Epoch 965/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0753 - acc: 0.9788 - val_loss: 3.5526 - val_acc: 0.5350\n",
            "Epoch 966/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0734 - acc: 0.9788 - val_loss: 3.7307 - val_acc: 0.4750\n",
            "Epoch 967/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0600 - acc: 0.9850 - val_loss: 3.7133 - val_acc: 0.4900\n",
            "Epoch 968/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.0680 - acc: 0.9788 - val_loss: 3.8130 - val_acc: 0.4950\n",
            "Epoch 969/1000\n",
            "800/800 [==============================] - 0s 256us/step - loss: 0.0649 - acc: 0.9838 - val_loss: 4.0465 - val_acc: 0.5150\n",
            "Epoch 970/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0719 - acc: 0.9812 - val_loss: 4.0695 - val_acc: 0.5400\n",
            "Epoch 971/1000\n",
            "800/800 [==============================] - 0s 254us/step - loss: 0.0427 - acc: 0.9862 - val_loss: 4.2651 - val_acc: 0.5200\n",
            "Epoch 972/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.0666 - acc: 0.9800 - val_loss: 4.0358 - val_acc: 0.5350\n",
            "Epoch 973/1000\n",
            "800/800 [==============================] - 0s 270us/step - loss: 0.0351 - acc: 0.9950 - val_loss: 4.2940 - val_acc: 0.4850\n",
            "Epoch 974/1000\n",
            "800/800 [==============================] - 0s 264us/step - loss: 0.0249 - acc: 0.9937 - val_loss: 4.1694 - val_acc: 0.5100\n",
            "Epoch 975/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.0390 - acc: 0.9900 - val_loss: 4.0733 - val_acc: 0.5050\n",
            "Epoch 976/1000\n",
            "800/800 [==============================] - 0s 265us/step - loss: 0.0442 - acc: 0.9812 - val_loss: 4.1786 - val_acc: 0.5250\n",
            "Epoch 977/1000\n",
            "800/800 [==============================] - 0s 260us/step - loss: 0.0917 - acc: 0.9738 - val_loss: 4.2215 - val_acc: 0.5450\n",
            "Epoch 978/1000\n",
            "800/800 [==============================] - 0s 271us/step - loss: 0.2095 - acc: 0.9563 - val_loss: 4.3213 - val_acc: 0.5200\n",
            "Epoch 979/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 0.1713 - acc: 0.9563 - val_loss: 3.9752 - val_acc: 0.5200\n",
            "Epoch 980/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.1080 - acc: 0.9650 - val_loss: 4.0011 - val_acc: 0.4950\n",
            "Epoch 981/1000\n",
            "800/800 [==============================] - 0s 272us/step - loss: 0.1373 - acc: 0.9713 - val_loss: 3.9280 - val_acc: 0.5200\n",
            "Epoch 982/1000\n",
            "800/800 [==============================] - 0s 274us/step - loss: 0.0881 - acc: 0.9750 - val_loss: 3.9516 - val_acc: 0.5100\n",
            "Epoch 983/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0883 - acc: 0.9750 - val_loss: 4.0649 - val_acc: 0.4950\n",
            "Epoch 984/1000\n",
            "800/800 [==============================] - 0s 262us/step - loss: 0.0966 - acc: 0.9675 - val_loss: 4.0683 - val_acc: 0.5150\n",
            "Epoch 985/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0905 - acc: 0.9812 - val_loss: 3.9699 - val_acc: 0.5000\n",
            "Epoch 986/1000\n",
            "800/800 [==============================] - 0s 251us/step - loss: 0.0796 - acc: 0.9763 - val_loss: 3.9946 - val_acc: 0.5100\n",
            "Epoch 987/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.0889 - acc: 0.9800 - val_loss: 3.9051 - val_acc: 0.5250\n",
            "Epoch 988/1000\n",
            "800/800 [==============================] - 0s 267us/step - loss: 0.1063 - acc: 0.9725 - val_loss: 3.8665 - val_acc: 0.5050\n",
            "Epoch 989/1000\n",
            "800/800 [==============================] - 0s 252us/step - loss: 0.0783 - acc: 0.9775 - val_loss: 3.9011 - val_acc: 0.4900\n",
            "Epoch 990/1000\n",
            "800/800 [==============================] - 0s 263us/step - loss: 0.0867 - acc: 0.9812 - val_loss: 3.8662 - val_acc: 0.5300\n",
            "Epoch 991/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0555 - acc: 0.9862 - val_loss: 3.7653 - val_acc: 0.5000\n",
            "Epoch 992/1000\n",
            "800/800 [==============================] - 0s 275us/step - loss: 0.0896 - acc: 0.9738 - val_loss: 4.0267 - val_acc: 0.5350\n",
            "Epoch 993/1000\n",
            "800/800 [==============================] - 0s 250us/step - loss: 0.0733 - acc: 0.9738 - val_loss: 4.0609 - val_acc: 0.4750\n",
            "Epoch 994/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.0766 - acc: 0.9738 - val_loss: 4.2386 - val_acc: 0.4750\n",
            "Epoch 995/1000\n",
            "800/800 [==============================] - 0s 258us/step - loss: 0.1649 - acc: 0.9513 - val_loss: 4.0646 - val_acc: 0.5100\n",
            "Epoch 996/1000\n",
            "800/800 [==============================] - 0s 245us/step - loss: 0.2885 - acc: 0.9200 - val_loss: 4.0573 - val_acc: 0.4450\n",
            "Epoch 997/1000\n",
            "800/800 [==============================] - 0s 278us/step - loss: 0.4184 - acc: 0.8837 - val_loss: 3.5719 - val_acc: 0.4350\n",
            "Epoch 998/1000\n",
            "800/800 [==============================] - 0s 259us/step - loss: 0.3354 - acc: 0.8813 - val_loss: 3.5395 - val_acc: 0.5000\n",
            "Epoch 999/1000\n",
            "800/800 [==============================] - 0s 266us/step - loss: 0.2721 - acc: 0.8962 - val_loss: 2.4931 - val_acc: 0.5200\n",
            "Epoch 1000/1000\n",
            "800/800 [==============================] - 0s 248us/step - loss: 0.1939 - acc: 0.9563 - val_loss: 2.6463 - val_acc: 0.5050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f89139d8940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "metadata": {
        "id": "bYEbOn_l2Xz_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "names = ['karim', 'omar', 'amr', 'farouq']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DoealRI8pKln",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "99c50a12-4954-4219-b52a-cd99122dff84"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 0s 171us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.6463146209716797, 0.505]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "ie2IhPK8og7o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "6ffc9159-a963-4163-cbd0-6d6578eaca05"
      },
      "cell_type": "code",
      "source": [
        "index = np.random.randint(0, 800)\n",
        "plt.imshow(X_train[index].reshape(48, 48), cmap='gray')\n",
        "names[np.argmax(model.predict(X_train[index].reshape(1, -1)))]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'omar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFLCAYAAABft66eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPChZioh4ULIUEfGgZCki4uF/tcf7L+sp5vYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f891242f6d8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "QXN-g0BApmBW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "a1999cce-6705-4d56-f414-6ec8bdfc3476"
      },
      "cell_type": "code",
      "source": [
        "index = np.random.randint(0, 200)\n",
        "plt.imshow(X_test[index].reshape(48, 48), cmap='gray')\n",
        "names[np.argmax(model.predict(X_train[index].reshape(1, -1)))]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'amr'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFLCAYAAABft66eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3X1sVuX9P/B3pZbSCpSWtrSgoEQB\neRCZuBVEJeADbsvYskXSaTaXGROi0z8cECTzDx8myAjqEjUSWDKzWNMt/jFnypziWFIQEIkgo4Dy\naOkTtRR63xWQ3x/L6a+9+3lfva7zLXdpeb/+4v70vs91zrlPL07P53NdV8aFCxcuQEREnK7o6x0Q\nEekP1FmKiHhQZyki4kGdpYiIB3WWIiIe1FmKiHjIjPvB559/Hrt27UJGRgaWL1+OadOm9eZ+iYhc\nUmJ1lh9//DEOHz6MiooKHDx4EMuXL0dFRQV9f05OTpfX27Ztw8yZM+M03WsGDRpEfzZkyBAznplp\nn65hw4aZ8ZMnT3Z5/dFHH+GOO+4AAGRlZZmfaWlpMeN5eXlm/Ior7D8OGhoazDgAjB071ozX1taa\ncXbc2dnZtI3Ox975+y4pKTHf39jYaMavueYaM/7ll1/StkeMGGHG2blqbm4240OHDqVtnDlzxox3\n/p6qqqpwzz33AADa29vN9xcUFNA2rrrqKjOeSCTM+MGDB814Mpk04/n5+bRt1sa5c+fMeOq59fkd\nP3/+vBln19s333xjxl3X4bfffhu0LRYHYv4ZXl1djfnz5wMAxo8fj5aWFpw+fdr785MnT47TbL83\nadKkvt6FPnG5ft8TJkzo613oMzfeeGNf70Kvi9VZNjY2dvnfOz8/33knIyLS38V+ZtlZTyMmt23b\n1u3uoq2trTea7nfq6+v7ehf6xOX6fR86dKivd6HPsEcV/VWszrKoqKjLM6b6+noUFhbS96c+u2hr\na+v2HDPd+uKZZX19PYqKigBcXs8sO3/fl9Mzy0OHDmHcuHEALr9nlmfOnEFubi7dPnCZPLOcPXs2\nqqqqAAB79uxBUVER/WJFRAaCWHeWM2bMwOTJk7Fo0SJkZGTg6aef7u39uujiTLbE/pfKyMjwjrP3\nRtgdb+j/wnGwbbG22Z0a0P1csXMXYeeFfe7KK6+k2zp79qwZZ38xhH6vLqmf6Wkbrrsi9jN2l8qO\nw/U9MaHbst7/7bffOttm54b9brLr03WO2bUQR+zftCeffLLXdkJE5FKnETwiIh7UWYqIeFBnKSLi\nQZ2liIiH3kul9jNxsuGhn7EygVGMZRtZG6y+jdWyubLPbFssE89qz+JkWRl23HEyvKHfEztuVxvs\nZ4MHDzZfs+NgdaQAr7M8cuSIGWdZclfGnWHXCKsPts55RkZGrN8zlsFOPbcRVq0B8Ax6nCoS3VmK\niHhQZyki4kGdpYiIB3WWIiIe1FmKiHhQZyki4kGlQwF6mgwiBCtdYFOxsWnE2DRbrokmWCkJmx6O\nHber/CL1/EavWclNaNx1fKzEhO0vKx2K00bquY1es0k82HR5AL9G9+3bZ8bZ98S+b9fvADt29hlW\nJuf6nQmdqIS93zWtGvueWGmUi+4sRUQ8qLMUEfGgzlJExIM6SxERD+osRUQ8pCUbbmXQ4mSje5Or\nfZZdC11kzcq4RTGW2WOLn7FVIdnCUsXFxUH7BfBJOVgbrmxx6s+i1yy7zTK2LIPNJnRwtcHibMIK\nVxutra1mPLVqIXrN2t61axdto7S01IynLoQXYd8Hu27ZMQC8QoBNcmH9Pp0/f96ZDWdtpGNplTj9\nj+4sRUQ8qLMUEfGgzlJExIM6SxERD+osRUQ8pCUb7lpeIVTowuyM6/0s+8uyxexYXNnw0Gn7CwoK\nzPipU6fM+Lhx48w4ANTV1ZlxdnyhSz4AfHkFltGMk5FmQsd6szHxbJkG18+SyWSX1z1lw1mVA8Cz\nvGxb7PtzjZ1m2Hln3x+Lu37P2WfY9xT6OxP3M4zuLEVEPKizFBHxoM5SRMSDOksREQ/qLEVEPKQl\nG+7KCltcmeremq3claVjGffUTGeEZS2t8dlR7NixY+ZnWJaVjRln++TKgJ45c8aMs9m8WebQ9R2m\nfiZ6zb7badOmmfHa2lozzjK/AK8QaGtrM+PsHLJZtgF+/aQeX/Q+lqFn5xzgFQLsu2XfB/u+WTYa\nCN9fa8z44MGDnddIb82a7+oT2PXmOnZGd5YiIh7UWYqIeFBnKSLiQZ2liIgHdZYiIh7UWYqIeEhL\n6ZA1MP7KK6+kpQCsbAHg5QOhkw642khdGiDCyhBYeYRVghTFWFkIK29hyy6UlJSYcVfpUOgkCaxM\nx1UWkrqUQfSalXmw4z5x4oQZZ+U+AC/9YnFWIsSWUHD9LPV6i443tBwN4N8ha5uV9YwYMcKMnz59\nOrhtxvodz8zMdJbosAkz2DXCfsddbbDPaFkJEZGLRJ2liIgHdZYiIh7UWYqIeFBnKSLiIS3Z8MLC\nQq+Yj9AlDkLjADBy5EgzHjqQ38o2RrHQheFZ1pRlyV1Z1vz8fDPOMqAsc88mrAC6Z8OjDDz7zOHD\nh804W/LBNQEFw7K/7By6lh5g7admvaMJUFjG1pWVZddbaDUD247rHLJrmk30YrWdTCadk5GwihRW\nGcHe7zqH7GfKhouIXCTqLEVEPKizFBHxoM5SRMSDOksREQ9pyYZbGc2WlpZYmerQ7FZo9hzg2d+8\nvLygNlzLabCxwr5Z1p7aDslUR1i2kb3f1UZqFjR6zY4jNFPtGtvPsvosi8yOj33fgLvaoLOhQ4cC\nCF8qAeBj79m1y46bnXPXmGr2GfZ9WPuUlZXlrChgx8c+w8bEu9pg54SNS3fRnaWIiAd1liIiHtRZ\nioh4UGcpIuLBq7OsqanB/Pnz8eabbwL431rODz74IMrLy/H4448HTxQqItLf9JgNb2trwzPPPIOy\nsrKO2Msvv4zy8nIsWLAAa9asQWVlJcrLy+k2EomEV8wHy9KFcmUh2aL0rO2CggIzbmUbe1rcnY2l\nDR0P7Mrksgw6yxCymbbZLObWtqLXLNvPMp2hY+gBfg7Z8YXO2A3wionU/Y2+h9CZ/AHg66+/NuOh\n2X72fpbZdu0Xu3asMeODBg1yVg2EZqTjzGjPzrvrM0yPd5ZZWVl44403UFRU1BHbunUr5s2bBwCY\nO3cuqqurgxsWEelPevxvOzMzs1vvnEgkOv4XKygoQENDw8XZOxGRS8T/uSjdZ6qj7du3Y/LkyV1i\ncf8M7+/q6+v7ehf6xLZt2/p6F/rE5fxX10C7iYrVWebk5CCZTCI7Oxt1dXVd/kS33HLLLV1eJxKJ\nWPMRAr33zNK1HfYfAHtOxZ5ZNjU1dXldX1/f47kKfWbJni25Vl5kbbBzMnr0aDO+d+9e2kZpaWnH\nv7dt24aZM2cC4M/h2DOkON83e9YY+szSNTKE/azzX2HV1dUdz/rT8cwydGSPq232Gd9nlg0NDSgs\nLHReh+y8s8+w/XU9fwxdkZXNpQnELB2aNWsWqqqqAAAbN27EnDlz4mxGRKTf6PHOcvfu3Vi5ciWO\nHz+OzMxMVFVVYfXq1Vi2bBkqKipQWlqKhQsXpmNfRUT6TI+d5ZQpU/DnP/+5W3zDhg3ejVglCtnZ\n2fS22vWnF5tAgX0mzp8/7BadbYv9+ZObm0tjbBIKNm2/q8zDEi1nYGFtDx8+3IyzPwdd31PqxBjR\na3Zu2XGHfq+Afd4Bvz+dO3M9KmJ/QrJSGRaPs3SF6zMW9n27SqNCrzfL4MGDnTXY7He5p/K6VK68\nCWvDdyKUzjSCR0TEgzpLEREP6ixFRDyosxQR8aDOUkTEQ1qWlcjPzzdjvVVg7hJrMXWSKWfZQ1dm\nnWEF7q4iXktzc7MZd2WL2c9Si+gjbF+jJRMsqcfR03GxDC/LkrvaZoX/bEIQNunImDFjaBss4566\nhMrcuXMBAAcOHDDf78oWs3Ni/T4BvHLgiy++MON1dXW0bXbeQ7LLGRkZzuuQHTv7DBv1F6eNOHRn\nKSLiQZ2liIgHdZYiIh7UWYqIeFBnKSLiIS3ZcCuz1t7eTrPLrgx26GdY3DUulmW3c3JyzDjL9Fpt\nRLHQBeYZNt2aa+wr+0xqJjfCMr9sO0D3pTmiayB0PDD7/lwVCGxMNfvMhAkTzPj48eNpG2xseOqY\n/LvuugsAcNttt5nvZ+O2XW2wONvWxIkTzXhjYyNtm1VGsDkqP/nkk26xrKws53XIfgdCp2hz/S6z\n6dvijH3XnaWIiAd1liIiHtRZioh4UGcpIuJBnaWIiIe0ZMNTM6MsFnGNGWc/Y3GWQXMt8M6yayzO\nMrxWJjeKhc4GzTKEbIx0nEWcQsdnu2YSTz3v0evQRe9ZBcKMGTNo2yz7y7L3o0aNMuNsLDnAx8un\nnqvo2igsLDTfX1xcTNtg5yQ0W8yuddf4ejb+vKSkxIxbv88TJkzArl27aBvsumLY9emqKGCfcVVy\nMLqzFBHxoM5SRMSDOksREQ/qLEVEPKizFBHxkJZsuJV1Gzp0aKx1w0M/E5olB3jWm2UhWdaSHTfA\nqwHYfrHMIdtXlq2N8xmWOezNWehZZvaaa64x4zfeeCNto6CgwIyzzLrrXDFs1u7UKofo2mDXIauk\nsLYVYdcCez+Lu2avZ8fX2tpKP2NxZbzZdcjGbbe1tZlxV3/BtuWqxmF0Zyki4kGdpYiIB3WWIiIe\n1FmKiHhQZyki4kGdpYiIh7SUDlklG6yMoydxlonojffH4VpWgk0owUot2GQIrKwndKIOgJdguCYd\nYVJLYqLXbNF7Vr6Tl5dnxuMsC8COg50rV3kJK4FK/V6j0qHQtl3ts6Ua2LUTWnYGuEuaLFb53Nmz\nZ52TrYTuL/vdZ9cIAJw+fdqMuyaaYXRnKSLiQZ2liIgHdZYiIh7UWYqIeFBnKSLiIS3ZcCvjFydb\n29dY9pBltl3Y8bM2WCaQtc0mQnC1zbA2QiaBiF731pIIrskTQiegYJlRVxUAO/bU44i+T/a9uiZ0\nYccYWhHC4q5JLtg5ZFUs48aNM2PHjh2jbbBMNTN8+HAzzqoDAP574DrvjO4sRUQ8qLMUEfGgzlJE\nxIM6SxERD+osRUQ8pCUbbmUOBw0adMlmxFm2kWVyWdbUen8UY8fOsnRsDDjLaLqWDGA/C11WwrXE\nQOo5jF6zMd0jRoww4xMmTDDjWVlZtO3Q7C9b0oLFAX6uUrPFo0aNotsA3BUFbFw1i7NlF9i5co2P\nPnXqlBln187IkSPNWElJCW3jxIkTZtw1ntwSZwx/HLqzFBHxoM5SRMSDOksREQ/qLEVEPKizFBHx\nkJZsuJWFdI1LZdnoONi2XGOLQ7cVMs47mik8dKZ21gbLTrqy4SxDyMb9sqy3a2xvagY2es3aZtlw\nFncdX1NTkxnftWuXGWcZetds7Nddd50Zv+mmmzr+PWPGDBw/fhwAMHbsWPP9rNIA4NdIbW2tGd+5\nc6cZP3LkiBn/+uuvadsswzx69GgzXlRU1C2WSCRQXFxM22AVBWw8N8v2p4vuLEVEPKizFBHxoM5S\nRMSDOksREQ/qLEVEPHhlw1etWoUdO3bg3LlzeOSRRzB16lQsWbIE58+fR2FhIV588UXnWF0Rkf6u\nx85yy5Yt2L9/PyoqKtDc3Iwf//jHKCsrQ3l5ORYsWIA1a9agsrIS5eXldBtWyc2FCxdo+U6csh4m\nzrZYeQuLs7IeKx6VvLAyJDYph2vqfIurNIn9x8bKcVjbrvKv1PajbbC22dIVzc3NZnzPnj207f37\n95txVu7D4jU1NbSNd99914x3Lsf54Q9/iI8++ggAcO+995rvd5Un1dfXm/Gqqioz/vnnn5vxG264\nwYyPHz+etn3w4EEzXl1dbcbHjBnTLbZjxw5nG+w7ZyVpoeV2AP+djbWtnt4wc+ZMvPTSSwCAYcOG\nIZFIYOvWrZg3bx4AYO7cufQEiogMFD12loMGDeooVq6srMTtt9+ORCLRcYdQUFCAhoaGi7uXIiJ9\nLOOC53CZ999/H6+//jrWr1+Pu+++u+Nu8vDhw1i6dCneeust+tl9+/bReQlFRPoDrwTP5s2b8dpr\nr2HdunUYOnQocnJykEwmkZ2djbq6OnOoU2f33HNPl9eHDh3CuHHjevXZZG9izznYMzrfZ5nHjx/v\nGC4W+swydKJk1/vZczI2/IwNM3NNutr5OI4ePYqrr74aAH9mOWPGDDM+bdo0M96bzyzZf+SuZ5ZH\njx4147Nnz+749+rVq/Hkk08C6N1nln//+9/NeOgzS7a0LMCfWe7du9eMpz6z3Lx5M+bMmeN8Zhk9\nz03FhnOm45llNBzZ3FZPjbW2tmLVqlV4/fXXkZeXBwCYNWtWx0PmjRs3Ys6cOT3utIhIf9bjneU/\n/vEPNDc344knnuiIvfDCC1ixYgUqKipQWlqKhQsXOrdh3UH29l1lb06+we7K0pGlZ8cRGnctV8Cw\nLKRr0gqGLSvBJuVgSziwOwDXJBDsP+9rr73WjBcUFJjx6ObAMm7cODOem5vb5XV0XKyiIPX9nbFj\nZJNyRHfvqQoLC4O2D/AJM9hdeDRhSGclJSXOu0E2cctDDz1kxj/44AMzfuDAAdoGu7Nkf8G59NhZ\n3n///bj//vu7xTds2BDcmIhIf6URPCIiHtRZioh4UGcpIuJBnaWIiIe0LCthZcTi1EzFkY4MNotb\nmbgoxo6fLXzP2mBZb9fi8qzt0MzzHXfcQdv46U9/2uX1s88+CwBYt26d+f5PPvnEjLMa3tTtd8Zq\nMDdv3mzGT548acanT59O27jxxhvN+BdffNHldVSb++mnn5rvZ5l4gJ8Tlt1mdaRsLDm71oD/DTax\nlJWVmXFrnHdmZiY95wAwcuRIM85qUtlyIa56WFY7PGTIEPoZRneWIiIe1FmKiHhQZyki4kGdpYiI\nB3WWIiIe0pINd2WFL0Wh+5aOzH5oVj90liJXG2wcrWspkdTZZqLXs2bNMt/PZqBh2VQ2gzrAZ0li\nY5HZzDhsDDYADB482KuN6DXLuFszjEfYLFesOoHNEzBp0iQz3tLSQttmWWR2XVmzQ02bNs05vv6z\nzz4z40eOHDHjbDYiV0UBy3q7jp25dHssEZFLiDpLEREP6ixFRDyosxQR8aDOUkTEQ1qy4X0pzgzq\nLOPHst5x2mCfYZl49n62T67sOdsW+wzLIu/bt4+28dVXX5mv2f5OnTrVjN95551m3DW2l2XD2TrV\ncSoHEomEGU+d+dw1EzoAlJaW0p+x8edsbR420z0bg83GmAPAxIkTzTirQrDaGDlypLOyhO0vm9Ge\nnXPX7x/LersqORjdWYqIeFBnKSLiQZ2liIgHdZYiIh7UWYqIeFBnKSLiIS2lQ1ZpRpxyDSBemU4o\nVkLTG+U70TbYtkKPL875CJ0ohLXhmsyCTSjBJnV49913zfi5c+fMOJvIAuATf7B4Mpmk22LYRBN1\ndXVdXp85cwYAL79i5UwAcMMNN5hxVjrEtsVKhL755hvaNsPOu/U9ffvtt92W2eiMlQINHTrUjLMl\nVFipGMDLpmL93gR/QkTkMqTOUkTEgzpLEREP6ixFRDyosxQR8ZCWbLiVFQ5dJiHCMrlxtxciNINt\nZcmjWGg2LvT4XBnv0G2x97u2k5p5jl6z/WKZ9VOnTplxluUE+CQbbPKE4cOHm3GWlQW6Z70jqVUe\n0evRo0eb72eTZQB8+YhDhw6ZcZYtZktXuLLh7Gcs82wtm3HFFVc4Kw3YshnV1dVmvLGx0YwXFRXR\nNtj1E1UphNCdpYiIB3WWIiIe1FmKiHhQZyki4kGdpYiIh7Rkw60MqCtb25vLNKRDb2biQ5d8YBlQ\nF7Yt9p2weGtrK23j7bff7vj3vffe2/G6rKzMfD/LssbJWrJzwsZzs6w3G5cO8OUKUjPr0Ws2F0Jt\nbS1tg2Wk2Rh39r2yuQvy8vJo2+y8s+vTGpeemZlJ2wb4d75x48ag98dZQmXYsGH0M4zuLEVEPKiz\nFBHxoM5SRMSDOksREQ/qLEVEPKQlG25lpOJmr9MxBpxhWWFXxi90W+y8hGa944wNZ22zTK4rW/zW\nW291/Hv9+vUdrwsKCsz3s+xkU1OTGT979ixtO7RygGVZjx07Rttg33lqxj16/dVXX5nvZ2OXAX5+\nWTY8NzfXjLOM+6hRo2jbrA12LVjjvNnY78hnn33mvS2AVy24Zs1nP4szO77uLEVEPKizFBHxoM5S\nRMSDOksREQ/qLEVEPKizFBHxkJbSIatkI84EEL0pTglSaEmKq7QmdHmM0HiciUqsyRAAXibjaiO1\nNCN6/d5775nvZ2Uhrsk6GDYJBCsjOX36tBn/8ssvaRs1NTVmvPNx//rXv8bWrVsB8CUiJkyYQNtg\npUusfKewsNCMs+U0XKVDkyZNMuOJRMKMW6U4yWSSnluAf085OTlmPDs7O2g7AC8Lcy1LwujOUkTE\ngzpLEREP6ixFRDyosxQR8dBjgieRSGDZsmVoampCe3s7Fi9ejIkTJ2LJkiU4f/48CgsL8eKLL9KH\nyCIiA0GPneWHH36IKVOm4OGHH8bx48fxq1/9CjNmzEB5eTkWLFiANWvWoLKyEuXl5XQboctKuH4W\nKs4kF6HbYssVWPFoIonQKfLTMYEIy4YzrskIUv/zjF6zLDLLdMbJkufn55txNvkGm+Ti3//+N22D\n7dd1113X5fXYsWMBAN/97nfN97uWN5g4caIZP3jwoBlnE2awCSvYMQA8W8yudTZZjmvZjNDfTZaJ\nd/UXbH/j/D712Cvdd999ePjhhwH878soLi7G1q1bMW/ePADA3LlzUV1dHdywiEh/4n0rsWjRIpw4\ncQKvvfYaHnrooY47hYKCAjQ0NFy0HRQRuRRkXAiYWHLv3r1YsmQJGhoasGXLFgDA4cOHsXTp0i7z\nF6aqqanBDTfc8H/fWxGRPtLjneXu3btRUFCAkpISTJo0CefPn0dubi6SySSys7NRV1eHoqIi5za+\n//3vd3m9f/9+XH/99fT9/e2ZJRtxkOqzzz7D1KlTAQyMZ5ZsVAoANDc3d/w7ulYAfg7ZM8vvfe97\nZvyBBx6gbbNnluwZHRup8+6779I2fJ5ZrlixAs8++ywAPlrG9cySLYUb+sySvd/1zPInP/mJGWfP\nAA8dOtTl9RNPPIG1a9eioqKCtrFr166gNnoziTxkyBAzvn//fvqZHnul7du3Y/369QCAxsZGtLW1\nYdasWaiqqgLwvzV+58yZE2d/RUT6jR5vJRYtWoSnnnoK5eXlSCaT+N3vfocpU6Zg6dKlqKioQGlp\nKRYuXOjchjVG+ty5c/Quw/VkIPQz7P1x7jhZG+wusbi4uFssGkfO/ldnGdvQZSjYsgBxsDZc0/mn\n7m9Pfy2w8c7sru/TTz+l25o/f74ZD11WoqSkhLbBpI7Pjl5PmzbNfH9eXh7dFrtG2dIc7FyxO23X\nXS2702f7ZFU51NTU4MSJE7SN0L+W4swnwa4r13hypsfOMjs7G3/4wx+6xTds2BDcmIhIf6URPCIi\nHtRZioh4UGcpIuJBnaWIiIe0zJRuZZ5c2ShXlqy3ZhJ31RSGzojOMthNTU00xjKUrJaMZZPj1KSG\nbotlQHuz9pNlLU+dOmXGd+zYQbfFMsyzZ88241bVAuC+RlgNZGqVQ/SazXp+8uRJ2gY7v+x6Yxls\nNus5qzUEeDUFq0L45JNPzJirFpddb8OHDzfjubm5ZrxzTW+qlpaWoDZcdGcpIuJBnaWIiAd1liIi\nHtRZioh4UGcpIuJBnaWIiIe0lA5ZJQ3Z2dmxSk/YpA5sW705YUboJBdW6UkUa29vNz/DSjZYGYs1\nSYlrOwDfX7YtxjWRRmqZVfSalQgxp0+fNuPHjx+nn9m5c6cZZ9OkjRgxwoy7Jrlg10hqSVF0vKxE\n6PDhw7QNdu2yshcWZ9cOO7cAcPToUTPOlqiwSqOOHTtGl4Jw7Rcry2Lfx/jx42kbvXWtA7qzFBHx\nos5SRMSDOksREQ/qLEVEPKizFBHxkJZsuJX5bW9v79VlJUKz3q73s/ZZnGX1rIXqo6wwm/SAZSjZ\npAds4g1Xto9lpNlxsAlEXAtepU4UEr12LTMQwpWJ3717txln5/Caa64x47fccgttgy2qlXqtR0s3\nsAkl4lSEWNcVwM8Jq7yor6+nbWzdutWMs0XGksmkGXNN9MKWtWCfSV0ULeK61tnvR8Citv9/v4I/\nISJyGVJnKSLiQZ2liIgHdZYiIh7UWYqIeEhLNtxaxL6trS14KQiAZ8pYxjbOcgyhbTBW1jmKsQyl\nlVV0tc0ygXGOj2GVA67x50VFReZrdnxs+Qh2nq6//nraNhtb/MEHHwRtq7CwkLbBPpN6TqLXbBkR\nNl4d4N/5V199ZcbZOOzW1lYz/p///Ie2vWXLFjPOluAYPXp0t9iUKVPM5SYioZlqVsXhup5ZX+Kq\npmB0Zyki4kGdpYiIB3WWIiIe1FmKiHhQZyki4iEt2XArE8iyg3GFjhl3jQ1nWTc2Uzp7vzX+O8oG\ns3HYbFtnzpwx42yWb9fYV3YcoTNXs30C+LjxkpKSoH1i2fPGxkba9s9//nMzXlBQYMZZlvxPf/oT\nbaOsrMyMdx5nft9993Vkg9n34aqwYNlilt1mY6cPHDhgxmtqamjb7Pdz3rx5Zry5ublb7Nprr6Uz\nqwO8moLNm+CqvmBYhYBVodMT3VmKiHhQZyki4kGdpYiIB3WWIiIe1FmKiHhISzbcypQ1NzfTzG9v\nzmIeGu9N1vjTqF3WPhuzyjKgLHPIZgUHeNabzebNZgV3ZSdTZ3yPXrO22bZYtriuro62zbL3LJPL\nZh5/7733aBtVVVVmvHN1wooVK1BZWQmAf9+uc8iOnf3eNDU1mXGWER4zZgxte86cOWacrU0+bty4\nbrGbb77ZuW74v/71LzPOrmlWGeGaKZ31JVo3XETkIlFnKSLiQZ2liIgHdZYiIh7UWYqIeFBnKSLi\nIS2lQ1b5gKukIM7C86HiTEUful9W2UIUY8slsDbYwP/c3Fwzzkp0AF6uwiZuYCVFOTk53m1EZTCs\nHIZtiy03wY4b4CUmbNKR6dOnm3E28QYA7Nmzx4x/+eWXXV5HZTCs9IsdH8C/Q1bKNX78eDOeusRH\nxLVsBpvwhF23VqnakCFDcPc7lRaNAAALRElEQVTdd9M2/vnPf5pxVhrFyupcZXJsf0OXVgF0Zyki\n4kWdpYiIB3WWIiIe1FmKiHhQZyki4iEt2XArizV48OBYWefQz7B4nGwY2xabJMGVDWfZbbZfLBPI\nJo1wZTrZchCsbbbEgCuTm5qRjrLBLDNbWlpqxtnx3XXXXbTtvLw8M86+P1YdMGzYMNrGd77zHTN+\n++23d3n9i1/8AgDP3rsmdGGVA+x7Yt8rm3SEnVuAT5jBKiOszH1mZqbzOmTfE6uUCf3dAOKdd0Z3\nliIiHtRZioh4UGcpIuJBnaWIiAevzjKZTGL+/Pn429/+htraWjz44IMoLy/H448/7nxILCIyUHhl\nw1999dWO7NjLL7+M8vJyLFiwAGvWrEFlZSXKy8udn7emic/Ozo6VkXItOWFhbcSZVp5h+2S1HWWJ\n2fhelvFjWb2TJ0+a8fr6ejMOAKNHjzbj1vIfAM/cs2MAup/faOw3GyM9dOhQM84y0q5MtbXEAQDs\n37/fjLPz4WqDnd/Ucc3Ra3a9sbYBPlb/0KFDZrylpcWMs6y6a+w7qxBg49UnT55sxlKXF+mMjelm\nVRauJTgYtr/snLj0eGd58OBBHDhwAHfeeScAYOvWrR1rmcydOxfV1dXBjYqI9Dc9dpYrV67EsmXL\nOl4nEomO//EKCgrQ0NBw8fZOROQS4fwz/J133sH06dNx9dVXmz/3/TN68+bNmDRpUpdYY2Oj5y4O\nLK4/SwayvXv39vUu4J577kl7m48++mja27xU3Hrrrc6f7969O0170jucneWmTZtw9OhRbNq0CSdO\nnEBWVhZycnKQTCaRnZ2Nuro6OiKjs9RlNRsbGzFy5Mg+fWbZm0vh+j6zPH36dMdzvtBnluz5FXtm\nyUZgAOHPLNnxsSVLga7P6Pbu3dvxnyU77+yZJfvPxfWcnP2Shj6zZM8AAf7MsvO8nI8++ij++Mc/\ndov7tA2EP7NkS+GePXvWjLvmI2XPyNk1cscdd3R5feutt+Ljjz923hz85je/MeNsxBHbX3aeAH6N\nsmeWn376Kd2Ws7Ncu3Ztx79feeUVjB49Gjt37kRVVRV+9KMfYePGjXR9YRGRgSR4bPhjjz2GpUuX\noqKiAqWlpVi4cGGPn7HKi/pjyVHo3aj1Pzr7Xz7C7gjZnQybFdw1XpaNIWZ/JbC7Cdds96ljgqNx\nwKFVCOwOgN19AOj2yCdy5MgRM85m03bdebHvKfUaibKxrKIgdWb1ztgdE9sWmwmeXbeuvwzYdXXT\nTTeZ8euuu86Mff7557QNdg5PnDhhxtm148qS9+Zflt6d5WOPPdbx7w0bNgQ3JCLSn2kEj4iIB3WW\nIiIe1FmKiHhQZyki4kGdpYiIh7QsK8GWV4iTvg/9TDraYFzLSrA22CQCbAIKVv7BiooBXgrEsDZc\ny3+klp6wUpRI6IQHrmG2rIi+rKzMjO/cudOMu0qHfJdEiM4dK+VyXWupk3JERo0aFbQtth3X0ipj\nxowx4zfffLMZt67PZDJJBxsAfLmSOOcqVJyJdHRnKSLiQZ2liIgHdZYiIh7UWYqIeFBnKSLiIS3Z\n8DgTSvQlV5Y3hJXVi2Isy8smp2CZwOLiYjPuyj6zTCCbkowtr8CWBQC67290vOw4WAabZcnZ1HQA\nn4hhypQp9DOWffv20Z+xyRtSM8+u6cMA99IVLFvNzhX7ztl2SkpKaNss680y64yromDkyJFBn2ET\niLj2qTcz6LqzFBHxoM5SRMSDOksREQ/qLEVEPKizFBHxkJZsuJX5HTRoEM06u8asss+w7DLblqsN\n9rPQtq1MXJT9ZOOzWWaPjc/2XSytM5bFZm2w97vOYep+ReeOHR87hyzOMqMAX+SMja8fP368GXeN\na2bLJaRmyaNtsEy1a3kVlkkPzfCmLvERYRlvgGf72RIcbB4E1/ImbLE2dh2ybbkWlgutsnDRnaWI\niAd1liIiHtRZioh4UGcpIuJBnaWIiIe0ZMOtGZHz8/NpVs81Njt00XSWfY0zQzXLrLG4NQabzYQe\nueqqq8w4y4yyLKtrTHJotjH03Fo/i84Fy0KGVi24ssg9neNULEvOsrUAH7/83//+t8vraEb1M2fO\nmO93XYfsXLHzzmbHnz59uhl3jdtm+8u+D+s4Lly44LwO2bwGDJudnmXuAT4HhbLhIiIXiTpLEREP\n6ixFRDyosxQR8aDOUkTEgzpLEREPaSkdskozWltbg8uA4mBlSK7ypNAJPlgZglU2EZVrsAkiQttm\nZROuCQzY/obuEyuZcgmdpIQtgcEmywCAo0ePmvFZs2YFteEqQRoxYoQZT52cInq9f/9+8/2sbAng\n3y2LX3/99WZ8+PDhZpxNigGEl4uxyXJcZT2hpUNsf10TurBrmn3nLrqzFBHxoM5SRMSDOksREQ/q\nLEVEPKizFBHxkJZsuJUVzsrKCs6Mun4WJ+vdF1zZQYDvL/scm6jA1U7oJAIs6+2aSIMdB/tMaDbc\npaGhIWhbLGPqkkgkzHjqUhTRxCgTJ04033/kyBHaBjtX7PcmdRKPyLXXXmvGWZbchX1PVvXF4MGD\nnb9/bGKM0CVU4lRlaCINEZGLRJ2liIgHdZYiIh7UWYqIeFBnKSLiIeNCbw7EFhEZoHRnKSLiQZ2l\niIgHdZYiIh7UWYqIeFBnKSLiQZ2liIiHtEykEXn++eexa9cuZGRkYPny5Zg2bVo6m0+7mpoaLF68\nGL/85S/xwAMPoLa2FkuWLMH58+dRWFiIF198kU6E0Z+tWrUKO3bswLlz5/DII49g6tSpA/64E4kE\nli1bhqamJrS3t2Px4sWYOHHigD/uSDKZxA9+8AMsXrwYZWVlA/K403Zn+fHHH+Pw4cOoqKjAc889\nh+eeey5dTfeJtrY2PPPMMygrK+uIvfzyyygvL8df/vIXjB07FpWVlX24hxfHli1bsH//flRUVGDd\nunV4/vnnL4vj/vDDDzFlyhS8+eabWLt2LV544YXL4rgjr776ascsRgP1uNPWWVZXV2P+/PkAgPHj\nx6OlpcW56FR/l5WVhTfeeANFRUUdsa1bt2LevHkAgLlz56K6urqvdu+imTlzJl566SUAwLBhw5BI\nJC6L477vvvvw8MMPAwBqa2tRXFx8WRw3ABw8eBAHDhzAnXfeCWDgXudp6ywbGxu7rIiXn59P5x0c\nCDIzM7vNy5dIJDr+HCkoKBiQxz9o0KCOFSwrKytx++23XxbHHVm0aBGefPJJLF++/LI57pUrV2LZ\nsmUdrwfqcaf1mWVnl/soy4F+/O+//z4qKyuxfv163H333R3xgX7cb731Fvbu3Yvf/va3XY51oB73\nO++8g+nTp+Pqq682fz6QjjttnWVRUREaGxs7XtfX16OwsDBdzV8ScnJykEwmkZ2djbq6ui5/og8k\nmzdvxmuvvYZ169Zh6NChl8Vx7969GwUFBSgpKcGkSZNw/vx55ObmDvjj3rRpE44ePYpNmzbhxIkT\nyMrKGrDfd9r+DJ89ezaqqqoAAHv27EFRUVHHlPuXi1mzZnWcg40bN2LOnDl9vEe9r7W1FatWrcLr\nr7/esWzA5XDc27dvx/r16wH875FTW1vbZXHca9euxV//+le8/fbb+NnPfobFixcP2ONO66xDq1ev\nxvbt25GRkYGnn36arksyEOzevRsrV67E8ePHkZmZieLiYqxevRrLli1De3s7SktL8fvf/77HNXn6\nm4qKCrzyyitd1n154YUXsGLFigF93MlkEk899RRqa2uRTCbx6KOPYsqUKVi6dOmAPu7OXnnlFYwe\nPRq33XbbgDxuTdEmIuJBI3hERDyosxQR8aDOUkTEgzpLEREP6ixFRDyosxQR8aDOUkTEgzpLEREP\n/w/0GZ3CpdipoQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f8915f8c5f8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "VIIcc7JkHAVz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}