{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_Resnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahmoudFarouq/faceVerification/blob/master/CNN_Resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "dO-pdwg6Nl3r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.initializers import glorot_uniform\n",
        "%matplotlib inline\n",
        "\n",
        "import keras.backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "K.set_learning_phase(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYj_AbTggOHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drop = 0.3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cg2A7loWN1Px",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resNetBlock_Identity(X, f, filters, stage, block):\n",
        "  \n",
        "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "  \n",
        "  F1, F2, F3 = filters\n",
        "  short_cut = X\n",
        "  \n",
        "  X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), padding='valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(drop)(X)\n",
        "  \n",
        "  X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same' , name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(drop)(X)\n",
        "  \n",
        "  X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='valid', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "  X = Add()([X, short_cut])\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(drop)(X)\n",
        "  \n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hPmI6mC0OBnI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def resNetBlock_convolutional(X, f, filters, stage, block, s=2):\n",
        "  \n",
        "  conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "  bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "  \n",
        "  F1, F2, F3 = filters\n",
        "  short_cut = X\n",
        "  \n",
        "  X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), padding='valid', name = conv_name_base + '2a', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2a')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(drop)(X)\n",
        "  \n",
        "  X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), padding='same' , name = conv_name_base + '2b', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2b')(X)\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(drop)(X)\n",
        "  \n",
        "  X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), padding='same', name = conv_name_base + '2c', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "  X = BatchNormalization(axis = 3, name = bn_name_base + '2c')(X)\n",
        "  \n",
        "  short_cut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s), padding='valid' , name = conv_name_base + '1', kernel_initializer = glorot_uniform(seed=0))(short_cut)\n",
        "  short_cut = BatchNormalization(axis = 3, name = bn_name_base + '1')(short_cut)\n",
        "  \n",
        "  X = Add()([X, short_cut])\n",
        "  X = Activation('relu')(X)\n",
        "  X = Dropout(drop)(X)\n",
        "  return X"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CHZ5vhoYOK-Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ResNet50(input_shape=(48, 48, 3), classes=4):\n",
        "\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    X = Conv2D(64, (7, 7), strides=(2, 2), name='conv1', kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    X = resNetBlock_convolutional(X, f=3, filters=[64, 64, 256], stage=2, block='a', s=1)\n",
        "    X = resNetBlock_Identity(X, 3, [64, 64, 256], stage=2, block='b')\n",
        "    X = resNetBlock_Identity(X, 3, [64, 64, 256], stage=2, block='c')\n",
        "\n",
        "    X = resNetBlock_convolutional(X, f=3, filters=[128, 128, 512], stage=3, block='a', s=2)\n",
        "    X = resNetBlock_Identity(X, 3, [128, 128, 512], stage=3, block='b')\n",
        "    X = resNetBlock_Identity(X, 3, [128, 128, 512], stage=3, block='c')\n",
        "    X = resNetBlock_Identity(X, 3, [128, 128, 512], stage=3, block='d')\n",
        "\n",
        "    X = resNetBlock_convolutional(X, f=3, filters=[256, 256, 1024], stage=4, block='a', s=2)\n",
        "    X = resNetBlock_Identity(X, 3, [256, 256, 1024], stage=4, block='b')\n",
        "    X = resNetBlock_Identity(X, 3, [256, 256, 1024], stage=4, block='c')\n",
        "    X = resNetBlock_Identity(X, 3, [256, 256, 1024], stage=4, block='d')\n",
        "    X = resNetBlock_Identity(X, 3, [256, 256, 1024], stage=4, block='e')\n",
        "    X = resNetBlock_Identity(X, 3, [256, 256, 1024], stage=4, block='f')\n",
        "\n",
        "    X = resNetBlock_convolutional(X, f=3, filters=[512, 512, 2048], stage=5, block='a', s=2)\n",
        "    X = resNetBlock_Identity(X, 3, [512, 512, 2048], stage=5, block='b')\n",
        "    X = resNetBlock_Identity(X, 3, [512, 512, 2048], stage=5, block='c')\n",
        "\n",
        "    X = AveragePooling2D(pool_size=(2, 2), padding='same')(X)\n",
        "\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer=glorot_uniform(seed=0))(X)\n",
        "\n",
        "    model = Model(inputs=X_input, outputs=X, name='ResNet50')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v1M3d1e0Og0p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!wget https://github.com/MahmoudFarouq/faceVerification/raw/master/dataSet/dataObject.pkl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QleD5099Owm3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cc8d45f-5174-4fdd-ad4b-13802e4bb16c"
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dataObject.pkl\tdataObject.pkl.1  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ALIDY84zOyws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open(\"dataObject.pkl\", 'rb') as dataFile:\n",
        "  X = pickle.load(dataFile)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IepGPJiQO6y_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y = [\n",
        "    np.full((250, 1), 0), \n",
        "    np.full((250, 1), 1), \n",
        "    np.full((250, 1), 2), \n",
        "    np.full((250, 1), 3)\n",
        "]\n",
        "y = np.vstack(y)\n",
        "\n",
        "# onehot encoding\n",
        "y_encoder = OneHotEncoder(sparse=False, categories='auto')\n",
        "y = y_encoder.fit_transform(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DUjkjVTtQC4Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "a6a4a911-9ef4-4cea-9302-9b0fbe035f7a"
      },
      "cell_type": "code",
      "source": [
        "plt.imshow( X[750].reshape(48, 48, 3) )"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7d02eb25f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD6CAYAAACI7Fo9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7d03065d30>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TXVoBiCsQOxg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14f04b3a-f6b7-45bf-ca02-2479e9a1a1c6"
      },
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 48, 48, 3), (1000, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "metadata": {
        "id": "PI3jCygjQUim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uM3yYedLOWNE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = ResNet50()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MuJjakm6Oagq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opt = Adam(lr=0.0001)\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=opt,\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJ690EYgROTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8735
        },
        "outputId": "bbb0959e-dfd3-4053-af37-02f2a4d4591e"
      },
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_train, \n",
        "    y_train,\n",
        "    validation_data=(X_test,y_test),\n",
        "    validation_split=0.8,\n",
        "    epochs=250,\n",
        "    shuffle=True,\n",
        "    batch_size=128,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/250\n",
            "800/800 [==============================] - 3s 4ms/step - loss: 0.0705 - acc: 0.9725 - val_loss: 0.9273 - val_acc: 0.7900\n",
            "Epoch 2/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0765 - acc: 0.9775 - val_loss: 0.7958 - val_acc: 0.8050\n",
            "Epoch 3/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0932 - acc: 0.9625 - val_loss: 0.7994 - val_acc: 0.8250\n",
            "Epoch 4/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0598 - acc: 0.9800 - val_loss: 0.9626 - val_acc: 0.8100\n",
            "Epoch 5/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0588 - acc: 0.9788 - val_loss: 0.7785 - val_acc: 0.8000\n",
            "Epoch 6/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0765 - acc: 0.9763 - val_loss: 0.7929 - val_acc: 0.8050\n",
            "Epoch 7/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0956 - acc: 0.9738 - val_loss: 1.0500 - val_acc: 0.7900\n",
            "Epoch 8/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0947 - acc: 0.9725 - val_loss: 0.7629 - val_acc: 0.8350\n",
            "Epoch 9/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0676 - acc: 0.9763 - val_loss: 0.8027 - val_acc: 0.8150\n",
            "Epoch 10/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0662 - acc: 0.9725 - val_loss: 0.6658 - val_acc: 0.8000\n",
            "Epoch 11/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0658 - acc: 0.9763 - val_loss: 0.7547 - val_acc: 0.7900\n",
            "Epoch 12/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1080 - acc: 0.9575 - val_loss: 0.6792 - val_acc: 0.8350\n",
            "Epoch 13/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0820 - acc: 0.9675 - val_loss: 0.7376 - val_acc: 0.8250\n",
            "Epoch 14/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0865 - acc: 0.9700 - val_loss: 0.7295 - val_acc: 0.8050\n",
            "Epoch 15/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0803 - acc: 0.9750 - val_loss: 0.8011 - val_acc: 0.8000\n",
            "Epoch 16/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0790 - acc: 0.9675 - val_loss: 1.0522 - val_acc: 0.7850\n",
            "Epoch 17/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0698 - acc: 0.9713 - val_loss: 0.7997 - val_acc: 0.7800\n",
            "Epoch 18/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0548 - acc: 0.9800 - val_loss: 0.6969 - val_acc: 0.8300\n",
            "Epoch 19/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0809 - acc: 0.9738 - val_loss: 0.7634 - val_acc: 0.8050\n",
            "Epoch 20/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0694 - acc: 0.9688 - val_loss: 0.8926 - val_acc: 0.7850\n",
            "Epoch 21/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0386 - acc: 0.9888 - val_loss: 0.8386 - val_acc: 0.7900\n",
            "Epoch 22/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0713 - acc: 0.9725 - val_loss: 0.7438 - val_acc: 0.8000\n",
            "Epoch 23/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0554 - acc: 0.9800 - val_loss: 0.9344 - val_acc: 0.8050\n",
            "Epoch 24/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0629 - acc: 0.9825 - val_loss: 0.8740 - val_acc: 0.7950\n",
            "Epoch 25/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1514 - acc: 0.9500 - val_loss: 0.9696 - val_acc: 0.8100\n",
            "Epoch 26/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0977 - acc: 0.9700 - val_loss: 0.7964 - val_acc: 0.8300\n",
            "Epoch 27/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1151 - acc: 0.9575 - val_loss: 0.8637 - val_acc: 0.7750\n",
            "Epoch 28/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0751 - acc: 0.9738 - val_loss: 0.8017 - val_acc: 0.7950\n",
            "Epoch 29/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1022 - acc: 0.9600 - val_loss: 0.7267 - val_acc: 0.8000\n",
            "Epoch 30/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0937 - acc: 0.9650 - val_loss: 0.8394 - val_acc: 0.7800\n",
            "Epoch 31/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0615 - acc: 0.9800 - val_loss: 0.8384 - val_acc: 0.8150\n",
            "Epoch 32/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0685 - acc: 0.9700 - val_loss: 0.7895 - val_acc: 0.8200\n",
            "Epoch 33/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0769 - acc: 0.9700 - val_loss: 0.8276 - val_acc: 0.7950\n",
            "Epoch 34/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0699 - acc: 0.9738 - val_loss: 0.7708 - val_acc: 0.7950\n",
            "Epoch 35/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0824 - acc: 0.9738 - val_loss: 0.8896 - val_acc: 0.8150\n",
            "Epoch 36/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0822 - acc: 0.9713 - val_loss: 0.9929 - val_acc: 0.7800\n",
            "Epoch 37/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0962 - acc: 0.9675 - val_loss: 0.6906 - val_acc: 0.8450\n",
            "Epoch 38/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0447 - acc: 0.9850 - val_loss: 0.8026 - val_acc: 0.7800\n",
            "Epoch 39/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0501 - acc: 0.9838 - val_loss: 0.8577 - val_acc: 0.8050\n",
            "Epoch 40/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0743 - acc: 0.9725 - val_loss: 0.8648 - val_acc: 0.8100\n",
            "Epoch 41/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0775 - acc: 0.9738 - val_loss: 0.9331 - val_acc: 0.7950\n",
            "Epoch 42/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1122 - acc: 0.9637 - val_loss: 0.9212 - val_acc: 0.7700\n",
            "Epoch 43/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0935 - acc: 0.9688 - val_loss: 0.8170 - val_acc: 0.7900\n",
            "Epoch 44/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0821 - acc: 0.9750 - val_loss: 0.7421 - val_acc: 0.8350\n",
            "Epoch 45/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0711 - acc: 0.9713 - val_loss: 0.8252 - val_acc: 0.8200\n",
            "Epoch 46/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0810 - acc: 0.9750 - val_loss: 0.9856 - val_acc: 0.7750\n",
            "Epoch 47/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0625 - acc: 0.9800 - val_loss: 0.9429 - val_acc: 0.7650\n",
            "Epoch 48/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0792 - acc: 0.9725 - val_loss: 1.0562 - val_acc: 0.7850\n",
            "Epoch 49/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0507 - acc: 0.9838 - val_loss: 0.7762 - val_acc: 0.8150\n",
            "Epoch 50/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0526 - acc: 0.9888 - val_loss: 0.8480 - val_acc: 0.8100\n",
            "Epoch 51/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0551 - acc: 0.9800 - val_loss: 0.7910 - val_acc: 0.8150\n",
            "Epoch 52/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0997 - acc: 0.9600 - val_loss: 0.7131 - val_acc: 0.8100\n",
            "Epoch 53/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1072 - acc: 0.9612 - val_loss: 0.8676 - val_acc: 0.7850\n",
            "Epoch 54/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1015 - acc: 0.9662 - val_loss: 0.9656 - val_acc: 0.7550\n",
            "Epoch 55/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1430 - acc: 0.9513 - val_loss: 0.8697 - val_acc: 0.7700\n",
            "Epoch 56/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1240 - acc: 0.9587 - val_loss: 0.7595 - val_acc: 0.7850\n",
            "Epoch 57/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0900 - acc: 0.9637 - val_loss: 0.5953 - val_acc: 0.8200\n",
            "Epoch 58/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0611 - acc: 0.9750 - val_loss: 0.7875 - val_acc: 0.8050\n",
            "Epoch 59/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0636 - acc: 0.9775 - val_loss: 0.7241 - val_acc: 0.8300\n",
            "Epoch 60/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0451 - acc: 0.9838 - val_loss: 0.8747 - val_acc: 0.8150\n",
            "Epoch 61/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0616 - acc: 0.9825 - val_loss: 0.8803 - val_acc: 0.7950\n",
            "Epoch 62/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0629 - acc: 0.9725 - val_loss: 0.9766 - val_acc: 0.7850\n",
            "Epoch 63/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0515 - acc: 0.9812 - val_loss: 0.8194 - val_acc: 0.8450\n",
            "Epoch 64/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0524 - acc: 0.9812 - val_loss: 0.8451 - val_acc: 0.8300\n",
            "Epoch 65/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0668 - acc: 0.9788 - val_loss: 0.9198 - val_acc: 0.7850\n",
            "Epoch 66/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0649 - acc: 0.9750 - val_loss: 0.8641 - val_acc: 0.8150\n",
            "Epoch 67/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0409 - acc: 0.9825 - val_loss: 0.8726 - val_acc: 0.8400\n",
            "Epoch 68/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0528 - acc: 0.9788 - val_loss: 0.8999 - val_acc: 0.8250\n",
            "Epoch 69/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0669 - acc: 0.9775 - val_loss: 0.7980 - val_acc: 0.7800\n",
            "Epoch 70/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0511 - acc: 0.9838 - val_loss: 0.8602 - val_acc: 0.8400\n",
            "Epoch 71/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0445 - acc: 0.9838 - val_loss: 0.7895 - val_acc: 0.7950\n",
            "Epoch 72/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0394 - acc: 0.9825 - val_loss: 0.7821 - val_acc: 0.7900\n",
            "Epoch 73/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0517 - acc: 0.9800 - val_loss: 0.8901 - val_acc: 0.8150\n",
            "Epoch 74/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0478 - acc: 0.9825 - val_loss: 0.7982 - val_acc: 0.8200\n",
            "Epoch 75/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0538 - acc: 0.9812 - val_loss: 0.7223 - val_acc: 0.8250\n",
            "Epoch 76/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0365 - acc: 0.9875 - val_loss: 0.8691 - val_acc: 0.8150\n",
            "Epoch 77/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0669 - acc: 0.9812 - val_loss: 0.7983 - val_acc: 0.8400\n",
            "Epoch 78/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0940 - acc: 0.9675 - val_loss: 0.8790 - val_acc: 0.7850\n",
            "Epoch 79/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0587 - acc: 0.9763 - val_loss: 0.9815 - val_acc: 0.8050\n",
            "Epoch 80/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0779 - acc: 0.9738 - val_loss: 0.9208 - val_acc: 0.7900\n",
            "Epoch 81/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0873 - acc: 0.9688 - val_loss: 0.8042 - val_acc: 0.8250\n",
            "Epoch 82/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0986 - acc: 0.9662 - val_loss: 0.8622 - val_acc: 0.8100\n",
            "Epoch 83/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0480 - acc: 0.9862 - val_loss: 1.0981 - val_acc: 0.7650\n",
            "Epoch 84/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0552 - acc: 0.9800 - val_loss: 0.7736 - val_acc: 0.8200\n",
            "Epoch 85/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0367 - acc: 0.9850 - val_loss: 0.8838 - val_acc: 0.8200\n",
            "Epoch 86/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0595 - acc: 0.9812 - val_loss: 0.7587 - val_acc: 0.8100\n",
            "Epoch 87/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0401 - acc: 0.9838 - val_loss: 0.9456 - val_acc: 0.7950\n",
            "Epoch 88/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0553 - acc: 0.9812 - val_loss: 0.8306 - val_acc: 0.8300\n",
            "Epoch 89/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0847 - acc: 0.9713 - val_loss: 0.7346 - val_acc: 0.8250\n",
            "Epoch 90/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0490 - acc: 0.9850 - val_loss: 0.7837 - val_acc: 0.8500\n",
            "Epoch 91/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0782 - acc: 0.9688 - val_loss: 0.7025 - val_acc: 0.8450\n",
            "Epoch 92/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0488 - acc: 0.9825 - val_loss: 0.8145 - val_acc: 0.8300\n",
            "Epoch 93/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0692 - acc: 0.9738 - val_loss: 0.9435 - val_acc: 0.7850\n",
            "Epoch 94/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1433 - acc: 0.9612 - val_loss: 1.0408 - val_acc: 0.7950\n",
            "Epoch 95/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0903 - acc: 0.9650 - val_loss: 0.9186 - val_acc: 0.7900\n",
            "Epoch 96/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1074 - acc: 0.9625 - val_loss: 0.8854 - val_acc: 0.8100\n",
            "Epoch 97/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0650 - acc: 0.9788 - val_loss: 0.7309 - val_acc: 0.8200\n",
            "Epoch 98/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0637 - acc: 0.9812 - val_loss: 0.7932 - val_acc: 0.8050\n",
            "Epoch 99/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0820 - acc: 0.9725 - val_loss: 0.7920 - val_acc: 0.8200\n",
            "Epoch 100/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0568 - acc: 0.9788 - val_loss: 1.0013 - val_acc: 0.7750\n",
            "Epoch 101/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0667 - acc: 0.9750 - val_loss: 0.8629 - val_acc: 0.8200\n",
            "Epoch 102/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0539 - acc: 0.9775 - val_loss: 0.7952 - val_acc: 0.8400\n",
            "Epoch 103/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0496 - acc: 0.9838 - val_loss: 1.0162 - val_acc: 0.8050\n",
            "Epoch 104/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0795 - acc: 0.9725 - val_loss: 0.8987 - val_acc: 0.8300\n",
            "Epoch 105/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0434 - acc: 0.9838 - val_loss: 0.8238 - val_acc: 0.8100\n",
            "Epoch 106/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0757 - acc: 0.9675 - val_loss: 0.8661 - val_acc: 0.7950\n",
            "Epoch 107/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0622 - acc: 0.9700 - val_loss: 0.8542 - val_acc: 0.8100\n",
            "Epoch 108/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0396 - acc: 0.9862 - val_loss: 0.9829 - val_acc: 0.7900\n",
            "Epoch 109/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0491 - acc: 0.9812 - val_loss: 0.8470 - val_acc: 0.8250\n",
            "Epoch 110/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0464 - acc: 0.9825 - val_loss: 0.8943 - val_acc: 0.8000\n",
            "Epoch 111/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0416 - acc: 0.9850 - val_loss: 0.9032 - val_acc: 0.8000\n",
            "Epoch 112/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0782 - acc: 0.9738 - val_loss: 0.8710 - val_acc: 0.8100\n",
            "Epoch 113/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0349 - acc: 0.9900 - val_loss: 0.8468 - val_acc: 0.8150\n",
            "Epoch 114/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0602 - acc: 0.9812 - val_loss: 1.0310 - val_acc: 0.7900\n",
            "Epoch 115/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0699 - acc: 0.9788 - val_loss: 0.8827 - val_acc: 0.7950\n",
            "Epoch 116/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0621 - acc: 0.9725 - val_loss: 0.7699 - val_acc: 0.8250\n",
            "Epoch 117/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0913 - acc: 0.9688 - val_loss: 0.8351 - val_acc: 0.7900\n",
            "Epoch 118/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0692 - acc: 0.9800 - val_loss: 0.6751 - val_acc: 0.8450\n",
            "Epoch 119/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0809 - acc: 0.9662 - val_loss: 0.8423 - val_acc: 0.7900\n",
            "Epoch 120/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0702 - acc: 0.9800 - val_loss: 0.8520 - val_acc: 0.8200\n",
            "Epoch 121/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0475 - acc: 0.9812 - val_loss: 0.8645 - val_acc: 0.8350\n",
            "Epoch 122/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1094 - acc: 0.9713 - val_loss: 0.7817 - val_acc: 0.8300\n",
            "Epoch 123/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0550 - acc: 0.9812 - val_loss: 0.7621 - val_acc: 0.8250\n",
            "Epoch 124/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0649 - acc: 0.9800 - val_loss: 0.8861 - val_acc: 0.8300\n",
            "Epoch 125/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0848 - acc: 0.9713 - val_loss: 0.8295 - val_acc: 0.8350\n",
            "Epoch 126/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0339 - acc: 0.9925 - val_loss: 0.7325 - val_acc: 0.8400\n",
            "Epoch 127/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0509 - acc: 0.9825 - val_loss: 0.7518 - val_acc: 0.8250\n",
            "Epoch 128/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0363 - acc: 0.9888 - val_loss: 0.7477 - val_acc: 0.8300\n",
            "Epoch 129/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0463 - acc: 0.9825 - val_loss: 0.9230 - val_acc: 0.8000\n",
            "Epoch 130/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0520 - acc: 0.9788 - val_loss: 0.8307 - val_acc: 0.8250\n",
            "Epoch 131/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0613 - acc: 0.9788 - val_loss: 0.6634 - val_acc: 0.8300\n",
            "Epoch 132/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0411 - acc: 0.9850 - val_loss: 0.8021 - val_acc: 0.8100\n",
            "Epoch 133/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0496 - acc: 0.9838 - val_loss: 0.8783 - val_acc: 0.8350\n",
            "Epoch 134/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0427 - acc: 0.9862 - val_loss: 0.6580 - val_acc: 0.8350\n",
            "Epoch 135/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0871 - acc: 0.9700 - val_loss: 0.8113 - val_acc: 0.7900\n",
            "Epoch 136/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0861 - acc: 0.9688 - val_loss: 0.8408 - val_acc: 0.8150\n",
            "Epoch 137/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0744 - acc: 0.9750 - val_loss: 0.7727 - val_acc: 0.8250\n",
            "Epoch 138/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0732 - acc: 0.9750 - val_loss: 0.8180 - val_acc: 0.8400\n",
            "Epoch 139/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0801 - acc: 0.9763 - val_loss: 0.8714 - val_acc: 0.8000\n",
            "Epoch 140/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0902 - acc: 0.9612 - val_loss: 0.8634 - val_acc: 0.8150\n",
            "Epoch 141/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0751 - acc: 0.9713 - val_loss: 0.7250 - val_acc: 0.8050\n",
            "Epoch 142/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0555 - acc: 0.9825 - val_loss: 1.0393 - val_acc: 0.7750\n",
            "Epoch 143/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0785 - acc: 0.9688 - val_loss: 0.8547 - val_acc: 0.8300\n",
            "Epoch 144/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0438 - acc: 0.9850 - val_loss: 0.8986 - val_acc: 0.8300\n",
            "Epoch 145/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0508 - acc: 0.9838 - val_loss: 0.8089 - val_acc: 0.8300\n",
            "Epoch 146/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0284 - acc: 0.9937 - val_loss: 0.8352 - val_acc: 0.8100\n",
            "Epoch 147/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0478 - acc: 0.9763 - val_loss: 0.7966 - val_acc: 0.8200\n",
            "Epoch 148/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0434 - acc: 0.9850 - val_loss: 0.8956 - val_acc: 0.8100\n",
            "Epoch 149/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0473 - acc: 0.9838 - val_loss: 0.8434 - val_acc: 0.8300\n",
            "Epoch 150/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0676 - acc: 0.9750 - val_loss: 0.8574 - val_acc: 0.8150\n",
            "Epoch 151/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0558 - acc: 0.9775 - val_loss: 0.8239 - val_acc: 0.8150\n",
            "Epoch 152/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0670 - acc: 0.9763 - val_loss: 0.9940 - val_acc: 0.8350\n",
            "Epoch 153/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0569 - acc: 0.9738 - val_loss: 0.7885 - val_acc: 0.8400\n",
            "Epoch 154/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0472 - acc: 0.9862 - val_loss: 0.9251 - val_acc: 0.8200\n",
            "Epoch 155/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0416 - acc: 0.9812 - val_loss: 0.7812 - val_acc: 0.8200\n",
            "Epoch 156/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0540 - acc: 0.9812 - val_loss: 0.7592 - val_acc: 0.8150\n",
            "Epoch 157/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0744 - acc: 0.9763 - val_loss: 0.9105 - val_acc: 0.7800\n",
            "Epoch 158/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0342 - acc: 0.9888 - val_loss: 0.9806 - val_acc: 0.8000\n",
            "Epoch 159/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0577 - acc: 0.9775 - val_loss: 0.7914 - val_acc: 0.8200\n",
            "Epoch 160/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0478 - acc: 0.9862 - val_loss: 0.7845 - val_acc: 0.8350\n",
            "Epoch 161/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0616 - acc: 0.9738 - val_loss: 0.7924 - val_acc: 0.8050\n",
            "Epoch 162/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0318 - acc: 0.9888 - val_loss: 0.7557 - val_acc: 0.8350\n",
            "Epoch 163/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0560 - acc: 0.9763 - val_loss: 0.8402 - val_acc: 0.8100\n",
            "Epoch 164/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0809 - acc: 0.9713 - val_loss: 0.6915 - val_acc: 0.8450\n",
            "Epoch 165/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0590 - acc: 0.9812 - val_loss: 0.6877 - val_acc: 0.8150\n",
            "Epoch 166/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0803 - acc: 0.9700 - val_loss: 0.9993 - val_acc: 0.7900\n",
            "Epoch 167/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0746 - acc: 0.9700 - val_loss: 1.0878 - val_acc: 0.7550\n",
            "Epoch 168/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1020 - acc: 0.9650 - val_loss: 0.8310 - val_acc: 0.8400\n",
            "Epoch 169/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0510 - acc: 0.9825 - val_loss: 0.8754 - val_acc: 0.8000\n",
            "Epoch 170/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0535 - acc: 0.9812 - val_loss: 0.7679 - val_acc: 0.8150\n",
            "Epoch 171/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0630 - acc: 0.9800 - val_loss: 0.7311 - val_acc: 0.8350\n",
            "Epoch 172/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0684 - acc: 0.9750 - val_loss: 0.9779 - val_acc: 0.7850\n",
            "Epoch 173/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0549 - acc: 0.9850 - val_loss: 0.9822 - val_acc: 0.8000\n",
            "Epoch 174/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0623 - acc: 0.9788 - val_loss: 0.8164 - val_acc: 0.8300\n",
            "Epoch 175/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0471 - acc: 0.9875 - val_loss: 0.6455 - val_acc: 0.8500\n",
            "Epoch 176/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0512 - acc: 0.9862 - val_loss: 0.6897 - val_acc: 0.8250\n",
            "Epoch 177/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0553 - acc: 0.9800 - val_loss: 0.8709 - val_acc: 0.8300\n",
            "Epoch 178/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0543 - acc: 0.9850 - val_loss: 0.6672 - val_acc: 0.8250\n",
            "Epoch 179/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0255 - acc: 0.9913 - val_loss: 0.7013 - val_acc: 0.8400\n",
            "Epoch 180/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0571 - acc: 0.9825 - val_loss: 0.8137 - val_acc: 0.8250\n",
            "Epoch 181/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0410 - acc: 0.9875 - val_loss: 0.8985 - val_acc: 0.7900\n",
            "Epoch 182/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0281 - acc: 0.9913 - val_loss: 0.7814 - val_acc: 0.8250\n",
            "Epoch 183/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0307 - acc: 0.9913 - val_loss: 0.8854 - val_acc: 0.7850\n",
            "Epoch 184/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0393 - acc: 0.9900 - val_loss: 0.6478 - val_acc: 0.8650\n",
            "Epoch 185/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0511 - acc: 0.9800 - val_loss: 0.8396 - val_acc: 0.8200\n",
            "Epoch 186/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1108 - acc: 0.9637 - val_loss: 0.8310 - val_acc: 0.8350\n",
            "Epoch 187/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0738 - acc: 0.9725 - val_loss: 0.8806 - val_acc: 0.8050\n",
            "Epoch 188/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0676 - acc: 0.9775 - val_loss: 0.9520 - val_acc: 0.7600\n",
            "Epoch 189/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0539 - acc: 0.9850 - val_loss: 0.8326 - val_acc: 0.8350\n",
            "Epoch 190/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0478 - acc: 0.9825 - val_loss: 0.6968 - val_acc: 0.8650\n",
            "Epoch 191/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0480 - acc: 0.9862 - val_loss: 0.8102 - val_acc: 0.7950\n",
            "Epoch 192/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0366 - acc: 0.9862 - val_loss: 0.8341 - val_acc: 0.8000\n",
            "Epoch 193/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0477 - acc: 0.9850 - val_loss: 0.5368 - val_acc: 0.9000\n",
            "Epoch 194/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0364 - acc: 0.9850 - val_loss: 0.9599 - val_acc: 0.8150\n",
            "Epoch 195/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0539 - acc: 0.9812 - val_loss: 0.7734 - val_acc: 0.8500\n",
            "Epoch 196/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0619 - acc: 0.9800 - val_loss: 0.7986 - val_acc: 0.8450\n",
            "Epoch 197/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0638 - acc: 0.9763 - val_loss: 0.7857 - val_acc: 0.8650\n",
            "Epoch 198/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0547 - acc: 0.9800 - val_loss: 0.8741 - val_acc: 0.8050\n",
            "Epoch 199/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0564 - acc: 0.9812 - val_loss: 0.7104 - val_acc: 0.8350\n",
            "Epoch 200/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0389 - acc: 0.9862 - val_loss: 0.7614 - val_acc: 0.8350\n",
            "Epoch 201/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0386 - acc: 0.9825 - val_loss: 0.6558 - val_acc: 0.8350\n",
            "Epoch 202/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0485 - acc: 0.9800 - val_loss: 0.9858 - val_acc: 0.8250\n",
            "Epoch 203/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0253 - acc: 0.9888 - val_loss: 0.7871 - val_acc: 0.8300\n",
            "Epoch 204/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0364 - acc: 0.9862 - val_loss: 0.8355 - val_acc: 0.8100\n",
            "Epoch 205/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0297 - acc: 0.9875 - val_loss: 0.9007 - val_acc: 0.8250\n",
            "Epoch 206/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0487 - acc: 0.9875 - val_loss: 0.8313 - val_acc: 0.8250\n",
            "Epoch 207/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0441 - acc: 0.9875 - val_loss: 0.8043 - val_acc: 0.8350\n",
            "Epoch 208/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0463 - acc: 0.9850 - val_loss: 0.8020 - val_acc: 0.8350\n",
            "Epoch 209/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0740 - acc: 0.9825 - val_loss: 1.0107 - val_acc: 0.7650\n",
            "Epoch 210/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0545 - acc: 0.9812 - val_loss: 0.7737 - val_acc: 0.8500\n",
            "Epoch 211/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0482 - acc: 0.9838 - val_loss: 0.7796 - val_acc: 0.8350\n",
            "Epoch 212/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0618 - acc: 0.9825 - val_loss: 0.9950 - val_acc: 0.8000\n",
            "Epoch 213/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0398 - acc: 0.9862 - val_loss: 0.8097 - val_acc: 0.7900\n",
            "Epoch 214/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0600 - acc: 0.9775 - val_loss: 0.9726 - val_acc: 0.7950\n",
            "Epoch 215/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0293 - acc: 0.9875 - val_loss: 0.9417 - val_acc: 0.8050\n",
            "Epoch 216/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0534 - acc: 0.9788 - val_loss: 0.9365 - val_acc: 0.8100\n",
            "Epoch 217/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0498 - acc: 0.9850 - val_loss: 0.8188 - val_acc: 0.8450\n",
            "Epoch 218/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0233 - acc: 0.9913 - val_loss: 0.8201 - val_acc: 0.8050\n",
            "Epoch 219/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0382 - acc: 0.9850 - val_loss: 0.8861 - val_acc: 0.8300\n",
            "Epoch 220/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0520 - acc: 0.9775 - val_loss: 0.8450 - val_acc: 0.8300\n",
            "Epoch 221/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0955 - acc: 0.9713 - val_loss: 0.8606 - val_acc: 0.8000\n",
            "Epoch 222/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.1304 - acc: 0.9500 - val_loss: 0.8265 - val_acc: 0.8050\n",
            "Epoch 223/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0992 - acc: 0.9700 - val_loss: 1.0635 - val_acc: 0.7850\n",
            "Epoch 224/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0704 - acc: 0.9750 - val_loss: 0.9162 - val_acc: 0.8250\n",
            "Epoch 225/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0689 - acc: 0.9812 - val_loss: 0.6933 - val_acc: 0.8650\n",
            "Epoch 226/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0723 - acc: 0.9763 - val_loss: 0.7960 - val_acc: 0.8150\n",
            "Epoch 227/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0403 - acc: 0.9862 - val_loss: 0.7656 - val_acc: 0.8450\n",
            "Epoch 228/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0450 - acc: 0.9850 - val_loss: 0.8862 - val_acc: 0.8200\n",
            "Epoch 229/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0468 - acc: 0.9862 - val_loss: 0.7073 - val_acc: 0.8300\n",
            "Epoch 230/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0479 - acc: 0.9800 - val_loss: 0.7208 - val_acc: 0.8300\n",
            "Epoch 231/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0352 - acc: 0.9888 - val_loss: 0.8360 - val_acc: 0.8150\n",
            "Epoch 232/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0291 - acc: 0.9913 - val_loss: 0.8775 - val_acc: 0.8400\n",
            "Epoch 233/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0232 - acc: 0.9937 - val_loss: 0.7685 - val_acc: 0.8400\n",
            "Epoch 234/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0505 - acc: 0.9812 - val_loss: 1.0063 - val_acc: 0.8150\n",
            "Epoch 235/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0470 - acc: 0.9838 - val_loss: 0.8781 - val_acc: 0.8100\n",
            "Epoch 236/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0352 - acc: 0.9862 - val_loss: 0.8296 - val_acc: 0.8250\n",
            "Epoch 237/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0292 - acc: 0.9900 - val_loss: 0.7409 - val_acc: 0.8400\n",
            "Epoch 238/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0145 - acc: 0.9962 - val_loss: 0.8274 - val_acc: 0.8450\n",
            "Epoch 239/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0212 - acc: 0.9900 - val_loss: 0.8099 - val_acc: 0.8050\n",
            "Epoch 240/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0254 - acc: 0.9888 - val_loss: 0.9082 - val_acc: 0.8300\n",
            "Epoch 241/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0247 - acc: 0.9925 - val_loss: 0.7644 - val_acc: 0.8550\n",
            "Epoch 242/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0335 - acc: 0.9862 - val_loss: 0.7437 - val_acc: 0.8450\n",
            "Epoch 243/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0464 - acc: 0.9875 - val_loss: 0.6484 - val_acc: 0.8600\n",
            "Epoch 244/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0706 - acc: 0.9713 - val_loss: 0.8400 - val_acc: 0.8000\n",
            "Epoch 245/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0332 - acc: 0.9913 - val_loss: 0.8200 - val_acc: 0.8200\n",
            "Epoch 246/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0506 - acc: 0.9800 - val_loss: 0.8086 - val_acc: 0.8200\n",
            "Epoch 247/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0432 - acc: 0.9875 - val_loss: 0.8549 - val_acc: 0.8250\n",
            "Epoch 248/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0360 - acc: 0.9888 - val_loss: 0.7413 - val_acc: 0.8250\n",
            "Epoch 249/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0481 - acc: 0.9800 - val_loss: 0.9115 - val_acc: 0.8000\n",
            "Epoch 250/250\n",
            "800/800 [==============================] - 3s 3ms/step - loss: 0.0372 - acc: 0.9875 - val_loss: 0.7669 - val_acc: 0.8200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f7cf9aaaa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "metadata": {
        "id": "GxjQPIEvQhDO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "names = ['karim', 'omar', 'amr', 'farouq']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oSJQirpvUPq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c6cfb80d-3dc6-4284-d446-7ee1f62dbaa2"
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "200/200 [==============================] - 0s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2138283634185791, 0.8]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "sRDLsfAUUSL5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "3a7da308-1da8-44ea-823a-64d392910ae8"
      },
      "cell_type": "code",
      "source": [
        "index = np.random.randint(0, 800)\n",
        "plt.imshow(X_train[index].reshape(48, 48, 3))\n",
        "names[np.argmax(model.predict(X_train[index].reshape(1, 48, 48, 3)))]"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'omar'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD6CAYAAACI7Fo9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f7cf8e2ebe0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "NEZ7KW7jUVsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9283ec01-e4fc-43d4-c720-7b82f96d9161"
      },
      "cell_type": "code",
      "source": [
        "print(\"train acc: \", np.sum(np.argmax(model.predict(X_train), axis=1) == np.argmax(y_train, axis=1)) / 800)"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train acc:  0.95625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8KiMZ38Lx7I7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a683edf-4a31-4f7f-92ab-2f9efba0f2d0"
      },
      "cell_type": "code",
      "source": [
        "print(\"test acc: \", np.sum(np.argmax(model.predict(X_test), axis=1) == np.argmax(y_test, axis=1)) / 200)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test acc:  0.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OXTBNuvF2z6D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}